@article{batesFittingLinearMixedEffects2015,
  title = {Fitting {{Linear Mixed-Effects Models Using}} {\textbf{Lme4}}},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  urldate = {2022-07-14},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/HDSUVZ87/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf}
}

@article{brauerLinearMixedeffectsModels2018,
  title = {Linear Mixed-Effects Models and the Analysis of Nonindependent Data: {{A}} Unified Framework to Analyze Categorical and Continuous Independent Variables That Vary within-Subjects and/or within-Items.},
  shorttitle = {Linear Mixed-Effects Models and the Analysis of Nonindependent Data},
  author = {Brauer, Markus and Curtin, John J.},
  year = {2018},
  month = sep,
  journal = {Psychological Methods},
  volume = {23},
  number = {3},
  pages = {389--411},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000159},
  urldate = {2023-04-24},
  abstract = {In this article we address a number of important issues that arise in the analysis of nonindependent data. Such data are common in studies in which predictors vary within ``units'' (e.g., within-subjects, within-classrooms). Most researchers analyze categorical within-unit predictors with repeated-measures ANOVAs, but continuous within-unit predictors with linear mixed-effects models (LMEMs). We show that both types of predictor variables can be analyzed within the LMEM framework. We discuss designs with multiple sources of nonindependence, for example, studies in which the same subjects rate the same set of items or in which students nested in classrooms provide multiple answers. We provide clear guidelines about the types of random effects that should be included in the analysis of such designs. We also present a number of corrective steps that researchers can take when convergence fails in LMEM models with too many parameters. We end with a brief discussion on the trade-off between power and generalizability in designs with ``within-unit'' predictors.},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/YHA4SGV9/Brauer und Curtin - 2018 - Linear mixed-effects models and the analysis of no.pdf}
}

@article{brownIntroductionLinearMixedEffects2021,
  title = {An {{Introduction}} to {{Linear Mixed-Effects Modeling}} in {{R}}},
  author = {Brown, Violet A.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920960351},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920960351},
  urldate = {2022-07-14},
  abstract = {This Tutorial serves as both an approachable theoretical introduction to mixed-effects modeling and a practical introduction to how to implement mixed-effects models in R. The intended audience is researchers who have some basic statistical knowledge, but little or no experience implementing mixed-effects models in R using their own data. In an attempt to increase the accessibility of this Tutorial, I deliberately avoid using mathematical terminology beyond what a student would learn in a standard graduate-level statistics course, but I reference articles and textbooks that provide more detail for interested readers. This Tutorial includes snippets of R code throughout; the data and R script used to build the models described in the text are available via OSF at https://osf.io/v6qag/, so readers can follow along if they wish. The goal of this practical introduction is to provide researchers with the tools they need to begin implementing mixed-effects models in their own research.},
  langid = {english},
  keywords = {language,mixed-effects modeling,open data,R,speech perception},
  file = {/Users/timokoch/Zotero/storage/6ET46ETC/Brown - 2021 - An Introduction to Linear Mixed-Effects Modeling i.pdf}
}

@article{brysbaertPowerAnalysisEffect2018,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}}},
  shorttitle = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}},
  author = {Brysbaert, Marc and Stevens, Micha{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Cognition},
  volume = {1},
  number = {1},
  pages = {9},
  issn = {2514-4820},
  doi = {10.5334/joc.10},
  urldate = {2022-07-14},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/N9VC25CC/Brysbaert und Stevens - 2018 - Power Analysis and Effect Size in Mixed Effects Mo.pdf}
}

@misc{debruineFauxSimulationFactorial2023,
  title = {Faux: {{Simulation}} for {{Factorial Designs}}},
  shorttitle = {Faux},
  author = {DeBruine, Lisa},
  year = {2023},
  month = feb,
  doi = {10.5281/ZENODO.2669586},
  urldate = {2023-08-03},
  abstract = {Create datasets with factorial structure through simulation by specifying variable parameters.},
  copyright = {MIT License, Open Access},
  howpublished = {Zenodo},
  langid = {english}
}

@article{debruineUnderstandingMixedEffectsModels2021,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}}},
  author = {DeBruine, Lisa M. and Barr, Dale J.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920965119},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920965119},
  urldate = {2022-07-14},
  abstract = {Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed-effects models. However, much of this research is analyzed using analysis of variance on aggregated responses because researchers are not confident specifying and interpreting mixed-effects models. This Tutorial explains how to simulate data with random-effects structure and analyze the data using linear mixed-effects regression (with the lme4 R package), with a focus on interpreting the output in light of the simulated parameters. Data simulation not only can enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs. All materials associated with this article can be accessed at https://osf.io/3cz2e/.},
  langid = {english},
  keywords = {lme4,mixed-effects models,open materials,power,R,simulation},
  file = {/Users/timokoch/Zotero/storage/E6KAKUDI/DeBruine und Barr - 2021 - Understanding Mixed-Effects Models Through Data Si.pdf}
}

@article{greenSIMRPackagePower2016,
  title = {{{SIMR}}: An {{R}} Package for Power Analysis of Generalized Linear Mixed Models by Simulation},
  shorttitle = {{{SIMR}}},
  author = {Green, Peter and MacLeod, Catriona J.},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {4},
  pages = {493--498},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12504},
  urldate = {2023-04-24},
  abstract = {The r package simr allows users to calculate power for generalized linear mixed models from the lme4 package. The power calculations are based on Monte Carlo simulations. It includes tools for (i) running a power analysis for a given model and design; and (ii) calculating power curves to assess trade-offs between power and sample size. This paper presents a tutorial using a simple example of count data with mixed effects (with structure representative of environmental monitoring data) to guide the user along a gentle learning curve, adding only a few commands or options at a time.},
  langid = {english},
  keywords = {experimental design,glmm,Monte Carlo,random effects,sample size,type II error},
  file = {/Users/timokoch/Zotero/storage/YCQ47I3E/Green und MacLeod - 2016 - SIMR an R package for power analysis of generaliz.pdf;/Users/timokoch/Zotero/storage/CYYNEQY9/2041-210X.html}
}

@article{kumleEstimatingPowerGeneralized2021,
  title = {Estimating Power in (Generalized) Linear Mixed Models: {{An}} Open Introduction and Tutorial in {{R}}},
  shorttitle = {Estimating Power in (Generalized) Linear Mixed Models},
  author = {Kumle, Levi and V{\~o}, Melissa L.-H. and Draschkow, Dejan},
  year = {2021},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {53},
  number = {6},
  pages = {2528--2543},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01546-0},
  urldate = {2022-07-07},
  abstract = {Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to analytic power solutions are simulation-based power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
  langid = {english},
  keywords = {lme4,Mixed models,mixedpower,Power,R,Simulation},
  file = {/Users/timokoch/Zotero/storage/YR3EVKIK/Kumle et al. - 2021 - Estimating power in (generalized) linear mixed mod.pdf}
}

@article{lakensSampleSizeJustification2022,
  title = {Sample {{Size Justification}}},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = mar,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  urldate = {2023-04-24},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  file = {/Users/timokoch/Zotero/storage/RIYUXBC7/Lakens - 2022 - Sample Size Justification.pdf;/Users/timokoch/Zotero/storage/MMJADTTA/Sample-Size-Justification.html}
}

@article{lukeEvaluatingSignificanceLinear2017,
  title = {Evaluating Significance in Linear Mixed-Effects Models in {{R}}},
  author = {Luke, Steven G.},
  year = {2017},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {49},
  number = {4},
  pages = {1494--1502},
  issn = {1554-3528},
  doi = {10.3758/s13428-016-0809-y},
  urldate = {2023-04-24},
  abstract = {Mixed-effects models are being used ever more frequently in the analysis of experimental data. However, in the lme4 package in R the standards for evaluating significance of fixed effects in these models (i.e., obtaining p-values) are somewhat vague. There are good reasons for this, but as researchers who are using these models are required in many cases to report p-values, some method for evaluating the significance of the model output is needed. This paper reports the results of simulations showing that the two most common methods for evaluating significance, using likelihood ratio tests and applying the z distribution to the Wald t values from the model output (t-as-z), are somewhat anti-conservative, especially for smaller sample sizes. Other methods for evaluating significance, including parametric bootstrapping and the Kenward-Roger and Satterthwaite approximations for degrees of freedom, were also evaluated. The results of these simulations suggest that Type 1 error rates are closest to .05 when models are fitted using REML and p-values are derived using the Kenward-Roger or Satterthwaite approximations, as these approximations both produced acceptable Type 1 error rates even for smaller samples.},
  langid = {english},
  keywords = {Linear mixed-effects models,lme4,p-values,Statistics,Type 1 error},
  file = {/Users/timokoch/Zotero/storage/TLJVWC3F/Luke - 2017 - Evaluating significance in linear mixed-effects mo.pdf}
}

@article{meteyardBestPracticeGuidance2020,
  title = {Best Practice Guidance for Linear Mixed-Effects Models in Psychological Science},
  author = {Meteyard, Lotte and Davies, Robert A. I.},
  year = {2020},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {112},
  pages = {104092},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2020.104092},
  urldate = {2023-04-24},
  abstract = {The use of Linear Mixed-effects Models (LMMs) is set to dominate statistical analyses in psychological science and may become the default approach to analyzing quantitative data. The rapid growth in adoption of LMMs has been matched by a proliferation of differences in practice. Unless this diversity is recognized, and checked, the field shall reap enormous difficulties in the future when attempts are made to consolidate or synthesize research findings. Here we examine this diversity using two methods \textendash{} a survey of researchers (n~=~163) and a quasi-systematic review of papers using LMMs (n~=~400). The survey reveals substantive concerns among psychologists using or planning to use LMMs and an absence of agreed standards. The review of papers complements the survey, showing variation in how the models are built, how effects are evaluated and, most worryingly, how models are reported. Using these data as our departure point, we present a set of best practice guidance, focusing on the reporting of LMMs. It is the authors' intention that the paper supports a step-change in the reporting of LMMs across the psychological sciences, preventing a trajectory in which findings reported today cannot be transparently understood and used tomorrow.},
  langid = {english},
  keywords = {Hierarchical models,Linear mixed effects models,Multilevel models},
  file = {/Users/timokoch/Zotero/storage/I3I8TLAM/Meteyard und Davies - 2020 - Best practice guidance for linear mixed-effects mo.pdf;/Users/timokoch/Zotero/storage/WY8PX7FR/S0749596X20300061.html}
}

@incollection{singmannIntroductionMixedModels2019,
  title = {An {{Introduction}} to {{Mixed Models}} for {{Experimental Psychology}}},
  booktitle = {New {{Methods}} in {{Cognitive Psychology}}},
  author = {Singmann, Henrik and Kellen, David},
  editor = {Spieler, Daniel and Schumacher, Eric},
  year = {2019},
  month = oct,
  edition = {1},
  pages = {4--31},
  publisher = {{Routledge}},
  doi = {10.4324/9780429318405-2},
  urldate = {2023-04-24},
  isbn = {978-0-429-31840-5},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/CDEQRLAW/Singmann und Kellen - 2019 - An Introduction to Mixed Models for Experimental P.pdf}
}

@article{westfallStatisticalPowerOptimal2014,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  pages = {2020--2045},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222},
  doi = {10.1037/xge0000014},
  abstract = {Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Design,Sampling (Experimental),Statistical Power,Stimulus Parameters},
  file = {/Users/timokoch/Zotero/storage/HVBDJDCL/Westfall et al. - 2014 - Statistical power and optimal design in experiment.pdf;/Users/timokoch/Zotero/storage/HQ9U2NBA/doiLanding.html}
}

@misc{wickelmaierSimulatingPowerStatistical2022,
  title = {Simulating the {{Power}} of {{Statistical Tests}}: {{A Collection}} of {{R Examples}}},
  shorttitle = {Simulating the {{Power}} of {{Statistical Tests}}},
  author = {Wickelmaier, Florian},
  year = {2022},
  month = mar,
  number = {arXiv:2110.09836},
  eprint = {2110.09836},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.09836},
  urldate = {2023-05-30},
  abstract = {This paper illustrates how to calculate the power of a statistical test by computer simulation. It provides R code for power simulations of several classical inference procedures including one- and two-sample t tests, chi-squared tests, regression, and analysis of variance.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications},
  file = {/Users/timokoch/Zotero/storage/5EZR87BR/Wickelmaier - 2022 - Simulating the Power of Statistical Tests A Colle.pdf;/Users/timokoch/Zotero/storage/EDTJ448P/2110.html}
}

@misc{zimmerSampleSizePlanning2022,
  title = {Sample {{Size Planning}} for {{Complex Study Designs}}: {{A Tutorial}} for the Mlpwr {{Package}}},
  shorttitle = {Sample {{Size Planning}} for {{Complex Study Designs}}},
  author = {Zimmer, Felix and Henninger, Mirka and Debelak, Rudolf},
  year = {2022},
  month = oct,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/r9w6t},
  urldate = {2023-04-24},
  abstract = {A common challenge in designing empirical studies is determining an appropriate sample size. When more complex models are used, estimates of power can only be obtained using Monte Carlo simulations. In this tutorial, we introduce the R package mlpwr to perform simulation-based power analysis based on surrogate modeling. Surrogate modeling is a powerful tool to guide the search for study design parameters that imply a desired power or meet a cost threshold (e.g., in terms of monetary cost). mlpwr can be used to search for the optimal allocation when there are multiple design parameters, e.g., when balancing the number of participants and the number of groups in multilevel modeling. At the same time, the approach can take into account the cost of each design parameter, and aims to find a cost-efficient design. We introduce the basic functionality of the package, which can be applied to a wide range of statistical models and study designs. Additionally, we provide two examples based on empirical studies for illustration: one for sample size planning when using an item response theory model, and one for assigning the number of participants and the number of countries for a study using multilevel modeling.},
  langid = {american},
  keywords = {machine learning,power analysis,Quantitative Methods,sample size,simulation,Social and Behavioral Sciences},
  file = {/Users/timokoch/Zotero/storage/73M67QE6/Zimmer et al. - 2022 - Sample Size Planning for Complex Study Designs A .pdf}
}
