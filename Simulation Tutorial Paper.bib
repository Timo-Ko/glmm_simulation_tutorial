@article{albersWhenPowerAnalyses2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Experimental Social Psychology},
  volume = {74},
  pages = {187--195},
  issn = {00221031},
  doi = {10.1016/j.jesp.2017.09.004},
  urldate = {2023-08-08},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/DKBUKKFH/Albers und Lakens - 2018 - When power analyses based on pilot data are biased.pdf}
}

@article{arendStatisticalPowerTwolevel2019,
  title = {Statistical Power in Two-Level Models: {{A}} Tutorial Based on {{Monte Carlo}} Simulation.},
  shorttitle = {Statistical Power in Two-Level Models},
  author = {Arend, Matthias G. and Sch{\"a}fer, Thomas},
  year = {2019},
  month = feb,
  journal = {Psychological Methods},
  volume = {24},
  number = {1},
  pages = {1--19},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000195},
  urldate = {2023-08-08},
  langid = {english}
}

@article{batesFittingLinearMixedEffects2015,
  title = {Fitting {{Linear Mixed-Effects Models Using}} {\textbf{Lme4}}},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  urldate = {2022-07-14},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/HDSUVZ87/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf}
}

@article{brauerLinearMixedeffectsModels2018,
  title = {Linear Mixed-Effects Models and the Analysis of Nonindependent Data: {{A}} Unified Framework to Analyze Categorical and Continuous Independent Variables That Vary within-Subjects and/or within-Items.},
  shorttitle = {Linear Mixed-Effects Models and the Analysis of Nonindependent Data},
  author = {Brauer, Markus and Curtin, John J.},
  year = {2018},
  month = sep,
  journal = {Psychological Methods},
  volume = {23},
  number = {3},
  pages = {389--411},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000159},
  urldate = {2023-04-24},
  abstract = {In this article we address a number of important issues that arise in the analysis of nonindependent data. Such data are common in studies in which predictors vary within ``units'' (e.g., within-subjects, within-classrooms). Most researchers analyze categorical within-unit predictors with repeated-measures ANOVAs, but continuous within-unit predictors with linear mixed-effects models (LMEMs). We show that both types of predictor variables can be analyzed within the LMEM framework. We discuss designs with multiple sources of nonindependence, for example, studies in which the same subjects rate the same set of items or in which students nested in classrooms provide multiple answers. We provide clear guidelines about the types of random effects that should be included in the analysis of such designs. We also present a number of corrective steps that researchers can take when convergence fails in LMEM models with too many parameters. We end with a brief discussion on the trade-off between power and generalizability in designs with ``within-unit'' predictors.},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/YHA4SGV9/Brauer und Curtin - 2018 - Linear mixed-effects models and the analysis of no.pdf}
}

@article{brownIntroductionLinearMixedEffects2021,
  title = {An {{Introduction}} to {{Linear Mixed-Effects Modeling}} in {{R}}},
  author = {Brown, Violet A.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920960351},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920960351},
  urldate = {2022-07-14},
  abstract = {This Tutorial serves as both an approachable theoretical introduction to mixed-effects modeling and a practical introduction to how to implement mixed-effects models in R. The intended audience is researchers who have some basic statistical knowledge, but little or no experience implementing mixed-effects models in R using their own data. In an attempt to increase the accessibility of this Tutorial, I deliberately avoid using mathematical terminology beyond what a student would learn in a standard graduate-level statistics course, but I reference articles and textbooks that provide more detail for interested readers. This Tutorial includes snippets of R code throughout; the data and R script used to build the models described in the text are available via OSF at https://osf.io/v6qag/, so readers can follow along if they wish. The goal of this practical introduction is to provide researchers with the tools they need to begin implementing mixed-effects models in their own research.},
  langid = {english},
  keywords = {language,mixed-effects modeling,open data,R,speech perception},
  file = {/Users/timokoch/Zotero/storage/6ET46ETC/Brown - 2021 - An Introduction to Linear Mixed-Effects Modeling i.pdf}
}

@article{brysbaertPowerAnalysisEffect2018,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}}},
  shorttitle = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}},
  author = {Brysbaert, Marc and Stevens, Micha{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Cognition},
  volume = {1},
  number = {1},
  pages = {9},
  issn = {2514-4820},
  doi = {10.5334/joc.10},
  urldate = {2022-07-14},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/N9VC25CC/Brysbaert und Stevens - 2018 - Power Analysis and Effect Size in Mixed Effects Mo.pdf}
}

@misc{debruineFauxSimulationFactorial2023,
  title = {Faux: {{Simulation}} for {{Factorial Designs}}},
  shorttitle = {Faux},
  author = {DeBruine, Lisa},
  year = {2023},
  month = feb,
  doi = {10.5281/ZENODO.2669586},
  urldate = {2023-08-03},
  abstract = {Create datasets with factorial structure through simulation by specifying variable parameters.},
  copyright = {MIT License, Open Access},
  howpublished = {Zenodo},
  langid = {english}
}

@article{debruineUnderstandingMixedEffectsModels2021,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}}},
  author = {DeBruine, Lisa M. and Barr, Dale J.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920965119},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920965119},
  urldate = {2022-07-14},
  abstract = {Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed-effects models. However, much of this research is analyzed using analysis of variance on aggregated responses because researchers are not confident specifying and interpreting mixed-effects models. This Tutorial explains how to simulate data with random-effects structure and analyze the data using linear mixed-effects regression (with the lme4 R package), with a focus on interpreting the output in light of the simulated parameters. Data simulation not only can enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs. All materials associated with this article can be accessed at https://osf.io/3cz2e/.},
  langid = {english},
  keywords = {lme4,mixed-effects models,open materials,power,R,simulation},
  file = {/Users/timokoch/Zotero/storage/E6KAKUDI/DeBruine und Barr - 2021 - Understanding Mixed-Effects Models Through Data Si.pdf}
}

@article{greenSIMRPackagePower2016,
  title = {{{SIMR}}: An {{R}} Package for Power Analysis of Generalized Linear Mixed Models by Simulation},
  shorttitle = {{{SIMR}}},
  author = {Green, Peter and MacLeod, Catriona J.},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {4},
  pages = {493--498},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12504},
  urldate = {2023-04-24},
  abstract = {The r package simr allows users to calculate power for generalized linear mixed models from the lme4 package. The power calculations are based on Monte Carlo simulations. It includes tools for (i) running a power analysis for a given model and design; and (ii) calculating power curves to assess trade-offs between power and sample size. This paper presents a tutorial using a simple example of count data with mixed effects (with structure representative of environmental monitoring data) to guide the user along a gentle learning curve, adding only a few commands or options at a time.},
  langid = {english},
  keywords = {experimental design,glmm,Monte Carlo,random effects,sample size,type II error},
  file = {/Users/timokoch/Zotero/storage/YCQ47I3E/Green und MacLeod - 2016 - SIMR an R package for power analysis of generaliz.pdf;/Users/timokoch/Zotero/storage/CYYNEQY9/2041-210X.html}
}

@article{kumleEstimatingPowerGeneralized2021,
  title = {Estimating Power in (Generalized) Linear Mixed Models: {{An}} Open Introduction and Tutorial in {{R}}},
  shorttitle = {Estimating Power in (Generalized) Linear Mixed Models},
  author = {Kumle, Levi and V{\~o}, Melissa L.-H. and Draschkow, Dejan},
  year = {2021},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {53},
  number = {6},
  pages = {2528--2543},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01546-0},
  urldate = {2022-07-07},
  abstract = {Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to analytic power solutions are simulation-based power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
  langid = {english},
  keywords = {lme4,Mixed models,mixedpower,Power,R,Simulation},
  file = {/Users/timokoch/Zotero/storage/YR3EVKIK/Kumle et al. - 2021 - Estimating power in (generalized) linear mixed mod.pdf}
}

@article{lafitSelectionNumberParticipants2021,
  title = {Selection of the {{Number}} of {{Participants}} in {{Intensive Longitudinal Studies}}: {{A User-Friendly Shiny App}} and {{Tutorial}} for {{Performing Power Analysis}} in {{Multilevel Regression Models That Account}} for {{Temporal Dependencies}}},
  shorttitle = {Selection of the {{Number}} of {{Participants}} in {{Intensive Longitudinal Studies}}},
  author = {Lafit, Ginette and Adolf, Janne K. and Dejonckheere, Egon and {Myin-Germeys}, Inez and Viechtbauer, Wolfgang and Ceulemans, Eva},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {251524592097873},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245920978738},
  urldate = {2023-08-08},
  abstract = {In recent years, the popularity of procedures for collecting intensive longitudinal data, such as the experience-sampling method, has increased greatly. The data collected using such designs allow researchers to study the dynamics of psychological functioning and how these dynamics differ across individuals. To this end, the data are often modeled with multilevel regression models. An important question that arises when researchers design intensive longitudinal studies is how to determine the number of participants needed to test specific hypotheses regarding the parameters of these models with sufficient power. Power calculations for intensive longitudinal studies are challenging because of the hierarchical data structure in which repeated observations are nested within the individuals and because of the serial dependence that is typically present in these data. We therefore present a user-friendly application and step-by-step tutorial for performing simulation-based power analyses for a set of models that are popular in intensive longitudinal research. Because many studies use the same sampling protocol (i.e., a fixed number of at least approximately equidistant observations) within individuals, we assume that this protocol is fixed and focus on the number of participants. All included models explicitly account for the temporal dependencies in the data by assuming serially correlated errors or including autoregressive effects.},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/32ZYBH9U/Lafit et al. - 2021 - Selection of the Number of Participants in Intensi.pdf}
}

@misc{lakensImprovingYourStatistical2022,
  title = {Improving {{Your Statistical Inferences}}},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = apr,
  doi = {10.5281/ZENODO.6409077},
  urldate = {2023-08-09},
  abstract = {This open educational resource contains information to improve statistical inferences, design better experiments, and report scientific research more transparently.},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International, Open Access},
  howpublished = {Zenodo},
  keywords = {experimental design,frequentist statistics,hypothesis testing,open science,statistical inferences}
}

@article{lakensJustifyYourAlpha2018,
  title = {Justify Your Alpha},
  author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and {van Harmelen}, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and {de Oliveira}, Cilene Lino and {de Xivry}, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k{a}}tkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {3},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  urldate = {2022-03-17},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/Q2TZAF3Z/Lakens et al. - 2018 - Justify your alpha.pdf}
}

@article{lakensSampleSizeJustification2022,
  title = {Sample {{Size Justification}}},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = mar,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  urldate = {2023-04-24},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  file = {/Users/timokoch/Zotero/storage/RIYUXBC7/Lakens - 2022 - Sample Size Justification.pdf;/Users/timokoch/Zotero/storage/MMJADTTA/Sample-Size-Justification.html}
}

@article{lanePowerStrugglesEstimating2018,
  title = {Power Struggles: {{Estimating}} Sample Size for Multilevel Relationships Research},
  shorttitle = {Power Struggles},
  author = {Lane, Sean P. and Hennes, Erin P.},
  year = {2018},
  month = jan,
  journal = {Journal of Social and Personal Relationships},
  volume = {35},
  number = {1},
  pages = {7--31},
  issn = {0265-4075, 1460-3608},
  doi = {10.1177/0265407517710342},
  urldate = {2023-08-08},
  langid = {english}
}

@article{lukeEvaluatingSignificanceLinear2017,
  title = {Evaluating Significance in Linear Mixed-Effects Models in {{R}}},
  author = {Luke, Steven G.},
  year = {2017},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {49},
  number = {4},
  pages = {1494--1502},
  issn = {1554-3528},
  doi = {10.3758/s13428-016-0809-y},
  urldate = {2023-04-24},
  abstract = {Mixed-effects models are being used ever more frequently in the analysis of experimental data. However, in the lme4 package in R the standards for evaluating significance of fixed effects in these models (i.e., obtaining p-values) are somewhat vague. There are good reasons for this, but as researchers who are using these models are required in many cases to report p-values, some method for evaluating the significance of the model output is needed. This paper reports the results of simulations showing that the two most common methods for evaluating significance, using likelihood ratio tests and applying the z distribution to the Wald t values from the model output (t-as-z), are somewhat anti-conservative, especially for smaller sample sizes. Other methods for evaluating significance, including parametric bootstrapping and the Kenward-Roger and Satterthwaite approximations for degrees of freedom, were also evaluated. The results of these simulations suggest that Type 1 error rates are closest to .05 when models are fitted using REML and p-values are derived using the Kenward-Roger or Satterthwaite approximations, as these approximations both produced acceptable Type 1 error rates even for smaller samples.},
  langid = {english},
  keywords = {Linear mixed-effects models,lme4,p-values,Statistics,Type 1 error},
  file = {/Users/timokoch/Zotero/storage/TLJVWC3F/Luke - 2017 - Evaluating significance in linear mixed-effects mo.pdf}
}

@article{maierJustifyYourAlpha2022,
  title = {Justify {{Your Alpha}}: {{A Primer}} on {{Two Practical Approaches}}},
  shorttitle = {Justify {{Your Alpha}}},
  author = {Maier, Maximilian and Lakens, Dani{\"e}l},
  year = {2022},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {2},
  pages = {251524592210803},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/25152459221080396},
  urldate = {2022-12-20},
  abstract = {The default use of an alpha level of .05 is suboptimal for two reasons. First, decisions based on data can be made more efficiently by choosing an alpha level that minimizes the combined Type 1 and Type 2 error rate. Second, it is possible that in studies with very high statistical power, p values lower than the alpha level can be more likely when the null hypothesis is true than when the alternative hypothesis is true (i.e., Lindley's paradox). In this article, we explain two approaches that can be used to justify a better choice of an alpha level than relying on the default threshold of .05. The first approach is based on the idea to either minimize or balance Type 1 and Type 2 error rates. The second approach lowers the alpha level as a function of the sample size to prevent Lindley's paradox. An R package and Shiny app are provided to perform the required calculations. Both approaches have their limitations (e.g., the challenge of specifying relative costs and priors) but can offer an improvement to current practices, especially when sample sizes are large. The use of alpha levels that are better justified should improve statistical inferences and can increase the efficiency and informativeness of scientific research.},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/VWYQYDEV/Maier und Lakens - 2022 - Justify Your Alpha A Primer on Two Practical Appr.pdf}
}

@article{maxwellSampleSizePlanning2008,
  title = {Sample {{Size Planning}} for {{Statistical Power}} and {{Accuracy}} in {{Parameter Estimation}}},
  author = {Maxwell, Scott E. and Kelley, Ken and Rausch, Joseph R.},
  year = {2008},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {59},
  number = {1},
  pages = {537--563},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.59.103006.093735},
  urldate = {2023-08-08},
  abstract = {This review examines recent advances in sample size planning, not only from the perspective of an individual researcher, but also with regard to the goal of developing cumulative knowledge. Psychologists have traditionally thought of sample size planning in terms of power analysis. Although we review recent advances in power analysis, our main focus is the desirability of achieving accurate parameter estimates, either instead of or in addition to obtaining sufficient power. Accuracy in parameter estimation (AIPE) has taken on increasing importance in light of recent emphasis on effect size estimation and formation of confidence intervals. The review provides an overview of the logic behind sample size planning for AIPE and summarizes recent advances in implementing this approach in designs commonly used in psychological research.},
  langid = {english}
}

@article{meteyardBestPracticeGuidance2020,
  title = {Best Practice Guidance for Linear Mixed-Effects Models in Psychological Science},
  author = {Meteyard, Lotte and Davies, Robert A. I.},
  year = {2020},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {112},
  pages = {104092},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2020.104092},
  urldate = {2023-04-24},
  abstract = {The use of Linear Mixed-effects Models (LMMs) is set to dominate statistical analyses in psychological science and may become the default approach to analyzing quantitative data. The rapid growth in adoption of LMMs has been matched by a proliferation of differences in practice. Unless this diversity is recognized, and checked, the field shall reap enormous difficulties in the future when attempts are made to consolidate or synthesize research findings. Here we examine this diversity using two methods \textendash{} a survey of researchers (n~=~163) and a quasi-systematic review of papers using LMMs (n~=~400). The survey reveals substantive concerns among psychologists using or planning to use LMMs and an absence of agreed standards. The review of papers complements the survey, showing variation in how the models are built, how effects are evaluated and, most worryingly, how models are reported. Using these data as our departure point, we present a set of best practice guidance, focusing on the reporting of LMMs. It is the authors' intention that the paper supports a step-change in the reporting of LMMs across the psychological sciences, preventing a trajectory in which findings reported today cannot be transparently understood and used tomorrow.},
  langid = {english},
  keywords = {Hierarchical models,Linear mixed effects models,Multilevel models},
  file = {/Users/timokoch/Zotero/storage/I3I8TLAM/Meteyard und Davies - 2020 - Best practice guidance for linear mixed-effects mo.pdf;/Users/timokoch/Zotero/storage/WY8PX7FR/S0749596X20300061.html}
}

@article{murayamaSummarystatisticsbasedPowerAnalysis2022,
  title = {Summary-Statistics-Based Power Analysis: {{A}} New and Practical Method to Determine Sample Size for Mixed-Effects Modeling.},
  shorttitle = {Summary-Statistics-Based Power Analysis},
  author = {Murayama, Kou and Usami, Satoshi and Sakaki, Michiko},
  year = {2022},
  month = jan,
  journal = {Psychological Methods},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000330},
  urldate = {2023-08-07},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/YSRNCZBA/Murayama et al. - 2022 - Summary-statistics-based power analysis A new and.pdf}
}

@article{scherbaumEstimatingStatisticalPower2009,
  title = {Estimating {{Statistical Power}} and {{Required Sample Sizes}} for {{Organizational Research Using Multilevel Modeling}}},
  author = {Scherbaum, Charles A. and Ferreter, Jennifer M.},
  year = {2009},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {12},
  number = {2},
  pages = {347--367},
  issn = {1094-4281, 1552-7425},
  doi = {10.1177/1094428107308906},
  urldate = {2023-08-08},
  abstract = {The use of multilevel modeling to investigate organizational phenomena is rapidly increasing. Unfortunately, little advice is readily available for organizational researchers attempting to determine statistical power when using multilevel models or when determining sample sizes for each level that will maximize statistical power. This article presents an introduction to statistical power in multilevel models. The unique factors influencing power in multilevel models and calculations for estimating power for simple fixed effects, variance components, and cross-level interactions are presented. The results of simulation studies and the existing general rules of thumb are discussed, and the available power analysis software is reviewed.},
  langid = {english}
}

@incollection{singmannIntroductionMixedModels2019,
  title = {An {{Introduction}} to {{Mixed Models}} for {{Experimental Psychology}}},
  booktitle = {New {{Methods}} in {{Cognitive Psychology}}},
  author = {Singmann, Henrik and Kellen, David},
  editor = {Spieler, Daniel and Schumacher, Eric},
  year = {2019},
  month = oct,
  edition = {1},
  pages = {4--31},
  publisher = {{Routledge}},
  doi = {10.4324/9780429318405-2},
  urldate = {2023-04-24},
  isbn = {978-0-429-31840-5},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/CDEQRLAW/Singmann und Kellen - 2019 - An Introduction to Mixed Models for Experimental P.pdf}
}

@article{westfallStatisticalPowerOptimal2014,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  pages = {2020--2045},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222},
  doi = {10.1037/xge0000014},
  abstract = {Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Design,Sampling (Experimental),Statistical Power,Stimulus Parameters},
  file = {/Users/timokoch/Zotero/storage/HVBDJDCL/Westfall et al. - 2014 - Statistical power and optimal design in experiment.pdf;/Users/timokoch/Zotero/storage/HQ9U2NBA/doiLanding.html}
}

@misc{wickelmaierSimulatingPowerStatistical2022,
  title = {Simulating the {{Power}} of {{Statistical Tests}}: {{A Collection}} of {{R Examples}}},
  shorttitle = {Simulating the {{Power}} of {{Statistical Tests}}},
  author = {Wickelmaier, Florian},
  year = {2022},
  month = mar,
  number = {arXiv:2110.09836},
  eprint = {2110.09836},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.09836},
  urldate = {2023-05-30},
  abstract = {This paper illustrates how to calculate the power of a statistical test by computer simulation. It provides R code for power simulations of several classical inference procedures including one- and two-sample t tests, chi-squared tests, regression, and analysis of variance.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications},
  file = {/Users/timokoch/Zotero/storage/5EZR87BR/Wickelmaier - 2022 - Simulating the Power of Statistical Tests A Colle.pdf;/Users/timokoch/Zotero/storage/EDTJ448P/2110.html}
}

@article{yarkoniGeneralizabilityCrisis2022,
  title = {The Generalizability Crisis},
  author = {Yarkoni, Tal},
  year = {2022},
  journal = {Behavioral and Brain Sciences},
  volume = {45},
  pages = {e1},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X20001685},
  urldate = {2022-11-30},
  abstract = {Abstract             Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned \textendash{} that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology \textendash{} the linear mixed model \textendash{} I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that although the ``random effect'' formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/Z4TK4K3B/Yarkoni - 2022 - The generalizability crisis.pdf}
}

@article{yarkoniRepliesCommentariesGeneralizability2022,
  title = {Replies to Commentaries on the Generalizability Crisis},
  author = {Yarkoni, Tal},
  year = {2022},
  journal = {Behavioral and Brain Sciences},
  volume = {45},
  pages = {e40},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X21001758},
  urldate = {2022-06-08},
  abstract = {Abstract             The 38 commentaries on the target article span a broad range of disciplines and perspectives. I have organized my response to the commentaries around three broad questions: First, how serious are the problems discussed in the target article? Second, are there are other, potentially more productive, ways to think about the issues that the target article framed in terms of generalizability? And third, what, if anything, should we collectively do about these problems?},
  langid = {english}
}

@misc{zimmerSampleSizePlanning2022,
  title = {Sample {{Size Planning}} for {{Complex Study Designs}}: {{A Tutorial}} for the Mlpwr {{Package}}},
  shorttitle = {Sample {{Size Planning}} for {{Complex Study Designs}}},
  author = {Zimmer, Felix and Henninger, Mirka and Debelak, Rudolf},
  year = {2022},
  month = oct,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/r9w6t},
  urldate = {2023-04-24},
  abstract = {A common challenge in designing empirical studies is determining an appropriate sample size. When more complex models are used, estimates of power can only be obtained using Monte Carlo simulations. In this tutorial, we introduce the R package mlpwr to perform simulation-based power analysis based on surrogate modeling. Surrogate modeling is a powerful tool to guide the search for study design parameters that imply a desired power or meet a cost threshold (e.g., in terms of monetary cost). mlpwr can be used to search for the optimal allocation when there are multiple design parameters, e.g., when balancing the number of participants and the number of groups in multilevel modeling. At the same time, the approach can take into account the cost of each design parameter, and aims to find a cost-efficient design. We introduce the basic functionality of the package, which can be applied to a wide range of statistical models and study designs. Additionally, we provide two examples based on empirical studies for illustration: one for sample size planning when using an item response theory model, and one for assigning the number of participants and the number of countries for a study using multilevel modeling.},
  langid = {american},
  keywords = {machine learning,power analysis,Quantitative Methods,sample size,simulation,Social and Behavioral Sciences},
  file = {/Users/timokoch/Zotero/storage/73M67QE6/Zimmer et al. - 2022 - Sample Size Planning for Complex Study Designs A .pdf}
}
