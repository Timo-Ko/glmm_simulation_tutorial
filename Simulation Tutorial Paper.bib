@article{albersWhenPowerAnalyses2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Experimental Social Psychology},
  volume = {74},
  pages = {187--195},
  issn = {00221031},
  doi = {10.1016/j.jesp.2017.09.004},
  urldate = {2023-08-08},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/DKBUKKFH/Albers und Lakens - 2018 - When power analyses based on pilot data are biased.pdf}
}

@article{albersWhenPowerAnalyses2018a,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Experimental Social Psychology},
  volume = {74},
  pages = {187--195},
  issn = {00221031},
  doi = {10.1016/j.jesp.2017.09.004},
  urldate = {2024-06-05},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/TG2I3YC4/Albers und Lakens - 2018 - When power analyses based on pilot data are biased.pdf}
}

@article{arendStatisticalPowerTwolevel2019,
  title = {Statistical Power in Two-Level Models: {{A}} Tutorial Based on {{Monte Carlo}} Simulation.},
  shorttitle = {Statistical Power in Two-Level Models},
  author = {Arend, Matthias G. and Sch{\"a}fer, Thomas},
  year = {2019},
  month = feb,
  journal = {Psychological Methods},
  volume = {24},
  number = {1},
  pages = {1--19},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000195},
  urldate = {2023-08-08},
  langid = {english}
}

@article{barangerTutorialPowerAnalyses2023,
  title = {Tutorial: {{Power Analyses}} for {{Interaction Effects}} in {{Cross-Sectional Regressions}}},
  shorttitle = {Tutorial},
  author = {Baranger, David A. A. and Finsaas, Megan C. and Goldstein, Brandon L. and Vize, Colin E. and Lynam, Donald R. and Olino, Thomas M.},
  year = {2023},
  month = jul,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {6},
  number = {3},
  pages = {25152459231187531},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459231187531},
  urldate = {2023-10-19},
  abstract = {Interaction analyses (also termed ``moderation'' analyses or ``moderated multiple regression'') are a form of linear regression analysis designed to test whether the association between two variables changes when conditioned on a third variable. It can be challenging to perform a power analysis for interactions with existing software, particularly when variables are correlated and continuous. Moreover, although power is affected by main effects, their correlation, and variable reliability, it can be unclear how to incorporate these effects into a power analysis. The R package InteractionPoweR and associated Shiny apps allow researchers with minimal or no programming experience to perform analytic and simulation-based power analyses for interactions. At minimum, these analyses require the Pearson's correlation between variables and sample size, and additional parameters, including reliability and the number of discrete levels that a variable takes (e.g., binary or Likert scale), can optionally be specified. In this tutorial, we demonstrate how to perform power analyses using our package and give examples of how power can be affected by main effects, correlations between main effects, reliability, and variable distributions. We also include a brief discussion of how researchers may select an appropriate interaction effect size when performing a power analysis.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/PFS2S9BF/Baranger et al. - 2023 - Tutorial Power Analyses for Interaction Effects i.pdf}
}

@article{batesFittingLinearMixedEffects2015,
  title = {Fitting {{Linear Mixed-Effects Models Using}} {\textbf{Lme4}}},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  urldate = {2022-07-14},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/HDSUVZ87/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf}
}

@article{benjaminRedefineStatisticalSignificance2017,
  title = {Redefine Statistical Significance},
  author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"o}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and De Boeck, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Hua Ho, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'o}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"o}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Van Zandt, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
  year = {2017},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {1},
  pages = {6--10},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0189-z},
  urldate = {2024-06-25},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/NQMPH2DV/Benjamin et al. - 2017 - Redefine statistical significance.pdf}
}

@incollection{bolkerLinearGeneralizedLinear2015,
  title = {Linear and Generalized Linear Mixed Models},
  booktitle = {Ecological {{Statistics}}},
  author = {Bolker, Benjamin M.},
  editor = {Fox, Gordon A. and {Negrete-Yankelevich}, Simoneta and Sosa, Vinicio J.},
  year = {2015},
  month = jan,
  edition = {1},
  pages = {309--333},
  publisher = {Oxford University PressOxford},
  doi = {10.1093/acprof:oso/9780199672547.003.0014},
  urldate = {2024-06-19},
  abstract = {Abstract             Generalized linear mixed models (GLMMs) are a powerful class of statistical models that combine the characteristics of generalized linear models and mixed models (models with both fixed and random predictor variables). This chapter: reviews the conceptual and theoretical background of GLMMs, focusing on the definition and meaning of random effects; gives basic guidelines and syntax for setting up a mixed model; and discusses the theoretical and practical details of estimating parameters, diagnosing problems with a model, and making statistical inferences (finding confidence intervals, estimating p values, and doing model selection) for GLMMs.},
  isbn = {978-0-19-967255-4 978-0-19-967254-7 978-0-19-179648-7},
  langid = {english}
}

@article{brauerLinearMixedeffectsModels2018,
  title = {Linear Mixed-Effects Models and the Analysis of Nonindependent Data: {{A}} Unified Framework to Analyze Categorical and Continuous Independent Variables That Vary within-Subjects and/or within-Items.},
  shorttitle = {Linear Mixed-Effects Models and the Analysis of Nonindependent Data},
  author = {Brauer, Markus and Curtin, John J.},
  year = {2018},
  month = sep,
  journal = {Psychological Methods},
  volume = {23},
  number = {3},
  pages = {389--411},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000159},
  urldate = {2023-04-24},
  abstract = {In this article we address a number of important issues that arise in the analysis of nonindependent data. Such data are common in studies in which predictors vary within ``units'' (e.g., within-subjects, within-classrooms). Most researchers analyze categorical within-unit predictors with repeated-measures ANOVAs, but continuous within-unit predictors with linear mixed-effects models (LMEMs). We show that both types of predictor variables can be analyzed within the LMEM framework. We discuss designs with multiple sources of nonindependence, for example, studies in which the same subjects rate the same set of items or in which students nested in classrooms provide multiple answers. We provide clear guidelines about the types of random effects that should be included in the analysis of such designs. We also present a number of corrective steps that researchers can take when convergence fails in LMEM models with too many parameters. We end with a brief discussion on the trade-off between power and generalizability in designs with ``within-unit'' predictors.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/YHA4SGV9/Brauer und Curtin - 2018 - Linear mixed-effects models and the analysis of no.pdf}
}

@article{brooksGlmmTMBBalancesSpeed2017,
  title = {{{glmmTMB Balances Speed}} and {{Flexibility Among Packages}} for {{Zero-inflated Generalized Linear Mixed Modeling}}},
  author = {Brooks, E., Mollie and Kristensen, Kasper and Benthem, J.,van, Koen and Magnusson, Arni and Berg, W., Casper and Nielsen, Anders and Skaug, J., Hans and M{\"a}chler, Martin and Bolker, M., Benjamin},
  year = {2017},
  journal = {The R Journal},
  volume = {9},
  number = {2},
  pages = {378},
  issn = {2073-4859},
  doi = {10.32614/RJ-2017-066},
  urldate = {2024-05-24},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/VH7V9LQ4/Brooks et al. - 2017 - glmmTMB Balances Speed and Flexibility Among Packa.pdf}
}

@article{brownIntroductionLinearMixedEffects2021,
  title = {An {{Introduction}} to {{Linear Mixed-Effects Modeling}} in {{R}}},
  author = {Brown, Violet A.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920960351},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920960351},
  urldate = {2022-07-14},
  abstract = {This Tutorial serves as both an approachable theoretical introduction to mixed-effects modeling and a practical introduction to how to implement mixed-effects models in R. The intended audience is researchers who have some basic statistical knowledge, but little or no experience implementing mixed-effects models in R using their own data. In an attempt to increase the accessibility of this Tutorial, I deliberately avoid using mathematical terminology beyond what a student would learn in a standard graduate-level statistics course, but I reference articles and textbooks that provide more detail for interested readers. This Tutorial includes snippets of R code throughout; the data and R script used to build the models described in the text are available via OSF at https://osf.io/v6qag/, so readers can follow along if they wish. The goal of this practical introduction is to provide researchers with the tools they need to begin implementing mixed-effects models in their own research.},
  langid = {english},
  keywords = {language,mixed-effects modeling,open data,R,speech perception},
  file = {/Users/florianpargent/Zotero/storage/6ET46ETC/Brown - 2021 - An Introduction to Linear Mixed-Effects Modeling i.pdf}
}

@article{brysbaertPowerAnalysisEffect2018,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}}},
  shorttitle = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}},
  author = {Brysbaert, Marc and Stevens, Micha{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Cognition},
  volume = {1},
  number = {1},
  pages = {9},
  issn = {2514-4820},
  doi = {10.5334/joc.10},
  urldate = {2022-07-14},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/N9VC25CC/Brysbaert und Stevens - 2018 - Power Analysis and Effect Size in Mixed Effects Mo.pdf}
}

@article{burknerAdvancedBayesianMultilevel2018,
  title = {Advanced {{Bayesian Multilevel Modeling}} with the {{R Package}} Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2018},
  journal = {The R Journal},
  volume = {10},
  number = {1},
  pages = {395},
  issn = {2073-4859},
  doi = {10.32614/RJ-2018-017},
  urldate = {2024-05-24},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/XCMX8UX7/Bürkner - 2018 - Advanced Bayesian Multilevel Modeling with the R P.pdf}
}

@article{burknerBrmsPackageBayesian2017,
  title = {Brms: {{An R Package}} for {{Bayesian Multilevel Models Using Stan}}},
  shorttitle = {Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2017},
  month = aug,
  journal = {Journal of Statistical Software},
  volume = {80},
  pages = {1--28},
  issn = {1548-7660},
  doi = {10.18637/jss.v080.i01},
  urldate = {2023-08-11},
  abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit  -  among others  -  linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as meta-analytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation.},
  copyright = {Copyright (c) 2017 Paul-Christian B{\"u}rkner},
  langid = {english},
  keywords = {Bayesian inference,MCMC,multilevel model,ordinal data,R,Stan},
  file = {/Users/florianpargent/Zotero/storage/466XWGHA/Bürkner - 2017 - brms An R Package for Bayesian Multilevel Models .pdf}
}

@article{buttonPowerFailureWhy2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3475},
  urldate = {2024-06-19},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/5E52KTY6/Button et al. - 2013 - Power failure why small sample size undermines th.pdf}
}

@article{camererEvaluatingReplicabilitySocial2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = aug,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637--644},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  urldate = {2024-06-25},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/C7X5UJ49/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf}
}

@article{chambersPresentFutureRegistered2022,
  title = {The Past, Present and Future of {{Registered Reports}}},
  author = {Chambers, Christopher D. and Tzavella, Loukia},
  year = {2022},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {6},
  number = {1},
  pages = {29--42},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01193-7},
  urldate = {2023-09-11},
  abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Culture,Publishing},
  file = {/Users/florianpargent/Zotero/storage/7VLRUK5E/Chambers und Tzavella - 2022 - The past, present and future of Registered Reports.pdf}
}

@article{champelyPackagePwr2018,
  title = {Package `Pwr'},
  author = {Champely, Stephane and Ekstrom, Claus and Dalgaard, Peter and Gill, Jeffrey and Weibelzahl, Stephan and Anandkumar, Aditya and Ford, Clay and Volcic, Robert and De Rosario, Helios and De Rosario, Maintainer Helios},
  year = {2018},
  journal = {R package version},
  volume = {1},
  number = {2},
  keywords = {No DOI found}
}

@article{choTwotailedTestingDirectional2013,
  title = {Is Two-Tailed Testing for Directional Research Hypotheses Tests Legitimate?},
  author = {Cho, Hyun-Chul and Abe, Shuzo},
  year = {2013},
  month = sep,
  journal = {Journal of Business Research},
  volume = {66},
  number = {9},
  pages = {1261--1266},
  issn = {01482963},
  doi = {10.1016/j.jbusres.2012.02.023},
  urldate = {2024-06-05},
  langid = {english}
}

@article{cockburnThreatsReplicationCrisis2020,
  title = {Threats of a Replication Crisis in Empirical Computer Science},
  author = {Cockburn, Andy and Dragicevic, Pierre and Besan{\c c}on, Lonni and Gutwin, Carl},
  year = {2020},
  month = jul,
  journal = {Communications of the ACM},
  volume = {63},
  number = {8},
  pages = {70--79},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3360311},
  urldate = {2023-09-05},
  abstract = {Research replication only works if there is confidence built into the results.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/APYYCZ6X/Cockburn et al. - 2020 - Threats of a replication crisis in empirical compu.pdf}
}

@article{cohenPowerPrimer1992,
  title = {A Power Primer.},
  author = {Cohen, Jacob},
  year = {1992},
  journal = {Psychological Bulletin},
  volume = {112},
  number = {1},
  pages = {155--159},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/0033-2909.112.1.155},
  urldate = {2024-06-04},
  langid = {english}
}

@article{cummingNewStatisticsWhy2014,
  title = {The {{New Statistics}}: {{Why}} and {{How}}},
  shorttitle = {The {{New Statistics}}},
  author = {Cumming, Geoff},
  year = {2014},
  month = jan,
  journal = {Psychological Science},
  volume = {25},
  number = {1},
  pages = {7--29},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797613504966},
  urldate = {2024-06-04},
  abstract = {We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight-step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.},
  langid = {english}
}

@misc{debruineFauxSimulationFactorial2023,
  title = {Faux: {{Simulation}} for {{Factorial Designs}}},
  shorttitle = {Faux},
  author = {DeBruine, Lisa},
  year = {2023},
  month = feb,
  doi = {10.5281/ZENODO.2669586},
  urldate = {2023-08-03},
  abstract = {Create datasets with factorial structure through simulation by specifying variable parameters.},
  copyright = {MIT License, Open Access},
  howpublished = {Zenodo},
  langid = {english}
}

@misc{debruineFauxSimulationFactorial2023a,
  title = {Faux: {{Simulation}} for {{Factorial Designs}}},
  shorttitle = {Faux},
  author = {DeBruine, Lisa},
  year = {2023},
  month = feb,
  doi = {10.5281/ZENODO.2669586},
  urldate = {2023-08-18},
  abstract = {Create datasets with factorial structure through simulation by specifying variable parameters.},
  copyright = {MIT License, Open Access},
  howpublished = {Zenodo},
  langid = {english}
}

@article{debruineUnderstandingMixedEffectsModels2021,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}}},
  author = {DeBruine, Lisa and Barr, Dale J.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920965119},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920965119},
  urldate = {2022-07-14},
  abstract = {Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed-effects models. However, much of this research is analyzed using analysis of variance on aggregated responses because researchers are not confident specifying and interpreting mixed-effects models. This Tutorial explains how to simulate data with random-effects structure and analyze the data using linear mixed-effects regression (with the lme4 R package), with a focus on interpreting the output in light of the simulated parameters. Data simulation not only can enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs. All materials associated with this article can be accessed at https://osf.io/3cz2e/.},
  langid = {english},
  keywords = {lme4,mixed-effects models,open materials,power,R,simulation},
  file = {/Users/florianpargent/Zotero/storage/E6KAKUDI/DeBruine und Barr - 2021 - Understanding Mixed-Effects Models Through Data Si.pdf}
}

@article{deffnerCausalFrameworkCrossCultural2022,
  title = {A {{Causal Framework}} for {{Cross-Cultural Generalizability}}},
  author = {Deffner, Dominik and Rohrer, Julia M. and McElreath, Richard},
  year = {2022},
  month = jul,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {3},
  pages = {251524592211063},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/25152459221106366},
  urldate = {2022-12-01},
  abstract = {Behavioral researchers increasingly recognize the need for more diverse samples that capture the breadth of human experience. Current attempts to establish generalizability across populations focus on threats to validity, constraints on generalization, and the accumulation of large, cross-cultural data sets. But for continued progress, we also require a framework that lets us determine which inferences can be drawn and how to make informative cross-cultural comparisons. We describe a generative causal-modeling framework and outline simple graphical criteria to derive analytic strategies and implied generalizations. Using both simulated and real data, we demonstrate how to project and compare estimates across populations and further show how to formally represent measurement equivalence or inequivalence across societies. We conclude with a discussion of how a formal framework for generalizability can assist researchers in designing more informative cross-cultural studies and thus provides a more solid foundation for cumulative and generalizable behavioral research.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/CJGZZLL8/Deffner et al. - 2022 - A Causal Framework for Cross-Cultural Generalizabi.pdf}
}

@article{dmitrienkoTraditionalMultiplicityAdjustment2013,
  title = {Traditional Multiplicity Adjustment Methods in Clinical Trials},
  author = {Dmitrienko, Alex and D'Agostino, Ralph},
  year = {2013},
  month = dec,
  journal = {Statistics in Medicine},
  volume = {32},
  number = {29},
  pages = {5172--5218},
  issn = {02776715},
  doi = {10.1002/sim.5990},
  urldate = {2023-08-18},
  langid = {english}
}

@article{ebersoleManyLabsTesting2020,
  title = {Many {{Labs}} 5: {{Testing Pre-Data-Collection Peer Review}} as an {{Intervention}} to {{Increase Replicability}}},
  shorttitle = {Many {{Labs}} 5},
  author = {Ebersole, Charles R. and Mathur, Maya B. and Baranski, Erica and {Bart-Plange}, Diane-Jo and Buttrick, Nicholas R. and Chartier, Christopher R. and Corker, Katherine S. and Corley, Martin and Hartshorne, Joshua K. and IJzerman, Hans and Lazarevi{\'c}, Ljiljana B. and Rabagliati, Hugh and Ropovik, Ivan and Aczel, Balazs and Aeschbach, Lena F. and Andrighetto, Luca and Arnal, Jack D. and Arrow, Holly and Babincak, Peter and Bakos, Bence E. and Ban{\'i}k, Gabriel and Baskin, Ernest and Belopavlovi{\'c}, Radomir and Bernstein, Michael H. and Bia{\l}ek, Micha{\l} and Bloxsom, Nicholas G. and Bodro{\v z}a, Bojana and Bonfiglio, Diane B. V. and Boucher, Leanne and Br{\"u}hlmann, Florian and Brumbaugh, Claudia C. and Casini, Erica and Chen, Yiling and Chiorri, Carlo and Chopik, William J. and Christ, Oliver and Ciunci, Antonia M. and Claypool, Heather M. and Coary, Sean and {\v C}oli{\'c}, Marija V. and Collins, W. Matthew and Curran, Paul G. and Day, Chris R. and Dering, Benjamin and Dreber, Anna and Edlund, John E. and Falc{\~a}o, Filipe and Fedor, Anna and Feinberg, Lily and Ferguson, Ian R. and Ford, M{\'a}ire and Frank, Michael C. and Fryberger, Emily and Garinther, Alexander and Gawryluk, Katarzyna and Ashbaugh, Kayla and Giacomantonio, Mauro and Giessner, Steffen R. and Grahe, Jon E. and Guadagno, Rosanna E. and Ha{\l}asa, Ewa and Hancock, Peter J. B. and Hilliard, Rias A. and H{\"u}ffmeier, Joachim and Hughes, Sean and Idzikowska, Katarzyna and Inzlicht, Michael and Jern, Alan and {Jim{\'e}nez-Leal}, William and Johannesson, Magnus and {Joy-Gaba}, Jennifer A. and Kauff, Mathias and Kellier, Danielle J. and Kessinger, Grecia and Kidwell, Mallory C. and Kimbrough, Amanda M. and King, Josiah P. J. and Kolb, Vanessa S. and Ko{\l}odziej, Sabina and Kovacs, Marton and Krasuska, Karolina and Kraus, Sue and Krueger, Lacy E. and Kuchno, Katarzyna and Lage, Caio Ambrosio and Langford, Eleanor V. and Levitan, Carmel A. and De Lima, Tiago Jess{\'e} Souza and Lin, Hause and Lins, Samuel and Loy, Jia E. and Manfredi, Dylan and Markiewicz, {\L}ukasz and Menon, Madhavi and Mercier, Brett and Metzger, Mitchell and Meyet, Venus and Millen, Ailsa E. and Miller, Jeremy K. and Montealegre, Andres and Moore, Don A. and Muda, Rafa{\l} and Nave, Gideon and Nichols, Austin Lee and Novak, Sarah A. and Nunnally, Christian and Orli{\'c}, Ana and Palinkas, Anna and Panno, Angelo and Parks, Kimberly P. and Pedovi{\'c}, Ivana and P{\k e}kala, Emilian and Penner, Matthew R. and Pessers, Sebastiaan and Petrovi{\'c}, Boban and Pfeiffer, Thomas and Pie{\'n}kosz, Damian and Preti, Emanuele and Puri{\'c}, Danka and Ramos, Tiago and Ravid, Jonathan and Razza, Timothy S. and Rentzsch, Katrin and Richetin, Juliette and Rife, Sean C. and Rosa, Anna Dalla and Rudy, Kaylis Hase and Salamon, Janos and Saunders, Blair and Sawicki, Przemys{\l}aw and Schmidt, Kathleen and Schuepfer, Kurt and Schultze, Thomas and {Schulz-Hardt}, Stefan and Sch{\"u}tz, Astrid and Shabazian, Ani N. and Shubella, Rachel L. and Siegel, Adam and Silva, R{\'u}ben and Sioma, Barbara and Skorb, Lauren and De Souza, Luana Elayne Cunha and Steegen, Sara and Stein, L. A. R. and Sternglanz, R. Weylin and Stojilovi{\'c}, Darko and Storage, Daniel and Sullivan, Gavin Brent and Szaszi, Barnabas and Szecsi, Peter and Sz{\"o}ke, Orsolya and Szuts, Attila and Thomae, Manuela and Tidwell, Natasha D. and Tocco, Carly and Torka, Ann-Kathrin and Tuerlinckx, Francis and Vanpaemel, Wolf and Vaughn, Leigh Ann and Vianello, Michelangelo and Viganola, Domenico and Vlachou, Maria and Walker, Ryan J. and Weissgerber, Sophia C. and Wichman, Aaron L. and Wiggins, Bradford J. and Wolf, Daniel and Wood, Michael J. and Zealley, David and {\v Z}e{\v z}elj, Iris and Zrubka, Mark and Nosek, Brian A.},
  year = {2020},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {3},
  pages = {309--331},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245920958687},
  urldate = {2024-06-29},
  abstract = {Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology (RP:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect ( p {$<$} .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these RP:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3--9; median total sample = 1,279.5, range = 276--3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols ({$\Delta$} r = .002 or .014, depending on analytic approach). The median effect size for the revised protocols ( r = .05) was similar to that of the RP:P protocols ( r = .04) and the original RP:P replications ( r = .11), and smaller than that of the original studies ( r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00--.15) were 78\% smaller, on average, than the original effect sizes (median r = .37, range = .19--.50).},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/A3AWR8JP/Ebersole et al. - 2020 - Many Labs 5 Testing Pre-Data-Collection Peer Revi.pdf}
}

@inproceedings{echtlerOpenSourceOpen2018,
  title = {Open {{Source}}, {{Open Science}}, and the {{Replication Crisis}} in {{HCI}}},
  booktitle = {Extended {{Abstracts}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Echtler, Florian and H{\"a}u{\ss}ler, Maximilian},
  year = {2018},
  month = apr,
  pages = {1--8},
  publisher = {ACM},
  address = {Montreal QC Canada},
  doi = {10.1145/3170427.3188395},
  urldate = {2023-09-12},
  abstract = {The open-source model of software development is an established and widely used method that has been making inroads into several scientific disciplines which use software, thereby also helping much-needed efforts at replication of scientific results. However, our own discipline of HCI does not seem to follow this trend so far. We analyze the entire body of papers from CHI 2016 and CHI 2017 regarding open-source releases, and compare our results with the discipline of bioinformatics. Based on our comparison, we suggest future directions for publication practices in HCI in order to improve scientific rigor and replicability.},
  isbn = {978-1-4503-5621-3},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/PPQ5ATYT/Echtler und Häußler - 2018 - Open Source, Open Science, and the Replication Cri.pdf}
}

@inproceedings{echtlerOpenSourceOpen2018a,
  title = {Open {{Source}}, {{Open Science}}, and the {{Replication Crisis}} in {{HCI}}},
  booktitle = {Extended {{Abstracts}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Echtler, Florian and H{\"a}u{\ss}ler, Maximilian},
  year = {2018},
  month = apr,
  series = {{{CHI EA}} '18},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3170427.3188395},
  urldate = {2023-09-05},
  abstract = {The open-source model of software development is an established and widely used method that has been making inroads into several scientific disciplines which use software, thereby also helping much-needed efforts at replication of scientific results. However, our own discipline of HCI does not seem to follow this trend so far. We analyze the entire body of papers from CHI 2016 and CHI 2017 regarding open-source releases, and compare our results with the discipline of bioinformatics. Based on our comparison, we suggest future directions for publication practices in HCI in order to improve scientific rigor and replicability.},
  isbn = {978-1-4503-5621-3},
  keywords = {hci,open science,open source,replication}
}

@article{endersSimpleMonteCarlo2023,
  title = {A Simple {{Monte Carlo}} Method for Estimating Power in Multilevel Designs.},
  author = {Enders, Craig K. and Keller, Brian T. and Woller, Michael P.},
  year = {2023},
  month = nov,
  journal = {Psychological Methods},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000614},
  urldate = {2024-06-24},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/MNLX48DJ/Enders et al. - 2023 - A simple Monte Carlo method for estimating power i.pdf}
}

@book{fahrmeirRegressionModelsMethods2021,
  title = {Regression: {{Models}}, {{Methods}} and {{Applications}}},
  shorttitle = {Regression},
  author = {Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan and Marx, Brian D.},
  year = {2021},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-63882-8},
  urldate = {2023-08-18},
  isbn = {978-3-662-63881-1 978-3-662-63882-8},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/P75BJQUB/Fahrmeir et al. - 2021 - Regression Models, Methods and Applications.pdf}
}

@article{faulStatisticalPowerAnalyses2009,
  title = {Statistical Power Analyses Using {{G}}*{{Power}} 3.1: {{Tests}} for Correlation and Regression Analyses},
  shorttitle = {Statistical Power Analyses Using {{G}}*{{Power}} 3.1},
  author = {Faul, Franz and Erdfelder, Edgar and Buchner, Axel and Lang, Albert-Georg},
  year = {2009},
  month = nov,
  journal = {Behavior Research Methods},
  volume = {41},
  number = {4},
  pages = {1149--1160},
  issn = {1554-3528},
  doi = {10.3758/BRM.41.4.1149},
  urldate = {2023-09-05},
  abstract = {G*Power is a free power analysis program for a variety of statistical tests. We present extensions and improvements of the version introduced by Faul, Erdfelder, Lang, and Buchner (2007) in the domain of correlation and regression analyses. In the new version, we have added procedures to analyze the power of tests based on (1) single-sample tetrachoric correlations, (2) comparisons of dependent correlations, (3) bivariate linear regression, (4) multiple linear regression based on the random predictor model, (5) logistic regression, and (6) Poisson regression. We describe these new features and provide a brief introduction to their scope and handling.},
  langid = {english},
  keywords = {Effect Size Measure,Implicit Association Test,Linear Multiple Regression,Multiple Correlation Coefficient,Noncentrality Parameter},
  file = {/Users/florianpargent/Zotero/storage/LQK3YIMQ/Faul et al. - 2009 - Statistical power analyses using GPower 3.1 Test.pdf}
}

@misc{gelmanBayesianWorkflow2020,
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  number = {arXiv:2011.01808},
  eprint = {2011.01808},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2024-04-09},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/florianpargent/Zotero/storage/W3HH4KJZ/Gelman et al. - 2020 - Bayesian Workflow.pdf;/Users/florianpargent/Zotero/storage/ULBVNABH/2011.html}
}

@article{gomilaMissingDataExperiments2022,
  title = {Missing Data in Experiments: {{Challenges}} and Solutions.},
  shorttitle = {Missing Data in Experiments},
  author = {Gomila, Robin and Clark, Chelsey S.},
  year = {2022},
  month = apr,
  journal = {Psychological Methods},
  volume = {27},
  number = {2},
  pages = {143--155},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000361},
  urldate = {2024-06-05},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/NKBHU5FI/Gomila und Clark - 2022 - Missing data in experiments Challenges and soluti.pdf}
}

@article{greenSIMRPackagePower2016,
  title = {{{SIMR}}: An {{R}} Package for Power Analysis of Generalized Linear Mixed Models by Simulation},
  shorttitle = {{{SIMR}}},
  author = {Green, Peter and MacLeod, Catriona J.},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {4},
  pages = {493--498},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12504},
  urldate = {2023-04-24},
  abstract = {The r package simr allows users to calculate power for generalized linear mixed models from the lme4 package. The power calculations are based on Monte Carlo simulations. It includes tools for (i) running a power analysis for a given model and design; and (ii) calculating power curves to assess trade-offs between power and sample size. This paper presents a tutorial using a simple example of count data with mixed effects (with structure representative of environmental monitoring data) to guide the user along a gentle learning curve, adding only a few commands or options at a time.},
  langid = {english},
  keywords = {experimental design,glmm,Monte Carlo,random effects,sample size,type II error},
  file = {/Users/florianpargent/Zotero/storage/YCQ47I3E/Green und MacLeod - 2016 - SIMR an R package for power analysis of generaliz.pdf;/Users/florianpargent/Zotero/storage/CYYNEQY9/2041-210X.html}
}

@article{greenSIMRPackagePower2016a,
  title = {{{{\textsc{SIMR}}}} : An {{R}} Package for Power Analysis of Generalized Linear Mixed Models by Simulation},
  author = {Green, Peter and MacLeod, Catriona J.},
  editor = {Nakagawa, Shinichi},
  year = {2016},
  month = apr,
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {4},
  pages = {493--498},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.12504},
  urldate = {2024-06-08},
  abstract = {Summary                                                                                     The                     r                     package                     simr                     allows users to calculate power for generalized linear mixed models from the                     lme                     4 package. The power calculations are based on Monte Carlo simulations.                                                                        It includes tools for (i) running a power analysis for a given model and design; and (ii) calculating power curves to assess trade-offs between power and sample size.                                                     This paper presents a tutorial using a simple example of count data with mixed effects (with structure representative of environmental monitoring data) to guide the user along a gentle learning curve, adding only a few commands or options at a time.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/E5P33HLM/Green und MacLeod - 2016 - SIMR.pdf}
}

@article{hallgrenConductingSimulationStudies2013,
  title = {Conducting {{Simulation Studies}} in the {{R Programming Environment}}},
  author = {Hallgren, Kevin A.},
  year = {2013},
  month = oct,
  journal = {Tutorials in Quantitative Methods for Psychology},
  volume = {9},
  number = {2},
  pages = {43--60},
  issn = {1913-4126},
  doi = {10.20982/tqmp.09.2.p043},
  urldate = {2024-06-19},
  file = {/Users/florianpargent/Zotero/storage/YZCG7WHA/Hallgren - 2013 - Conducting Simulation Studies in the R Programming.pdf}
}

@article{heGeneralizedLinearMixedeffects2022,
  title = {Generalized Linear Mixed-Effects Models for Studies Using Different Sets of Stimuli across Conditions},
  author = {He, ShunCheng and Lee, Wooyeol},
  year = {2022},
  journal = {Frontiers in Psychology},
  volume = {13},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2022.955722},
  urldate = {2023-09-05},
  abstract = {A non-repeated item (NRI) design refers to an experimental design in which items used in one level of experimental conditions are not repeatedly used at other levels. Recent literature has suggested the use of generalized linear mixed-effects models (GLMMs) for experimental data analysis, but the existing specification of GLMMs does not account for all possible dependencies among the outcomes in NRI designs. Therefore, the current study proposed a GLMM with a level-specific item random effect for NRI designs. The hypothesis testing performance of the newly proposed model was evaluated via a simulation study to detect the experimental condition effect. The model with a level-specific item random effect performed better than the existing model in terms of power when the variance of the item effect was heterogeneous. Based on these results, we suggest that experimental researchers using NRI designs consider setting a level-specific item random effect in the model.},
  file = {/Users/florianpargent/Zotero/storage/493VLEM5/He und Lee - 2022 - Generalized linear mixed-effects models for studie.pdf}
}

@article{hothornSimultaneousInferenceGeneral2008,
  title = {Simultaneous {{Inference}} in {{General Parametric Models}}},
  author = {Hothorn, Torsten and Bretz, Frank and Westfall, Peter},
  year = {2008},
  month = jun,
  journal = {Biometrical Journal},
  volume = {50},
  number = {3},
  pages = {346--363},
  issn = {03233847, 15214036},
  doi = {10.1002/bimj.200810425},
  urldate = {2023-08-18},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/3FRQQ26Y/Hothorn et al. - 2008 - Simultaneous Inference in General Parametric Model.pdf}
}

@article{iddiPowerSampleSize2022,
  title = {Power and {{Sample Size}} for {{Longitudinal Models}} in {{R}} -- {{The}} Longpower {{Package}} and {{Shiny App}}},
  author = {Iddi, Samuel and Donohue, Michael C},
  year = {2022},
  month = jul,
  journal = {The R Journal},
  volume = {14},
  number = {1},
  pages = {264--282},
  issn = {2073-4859},
  doi = {10.32614/RJ-2022-022},
  urldate = {2024-06-24},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/R64TANT9/Iddi und C Donohue - 2022 - Power and Sample Size for Longitudinal Models in R.pdf}
}

@article{johnsonPowerAnalysisGeneralized2015,
  title = {Power Analysis for Generalized Linear Mixed Models in Ecology and Evolution},
  author = {Johnson, Paul C. D. and Barry, Sarah J. E. and Ferguson, Heather M. and M{\"u}ller, Pie},
  year = {2015},
  journal = {Methods in Ecology and Evolution},
  volume = {6},
  number = {2},
  pages = {133--142},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12306},
  urldate = {2023-09-13},
  abstract = {`Will my study answer my research question?' is the most fundamental question a researcher can ask when designing a study, yet when phrased in statistical terms -- `What is the power of my study?' or `How precise will my parameter estimate be?' -- few researchers in ecology and evolution (EE) try to answer it, despite the detrimental consequences of performing under- or over-powered research. We suggest that this reluctance is due in large part to the unsuitability of simple methods of power analysis (broadly defined as any attempt to quantify prospectively the `informativeness' of a study) for the complex models commonly used in EE research. With the aim of encouraging the use of power analysis, we present simulation from generalized linear mixed models (GLMMs) as a flexible and accessible approach to power analysis that can account for random effects, overdispersion and diverse response distributions. We illustrate the benefits of simulation-based power analysis in two research scenarios: estimating the precision of a survey to estimate tick burdens on grouse chicks and estimating the power of a trial to compare the efficacy of insecticide-treated nets in malaria mosquito control. We provide a freely available R function, sim.glmm, for simulating from GLMMs. Analysis of simulated data revealed that the effects of accounting for realistic levels of random effects and overdispersion on power and precision estimates were substantial, with correspondingly severe implications for study design in the form of up to fivefold increases in sampling effort. We also show the utility of simulations for identifying scenarios where GLMM-fitting methods can perform poorly. These results illustrate the inadequacy of standard analytical power analysis methods and the flexibility of simulation-based power analysis for GLMMs. The wider use of these methods should contribute to improving the quality of study design in EE.},
  copyright = {{\copyright} 2014 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
  langid = {english},
  keywords = {experimental design,generalized linear mixed model,long-lasting insecticidal net,overdispersion,precision,random effects,sample size,simulation},
  file = {/Users/florianpargent/Zotero/storage/RK7LIRY2/Johnson et al. - 2015 - Power analysis for generalized linear mixed models.pdf}
}

@article{kainPracticalGuidePower2015,
  title = {A Practical Guide and Power Analysis for {{GLMMs}}: Detecting among Treatment Variation in Random Effects},
  shorttitle = {A Practical Guide and Power Analysis for {{GLMMs}}},
  author = {Kain, Morgan P. and Bolker, Ben M. and McCoy, Michael W.},
  year = {2015},
  month = sep,
  journal = {PeerJ},
  volume = {3},
  pages = {e1226},
  issn = {2167-8359},
  doi = {10.7717/peerj.1226},
  urldate = {2024-06-08},
  abstract = {In ecology and evolution generalized linear mixed models (GLMMs) are becoming increasingly used to test for differences in variation by treatment at multiple hierarchical levels. Yet, the specific sampling schemes that optimize the power of an experiment to detect differences in random effects by treatment/group remain unknown. In this paper we develop a blueprint for conducting power analyses for GLMMs focusing on detecting differences in variance by treatment. We present parameterization and power analyses for random-intercepts and random-slopes GLMMs because of their generality as focal parameters for most applications and because of their immediate applicability to emerging questions in the field of behavioral ecology. We focus on the extreme case of hierarchically structured binomial data, though the framework presented here generalizes easily to any error distribution model. First, we determine the optimal ratio of individuals to repeated measures within individuals that maximizes power to detect differences by treatment in among-individual variation in intercept, among-individual variation in slope, and within-individual variation in intercept. Second, we explore how power to detect differences in target variance parameters is affected by total variation. Our results indicate heterogeneity in power across ratios of individuals to repeated measures with an optimal ratio determined by both the target variance parameter and total sample size. Additionally, power to detect each variance parameter was low overall (in most cases {$>$}1,000 total observations per treatment needed to achieve 80\% power) and decreased with increasing variance in non-target random effects. With growing interest in variance as the parameter of inquiry, these power analyses provide a crucial component for designing experiments focused on detecting differences in variance. We hope to inspire novel experimental designs in ecology and evolution investigating the causes and implications of individual-level phenotypic variance, such as the adaptive significance of within-individual variation.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/79TYFEUW/Kain et al. - 2015 - A practical guide and power analysis for GLMMs de.pdf}
}

@inproceedings{kapteinRethinkingStatisticalAnalysis2012,
  title = {Rethinking Statistical Analysis Methods for {{CHI}}},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kaptein, Maurits and Robertson, Judy},
  year = {2012},
  month = may,
  series = {{{CHI}} '12},
  pages = {1105--1114},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2207676.2208557},
  urldate = {2023-09-05},
  abstract = {CHI researchers typically use a significance testing approach to statistical analysis when testing hypotheses during usability evaluations. However, the appropriateness of this approach is under increasing criticism, with statisticians, economists, and psychologists arguing against the use of routine interpretation of results using "canned" p values. Three problems with current practice - the fallacy of the transposed conditional, a neglect of power, and the reluctance to interpret the size of effects - can lead us to build weak theories based on vaguely specified hypothesis, resulting in empirical studies which produce results that are of limited practical or scientific use. Using publicly available data presented at CHI 2010 [19] as an example we address each of the three concerns and promote consideration of the magnitude and actual importance of effects, as opposed to statistical significance, as the new criteria for evaluating CHI research.},
  isbn = {978-1-4503-1015-4},
  keywords = {bayesian statistics,research methods,usability evaluation},
  file = {/Users/florianpargent/Zotero/storage/GLNDLPAR/Kaptein und Robertson - 2012 - Rethinking statistical analysis methods for CHI.pdf}
}

@incollection{kapteinUsingGeneralizedLinear2016,
  title = {Using {{Generalized Linear}} ({{Mixed}}) {{Models}} in {{HCI}}},
  booktitle = {Modern {{Statistical Methods}} for {{HCI}}},
  author = {Kaptein, Maurits},
  editor = {Robertson, Judy and Kaptein, Maurits},
  year = {2016},
  pages = {251--274},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-26633-6_11},
  urldate = {2023-09-05},
  abstract = {In HCI we often encounter dependent variables which are not (conditionally) normally distributed: we measure response-times, mouse-clicks, or the number of dialog steps it took a user to complete a task. Furthermore, we often encounter nested or grouped data; users are grouped within companies or institutes, or we obtain multiple observations within users. The standard linear regression models and ANOVAs used to analyze our experimental data are not always feasible in such cases since their assumptions are violated, or the predictions from the fitted models are outside the range of the observed data. In this chapter we introduce extensions to the standard linear model (LM) to enable the analysis of these data. The use of [R] to fit both Generalized Linear Models (GLMs) as well as Generalized Linear Mixed Models (GLMMs, also known as random effects models or hierarchical models) is explained. The chapter also briefly covers regularized regression models which are hardly used in the social sciences despite the fact that these models are extremely popular in Machine Learning, often for good reasons. We end with a number of recommendations for further reading on the topics that are introduced: the current text serves as a basic introduction.},
  isbn = {978-3-319-26631-2 978-3-319-26633-6},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/DXTM4TDX/Kaptein - 2016 - Using Generalized Linear (Mixed) Models in HCI.pdf}
}

@inproceedings{kayResearcherCenteredDesignStatistics2016,
  title = {Researcher-{{Centered Design}} of {{Statistics}}: {{Why Bayesian Statistics Better Fit}} the {{Culture}} and {{Incentives}} of {{HCI}}},
  shorttitle = {Researcher-{{Centered Design}} of {{Statistics}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kay, Matthew and Nelson, Gregory L. and Hekler, Eric B.},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {4521--4532},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2858036.2858465},
  urldate = {2023-09-05},
  abstract = {A core tradition of HCI lies in the experimental evaluation of the effects of techniques and interfaces to determine if they are useful for achieving their purpose. However, our individual analyses tend to stand alone, and study results rarely accrue in more precise estimates via meta-analysis: in a literature search, we found only 56 meta-analyses in HCI in the ACM Digital Library, 3 of which were published at CHI (often called the top HCI venue). Yet meta-analysis is the gold standard for demonstrating robust quantitative knowledge. We treat this as a user-centered design problem: the failure to accrue quantitative knowledge is not the users' (i.e. researchers') failure, but a failure to consider those users' needs when designing statistical practice. Using simulation, we compare hypothetical publication worlds following existing frequentist against Bayesian practice. We show that Bayesian analysis yields more precise effects with each new study, facilitating knowledge accrual without traditional meta-analyses. Bayesian practices also allow more principled conclusions from small-n studies of novel techniques. These advantages make Bayesian practices a likely better fit for the culture and incentives of the field. Instead of admonishing ourselves to spend resources on larger studies, we propose using tools that more appropriately analyze small studies and encourage knowledge accrual from one study to the next. We also believe Bayesian methods can be adopted from the bottom up without the need for new incentives for replication or meta-analysis. These techniques offer the potential for a more user- (i.e. researcher-) centered approach to statistical analysis in HCI.},
  isbn = {978-1-4503-3362-7},
  keywords = {bayesian statistics,effect size,estimation,meta-analysis,replication,small stud-ies},
  file = {/Users/florianpargent/Zotero/storage/CCUKDBDR/Kay et al. - 2016 - Researcher-Centered Design of Statistics Why Baye.pdf}
}

@article{kelleyObtainingPowerObtaining2003,
  title = {Obtaining {{Power}} or {{Obtaining Precision}}: {{Delineating Methods}} of {{Sample-Size Planning}}},
  shorttitle = {Obtaining {{Power}} or {{Obtaining Precision}}},
  author = {Kelley, Ken and Maxwell, Scott E. and Rausch, Joseph R.},
  year = {2003},
  month = sep,
  journal = {Evaluation \& the Health Professions},
  volume = {26},
  number = {3},
  pages = {258--287},
  issn = {0163-2787, 1552-3918},
  doi = {10.1177/0163278703255242},
  urldate = {2024-03-13},
  abstract = {Sample-size planning historically has been approached from a power analytic perspective in order to have some reasonable probability of correctly rejecting the null hypothesis. Another approach that is not as well-known is one that emphasizes accuracy in parameter estimation (AIPE). From the AIPE perspective, sample size is chosen such that the expected width of a confidence interval will be sufficiently narrow. The rationales of both approaches are delineated and two procedures are given for estimating the sample size from the AIPE perspective for a two-group mean comparison. One method yields the required sample size, such that the expected width of the computed confidence interval will be the value specified. A modification allows for a defined degree of probabilistic assurance that the width of the computed confidence interval will be no larger than specified. The authors emphasize that the correct conceptualization of sample-size planning depends on the research questions and particular goals of the study.},
  langid = {english}
}

@article{kelleySampleSizePlanning2006,
  title = {Sample Size Planning for the Standardized Mean Difference: {{Accuracy}} in Parameter Estimation via Narrow Confidence Intervals.},
  shorttitle = {Sample Size Planning for the Standardized Mean Difference},
  author = {Kelley, Ken and Rausch, Joseph R.},
  year = {2006},
  journal = {Psychological Methods},
  volume = {11},
  number = {4},
  pages = {363--385},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.4.363},
  urldate = {2024-03-13},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/SUHBQIEH/Kelley und Rausch - 2006 - Sample size planning for the standardized mean dif.pdf}
}

@article{kingPointMinimalImportant2011,
  title = {A Point of Minimal Important Difference ({{MID}}): A Critique of Terminology and Methods},
  shorttitle = {A Point of Minimal Important Difference ({{MID}})},
  author = {King, Madeleine T},
  year = {2011},
  month = apr,
  journal = {Expert Review of Pharmacoeconomics \& Outcomes Research},
  volume = {11},
  number = {2},
  pages = {171--184},
  issn = {1473-7167, 1744-8379},
  doi = {10.1586/erp.11.9},
  urldate = {2023-08-18},
  langid = {english}
}

@article{kruschkeBayesianNewStatistics2018,
  title = {The {{Bayesian New Statistics}}: {{Hypothesis}} Testing, Estimation, Meta-Analysis, and Power Analysis from a {{Bayesian}} Perspective},
  shorttitle = {The {{Bayesian New Statistics}}},
  author = {Kruschke, John K. and Liddell, Torrin M.},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {178--206},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1221-4},
  urldate = {2024-06-04},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/T9HKACPP/Kruschke und Liddell - 2018 - The Bayesian New Statistics Hypothesis testing, e.pdf}
}

@article{kumleEstimatingPowerGeneralized2021,
  title = {Estimating Power in (Generalized) Linear Mixed Models: {{An}} Open Introduction and Tutorial in {{R}}},
  shorttitle = {Estimating Power in (Generalized) Linear Mixed Models},
  author = {Kumle, Levi and V{\~o}, Melissa L.-H. and Draschkow, Dejan},
  year = {2021},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {53},
  number = {6},
  pages = {2528--2543},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01546-0},
  urldate = {2022-07-07},
  abstract = {Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to analytic power solutions are simulation-based power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
  langid = {english},
  keywords = {lme4,Mixed models,mixedpower,Power,R,Simulation},
  file = {/Users/florianpargent/Zotero/storage/YR3EVKIK/Kumle et al. - 2021 - Estimating power in (generalized) linear mixed mod.pdf}
}

@article{lafitSelectionNumberParticipants2021,
  title = {Selection of the {{Number}} of {{Participants}} in {{Intensive Longitudinal Studies}}: {{A User-Friendly Shiny App}} and {{Tutorial}} for {{Performing Power Analysis}} in {{Multilevel Regression Models That Account}} for {{Temporal Dependencies}}},
  shorttitle = {Selection of the {{Number}} of {{Participants}} in {{Intensive Longitudinal Studies}}},
  author = {Lafit, Ginette and Adolf, Janne K. and Dejonckheere, Egon and {Myin-Germeys}, Inez and Viechtbauer, Wolfgang and Ceulemans, Eva},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {251524592097873},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245920978738},
  urldate = {2023-08-08},
  abstract = {In recent years, the popularity of procedures for collecting intensive longitudinal data, such as the experience-sampling method, has increased greatly. The data collected using such designs allow researchers to study the dynamics of psychological functioning and how these dynamics differ across individuals. To this end, the data are often modeled with multilevel regression models. An important question that arises when researchers design intensive longitudinal studies is how to determine the number of participants needed to test specific hypotheses regarding the parameters of these models with sufficient power. Power calculations for intensive longitudinal studies are challenging because of the hierarchical data structure in which repeated observations are nested within the individuals and because of the serial dependence that is typically present in these data. We therefore present a user-friendly application and step-by-step tutorial for performing simulation-based power analyses for a set of models that are popular in intensive longitudinal research. Because many studies use the same sampling protocol (i.e., a fixed number of at least approximately equidistant observations) within individuals, we assume that this protocol is fixed and focus on the number of participants. All included models explicitly account for the temporal dependencies in the data by assuming serially correlated errors or including autoregressive effects.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/32ZYBH9U/Lafit et al. - 2021 - Selection of the Number of Participants in Intensi.pdf}
}

@article{lakensEquivalenceTestingPsychological2018,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {259--269},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245918770963},
  urldate = {2024-06-05},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/FF8V8EHQ/Lakens et al. - 2018 - Equivalence Testing for Psychological Research A .pdf}
}

@misc{lakensImprovingYourStatistical2022,
  title = {Improving {{Your Statistical Inferences}}},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = apr,
  doi = {10.5281/ZENODO.6409077},
  urldate = {2023-08-09},
  abstract = {This open educational resource contains information to improve statistical inferences, design better experiments, and report scientific research more transparently.},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International, Open Access},
  howpublished = {Zenodo},
  keywords = {experimental design,frequentist statistics,hypothesis testing,open science,statistical inferences}
}

@article{lakensJustifyYourAlpha2018,
  title = {Justify Your Alpha},
  author = {Lakens, Dani{\"e}l and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and {van Harmelen}, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and {de Oliveira}, Cilene Lino and {de Xivry}, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k a}tkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {3},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  urldate = {2022-03-17},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/Q2TZAF3Z/Lakens et al. - 2018 - Justify your alpha.pdf}
}

@article{lakensSampleSizeJustification2022,
  title = {Sample {{Size Justification}}},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = mar,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  urldate = {2023-04-24},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  file = {/Users/florianpargent/Zotero/storage/RIYUXBC7/Lakens - 2022 - Sample Size Justification.pdf;/Users/florianpargent/Zotero/storage/MMJADTTA/Sample-Size-Justification.html}
}

@article{lakensSimulationBasedPowerAnalysis2021,
  title = {Simulation-{{Based Power Analysis}} for {{Factorial Analysis}} of {{Variance Designs}}},
  author = {Lakens, Dani{\"e}l and Caldwell, Aaron R.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {251524592095150},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245920951503},
  urldate = {2024-06-20},
  abstract = {Researchers often rely on analysis of variance (ANOVA) when they report results of experiments. To ensure that a study is adequately powered to yield informative results with an ANOVA, researchers can perform an a priori power analysis. However, power analysis for factorial ANOVA designs is often a challenge. Current software solutions do not allow power analyses for complex designs with several within-participants factors. Moreover, power analyses often need [Formula: see text] or Cohen's f as input, but these effect sizes are not intuitive and do not generalize to different experimental designs. We have created the R package Superpower and online Shiny apps to enable researchers without extensive programming experience to perform simulation-based power analysis for ANOVA designs of up to three within- or between-participants factors. Predicted effects are entered by specifying means, standard deviations, and, for within-participants factors, the correlations. The simulation provides the statistical power for all ANOVA main effects, interactions, and individual comparisons. The software can plot power across a range of sample sizes, can control for multiple comparisons, and can compute power when the homogeneity or sphericity assumption is violated. This Tutorial demonstrates how to perform a priori power analysis to design informative studies for main effects, interactions, and individual comparisons and highlights important factors that determine the statistical power for factorial ANOVA designs.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/TJAM4ZJB/Lakens und Caldwell - 2021 - Simulation-Based Power Analysis for Factorial Anal.pdf}
}

@article{lanePowerStrugglesEstimating2018,
  title = {Power Struggles: {{Estimating}} Sample Size for Multilevel Relationships Research},
  shorttitle = {Power Struggles},
  author = {Lane, Sean P. and Hennes, Erin P.},
  year = {2018},
  month = jan,
  journal = {Journal of Social and Personal Relationships},
  volume = {35},
  number = {1},
  pages = {7--31},
  issn = {0265-4075, 1460-3608},
  doi = {10.1177/0265407517710342},
  urldate = {2023-08-08},
  langid = {english}
}

@misc{lebeauPowerAnalysisSimulation2019,
  title = {Power {{Analysis}} by {{Simulation}} Using {{R}} and Simglm},
  author = {LeBeau, Brandon},
  year = {2019},
  month = jun,
  doi = {10.17077/f7kk-6w7f},
  urldate = {2024-06-28},
  abstract = {Power is a task that is commonly done prior to collecting data for a primary study. In most cases closed-form solutions are used to estimate power which may statistical assumptions to be able to perform the computations, for example assume residuals are normally distributed. In real-world data, these statistical assumptions may not hold, therefore estimates of power when these assumptions are assumed will likely be inflated. Power by simulation is another way to compute power estimates and offers significant flexibility to the user to explore the impact of various statistical assumption violations may have on power. This tutorial uses the simglm R package to perform the power by simulation. The simglm package provides a framework to simulate data from generalized linear mixed models which includes a wide variety of models. In addition, functions to perform replications and to compute power estimate summaries are available for users to take advantage of. Two worked examples are shown, one for a two-sample t-test and another within a repeated measures or longitudinal framework.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}

@misc{lebeauSimglmSimulateModels2017,
  title = {Simglm: {{Simulate Models Based}} on the {{Generalized Linear Model}}},
  shorttitle = {Simglm},
  author = {LeBeau, Brandon},
  year = {2017},
  month = may,
  pages = {0.8.9},
  publisher = {Comprehensive R Archive Network},
  doi = {10.32614/CRAN.package.simglm},
  urldate = {2024-06-24},
  abstract = {Simulates regression models, including both simple regression and generalized linear mixed models with up to three level of nesting. Power simulations that are flexible allowing the specification of missing data, unbalanced designs, and different random error distributions are built into the package.},
  langid = {english}
}

@article{leeUsingTidyversePackage2020,
  title = {Using the {{Tidyverse Package}} in {{R}} for {{Simulation Studies}} in {{SEM}}},
  author = {Lee, Sunbok and Sriutaisuk, Suppanut and Kim, Hanjoe},
  year = {2020},
  month = may,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {27},
  number = {3},
  pages = {468--482},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2019.1644515},
  urldate = {2024-06-19},
  langid = {english}
}

@book{littleStatisticalAnalysisMissing2014,
  title = {Statistical {{Analysis}} with {{Missing Data}}},
  author = {Little, Roderick J. A. and Rubin, Donald B.},
  year = {2014},
  edition = {2nd ed},
  publisher = {Wiley},
  address = {Somerset},
  isbn = {978-1-118-62588-0},
  langid = {english},
  annotation = {OCLC: 927508559}
}

@article{lukeEvaluatingSignificanceLinear2017,
  title = {Evaluating Significance in Linear Mixed-Effects Models in {{R}}},
  author = {Luke, Steven G.},
  year = {2017},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {49},
  number = {4},
  pages = {1494--1502},
  issn = {1554-3528},
  doi = {10.3758/s13428-016-0809-y},
  urldate = {2023-04-24},
  abstract = {Mixed-effects models are being used ever more frequently in the analysis of experimental data. However, in the lme4 package in R the standards for evaluating significance of fixed effects in these models (i.e., obtaining p-values) are somewhat vague. There are good reasons for this, but as researchers who are using these models are required in many cases to report p-values, some method for evaluating the significance of the model output is needed. This paper reports the results of simulations showing that the two most common methods for evaluating significance, using likelihood ratio tests and applying the z distribution to the Wald t values from the model output (t-as-z), are somewhat anti-conservative, especially for smaller sample sizes. Other methods for evaluating significance, including parametric bootstrapping and the Kenward-Roger and Satterthwaite approximations for degrees of freedom, were also evaluated. The results of these simulations suggest that Type 1 error rates are closest to .05 when models are fitted using REML and p-values are derived using the Kenward-Roger or Satterthwaite approximations, as these approximations both produced acceptable Type 1 error rates even for smaller samples.},
  langid = {english},
  keywords = {Linear mixed-effects models,lme4,p-values,Statistics,Type 1 error},
  file = {/Users/florianpargent/Zotero/storage/TLJVWC3F/Luke - 2017 - Evaluating significance in linear mixed-effects mo.pdf}
}

@article{lundbergWhatYourEstimand2021,
  title = {What {{Is Your Estimand}}? {{Defining}} the {{Target Quantity Connects Statistical Evidence}} to {{Theory}}},
  shorttitle = {What {{Is Your Estimand}}?},
  author = {Lundberg, Ian and Johnson, Rebecca and Stewart, Brandon M.},
  year = {2021},
  month = jun,
  journal = {American Sociological Review},
  volume = {86},
  number = {3},
  pages = {532--565},
  issn = {0003-1224, 1939-8271},
  doi = {10.1177/00031224211004187},
  urldate = {2024-03-13},
  abstract = {We make only one point in this article. Every quantitative study must be able to answer the question: what is your estimand? The estimand is the target quantity---the purpose of the statistical analysis. Much attention is already placed on how to do estimation; a similar degree of care should be given to defining the thing we are estimating. We advocate that authors state the central quantity of each analysis---the theoretical estimand---in precise terms that exist outside of any statistical model. In our framework, researchers do three things: (1) set a theoretical estimand, clearly connecting this quantity to theory; (2) link to an empirical estimand, which is informative about the theoretical estimand under some identification assumptions; and (3) learn from data. Adding precise estimands to research practice expands the space of theoretical questions, clarifies how evidence can speak to those questions, and unlocks new tools for estimation. By grounding all three steps in a precise statement of the target quantity, our framework connects statistical evidence to theory.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/5JHCSG6V/lundberg-et-al-2021-what-is-your-estimand-defining-the-target-quantity-connects-statistical-evidence-to-theory.pdf}
}

@article{magnussonConsequencesIgnoringTherapist2018,
  title = {The Consequences of Ignoring Therapist Effects in Trials with Longitudinal Data: {{A}} Simulation Study.},
  shorttitle = {The Consequences of Ignoring Therapist Effects in Trials with Longitudinal Data},
  author = {Magnusson, Kristoffer and Andersson, Gerhard and Carlbring, Per},
  year = {2018},
  month = sep,
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {86},
  number = {9},
  pages = {711--725},
  issn = {1939-2117, 0022-006X},
  doi = {10.1037/ccp0000333},
  urldate = {2024-06-24},
  langid = {english}
}

@article{maierJustifyYourAlpha2022,
  title = {Justify {{Your Alpha}}: {{A Primer}} on {{Two Practical Approaches}}},
  shorttitle = {Justify {{Your Alpha}}},
  author = {Maier, Maximilian and Lakens, Dani{\"e}l},
  year = {2022},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {2},
  pages = {251524592210803},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/25152459221080396},
  urldate = {2022-12-20},
  abstract = {The default use of an alpha level of .05 is suboptimal for two reasons. First, decisions based on data can be made more efficiently by choosing an alpha level that minimizes the combined Type 1 and Type 2 error rate. Second, it is possible that in studies with very high statistical power, p values lower than the alpha level can be more likely when the null hypothesis is true than when the alternative hypothesis is true (i.e., Lindley's paradox). In this article, we explain two approaches that can be used to justify a better choice of an alpha level than relying on the default threshold of .05. The first approach is based on the idea to either minimize or balance Type 1 and Type 2 error rates. The second approach lowers the alpha level as a function of the sample size to prevent Lindley's paradox. An R package and Shiny app are provided to perform the required calculations. Both approaches have their limitations (e.g., the challenge of specifying relative costs and priors) but can offer an improvement to current practices, especially when sample sizes are large. The use of alpha levels that are better justified should improve statistical inferences and can increase the efficiency and informativeness of scientific research.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/VWYQYDEV/Maier und Lakens - 2022 - Justify Your Alpha A Primer on Two Practical Appr.pdf}
}

@article{martinMeasuringIndividualDifferences2011,
  title = {Measuring Individual Differences in Reaction Norms in Field and Experimental Studies: A Power Analysis of Random Regression Models},
  shorttitle = {Measuring Individual Differences in Reaction Norms in Field and Experimental Studies},
  author = {Martin, Julien G. A. and Nussey, Daniel H. and Wilson, Alastair J. and R{\'e}ale, Denis},
  year = {2011},
  month = aug,
  journal = {Methods in Ecology and Evolution},
  volume = {2},
  number = {4},
  pages = {362--374},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/j.2041-210X.2010.00084.x},
  urldate = {2024-06-24},
  abstract = {Summary                            1.               Interest in measuring individual variation in reaction norms using mixed-effects and, more specifically, random regression models have grown apace in the last few years within evolution and ecology. However, these are data hungry methods, and little effort to date has been put into understanding how much and what kind of data we need to collect in order to apply these models usefully and reliably.                                         2.               We conducted simulations to address three central questions. First, what is the best sampling strategy to collect sufficient data to test for individual variation using random regression models? Second, on occasions when precision is difficult to assess, can we be confident that a failure to detect significant variance in plasticity using random regression represents a biological reality rather than a lack of statistical power? Finally, does the common practice of censoring individuals with one or few repeated measures improve or reduce power to estimate individual variation in random regressions?                                         3.               We have also developed a series of easy-to-use functions in the `pamm' statistical package for R, which is freely available, that will allow researchers to conduct similar power analyses tailored more specifically to their own data.                                         4.               Our results reveal potentially useful rules of thumb: large data sets (               N               {$\quad>\quad$}200) are needed to evaluate the variance of individual-specific slopes; a number of individuals/number of observations per individual ratio of approximately 0{$\cdot$}5 consistently yielded the highest power to detect random effects; individuals with one or few observations should not generally be censored as this reduces power to detect variance in plasticity.                                         5.               We discuss the wider implications of these simulations and remaining challenges and suggest a new way to standardize results that would better facilitate the comparison of findings across empirical studies.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english}
}

@article{matuschekBalancingTypeError2017,
  title = {Balancing {{Type I}} Error and Power in Linear Mixed Models},
  author = {Matuschek, Hannes and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald and Bates, Douglas},
  year = {2017},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {94},
  pages = {305--315},
  issn = {0749596X},
  doi = {10.1016/j.jml.2017.01.001},
  urldate = {2024-06-21},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/9TJNWNZY/Matuschek et al. - 2017 - Balancing Type I error and power in linear mixed m.pdf}
}

@article{maxwellSampleSizePlanning2008,
  title = {Sample {{Size Planning}} for {{Statistical Power}} and {{Accuracy}} in {{Parameter Estimation}}},
  author = {Maxwell, Scott E. and Kelley, Ken and Rausch, Joseph R.},
  year = {2008},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {59},
  number = {1},
  pages = {537--563},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.59.103006.093735},
  urldate = {2023-08-08},
  abstract = {This review examines recent advances in sample size planning, not only from the perspective of an individual researcher, but also with regard to the goal of developing cumulative knowledge. Psychologists have traditionally thought of sample size planning in terms of power analysis. Although we review recent advances in power analysis, our main focus is the desirability of achieving accurate parameter estimates, either instead of or in addition to obtaining sufficient power. Accuracy in parameter estimation (AIPE) has taken on increasing importance in light of recent emphasis on effect size estimation and formation of confidence intervals. The review provides an overview of the logic behind sample size planning for AIPE and summarizes recent advances in implementing this approach in designs commonly used in psychological research.},
  langid = {english}
}

@book{mcelreathStatisticalRethinkingBayesian2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  edition = {2},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9780429029608},
  urldate = {2024-06-04},
  isbn = {978-0-429-02960-8},
  langid = {english}
}

@article{meteyardBestPracticeGuidance2020,
  title = {Best Practice Guidance for Linear Mixed-Effects Models in Psychological Science},
  author = {Meteyard, Lotte and Davies, Robert A. I.},
  year = {2020},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {112},
  pages = {104092},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2020.104092},
  urldate = {2023-04-24},
  abstract = {The use of Linear Mixed-effects Models (LMMs) is set to dominate statistical analyses in psychological science and may become the default approach to analyzing quantitative data. The rapid growth in adoption of LMMs has been matched by a proliferation of differences in practice. Unless this diversity is recognized, and checked, the field shall reap enormous difficulties in the future when attempts are made to consolidate or synthesize research findings. Here we examine this diversity using two methods -- a survey of researchers (n~=~163) and a quasi-systematic review of papers using LMMs (n~=~400). The survey reveals substantive concerns among psychologists using or planning to use LMMs and an absence of agreed standards. The review of papers complements the survey, showing variation in how the models are built, how effects are evaluated and, most worryingly, how models are reported. Using these data as our departure point, we present a set of best practice guidance, focusing on the reporting of LMMs. It is the authors' intention that the paper supports a step-change in the reporting of LMMs across the psychological sciences, preventing a trajectory in which findings reported today cannot be transparently understood and used tomorrow.},
  langid = {english},
  keywords = {Hierarchical models,Linear mixed effects models,Multilevel models},
  file = {/Users/florianpargent/Zotero/storage/I3I8TLAM/Meteyard und Davies - 2020 - Best practice guidance for linear mixed-effects mo.pdf;/Users/florianpargent/Zotero/storage/WY8PX7FR/S0749596X20300061.html}
}

@article{meteyardBestPracticeGuidance2020a,
  title = {Best Practice Guidance for Linear Mixed-Effects Models in Psychological Science},
  author = {Meteyard, Lotte and Davies, Robert A.I.},
  year = {2020},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {112},
  pages = {104092},
  issn = {0749596X},
  doi = {10.1016/j.jml.2020.104092},
  urldate = {2024-06-21},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/DSR6QLHR/Meteyard und Davies - 2020 - Best practice guidance for linear mixed-effects mo.pdf}
}

@article{murayamaSummarystatisticsbasedPowerAnalysis2022,
  title = {Summary-Statistics-Based Power Analysis: {{A}} New and Practical Method to Determine Sample Size for Mixed-Effects Modeling.},
  shorttitle = {Summary-Statistics-Based Power Analysis},
  author = {Murayama, Kou and Usami, Satoshi and Sakaki, Michiko},
  year = {2022},
  month = jan,
  journal = {Psychological Methods},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000330},
  urldate = {2023-08-07},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/772RYEV2/murayama.pdf;/Users/florianpargent/Zotero/storage/YSRNCZBA/Murayama et al. - 2022 - Summary-statistics-based power analysis A new and.pdf}
}

@article{peruginiSafeguardPowerProtection2014,
  title = {Safeguard {{Power}} as a {{Protection Against Imprecise Power Estimates}}},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  year = {2014},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {3},
  pages = {319--332},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691614528519},
  urldate = {2024-06-25},
  abstract = {An essential first step in planning a confirmatory or a replication study is to determine the sample size necessary to draw statistically reliable inferences using power analysis. A key problem, however, is that what is available is the sample-size estimate of the effect size, and its use can lead to severely underpowered studies when the effect size is overestimated. As a potential remedy, we introduce safeguard power analysis, which uses the uncertainty in the estimate of the effect size to achieve a better likelihood of correctly identifying the population effect size. Using a lower-bound estimate of the effect size, in turn, allows researchers to calculate a sample size for a replication study that helps protect it from being underpowered. We show that in most common instances, compared with nominal power, safeguard power is higher whereas standard power is lower. We additionally recommend the use of safeguard power analysis to evaluate the strength of the evidence provided by the original study.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/LM3GWCM6/Perugini et al. - 2014 - Safeguard Power as a Protection Against Imprecise .pdf}
}

@inproceedings{phelanPriorExperienceNecessary2019,
  title = {Some {{Prior}}(s) {{Experience Necessary}}: {{Templates}} for {{Getting Started With Bayesian Analysis}}},
  shorttitle = {Some {{Prior}}(s) {{Experience Necessary}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Phelan, Chanda and Hullman, Jessica and Kay, Matthew and Resnick, Paul},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3290605.3300709},
  urldate = {2023-09-04},
  abstract = {Bayesian statistical analysis has gained attention in recent years, including in HCI. The Bayesian approach has several advantages over traditional statistics, including producing results with more intuitive interpretations. Despite growing interest, few papers in CHI use Bayesian analysis. Existing tools to learn Bayesian statistics require significant time investment, making it difficult to casually explore Bayesian methods. Here, we present a tool that lowers the barrier to exploration: a set of R code templates that guide Bayesian novices through their first analysis. The templates are tailored to CHI, supporting analyses found to be most common in recent CHI papers. In a user study, we found that the templates were easy to understand and use. However, we found that participants without a statistical background were not confident in their use. Together our contributions provide a concise analysis tool and empirical results for understanding and addressing barriers to using Bayesian analysis in HCI.},
  isbn = {978-1-4503-5970-2},
  keywords = {bayesian statistics,code templates,evaluation,hypothesis testing,statistics,tutorials}
}

@article{preacherComputationalToolsProbing2006,
  title = {Computational {{Tools}} for {{Probing Interactions}} in {{Multiple Linear Regression}}, {{Multilevel Modeling}}, and {{Latent Curve Analysis}}},
  author = {Preacher, Kristopher J. and Curran, Patrick J. and Bauer, Daniel J.},
  year = {2006},
  month = dec,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {31},
  number = {4},
  pages = {437--448},
  issn = {1076-9986, 1935-1054},
  doi = {10.3102/10769986031004437},
  urldate = {2024-06-05},
  abstract = {Simple slopes, regions of significance, and confidence bands are commonly used to evaluate interactions in multiple linear regression (MLR) models, and the use of these techniques has recently been extended to multilevel or hierarchical linear modeling (HLM) and latent curve analysis (LCA). However, conducting these tests and plotting the conditional relations is often a tedious and error-prone task. This article provides an overview of methods used to probe interaction effects and describes a unified collection of freely available online resources that researchers can use to obtain significance tests for simple slopes, compute regions of significance, and obtain confidence bands for simple slopes across the range of the moderator in the MLR, HLM, and LCA contexts. Plotting capabilities are also provided.},
  copyright = {http://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/PUSMNJPD/Preacher et al. - 2006 - Computational Tools for Probing Interactions in Mu.pdf}
}

@article{raudenbushStatisticalPowerOptimal,
  title = {Statistical {{Power}} and {{Optimal Design}} for {{Multisite Randomized Trials}}},
  author = {Raudenbush, Stephen W and Liu, Xiaofeng},
  doi = {10.1037/1082-989X.5.2.199},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/6QYR8ETB/Raudenbush und Liu - Statistical Power and Optimal Design for Multisite.pdf}
}

@article{riesthuisSimulationBasedPowerAnalyses2024,
  title = {Simulation-{{Based Power Analyses}} for the {{Smallest Effect Size}} of {{Interest}}: {{A Confidence-Interval Approach}} for {{Minimum-Effect}} and {{Equivalence Testing}}},
  shorttitle = {Simulation-{{Based Power Analyses}} for the {{Smallest Effect Size}} of {{Interest}}},
  author = {Riesthuis, Paul},
  year = {2024},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {7},
  number = {2},
  pages = {25152459241240722},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459241240722},
  urldate = {2024-04-23},
  abstract = {Effect sizes are often used in psychology because they are crucial when determining the required sample size of a study and when interpreting the implications of a result. Recently, researchers have been encouraged to contextualize their effect sizes and determine what the smallest effect size is that yields theoretical or practical implications, also known as the ``smallest effect size of interest'' (SESOI). Having a SESOI will allow researchers to have more specific hypotheses, such as whether their findings are truly meaningful (i.e., minimum-effect testing) or whether no meaningful effect exists (i.e., equivalence testing). These types of hypotheses should be reflected in power analyses to accurately determine the required sample size. Through a confidence-interval-focused approach and simulations, I show how to conduct power analyses for minimum-effect and equivalence testing. Moreover, I show that conducting a power analysis for the SESOI might result in inconclusive results. This confidence-interval-focused simulation-based power analysis can be easily adopted to different types of research areas and designs. Last, I provide recommendations on how to conduct such simulation-based power analyses.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/8XU7EB4K/Riesthuis - 2024 - Simulation-Based Power Analyses for the Smallest E.pdf}
}

@incollection{robertsonImprovingStatisticalPractice2016,
  title = {Improving {{Statistical Practice}} in {{HCI}}},
  booktitle = {Modern {{Statistical Methods}} for {{HCI}}},
  author = {Robertson, Judy and Kaptein, Maurits},
  editor = {Robertson, Judy and Kaptein, Maurits},
  year = {2016},
  pages = {331--348},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-26633-6_14},
  urldate = {2023-09-05},
  isbn = {978-3-319-26631-2 978-3-319-26633-6},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/XJ56VFD8/Robertson und Kaptein - 2016 - Improving Statistical Practice in HCI.pdf}
}

@book{robertsonModernStatisticalMethods2016,
  title = {Modern {{Statistical Methods}} for {{HCI}}},
  editor = {Robertson, Judy and Kaptein, Maurits},
  year = {2016},
  series = {Human--{{Computer Interaction Series}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-26633-6},
  urldate = {2023-09-05},
  isbn = {978-3-319-26631-2 978-3-319-26633-6},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/BAYH5MAW/Robertson und Kaptein - 2016 - Modern Statistical Methods for HCI.pdf}
}

@book{robertsonModernStatisticalMethods2016a,
  title = {Modern {{Statistical Methods}} for {{HCI}}},
  editor = {Robertson, Judy and Kaptein, Maurits},
  year = {2016},
  series = {Human--{{Computer Interaction Series}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-26633-6},
  urldate = {2023-09-05},
  isbn = {978-3-319-26631-2 978-3-319-26633-6},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/NHP7PTVL/Robertson und Kaptein - 2016 - Modern Statistical Methods for HCI.pdf}
}

@article{schadHowCapitalizePriori2020,
  title = {How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: {{A}} Tutorial},
  shorttitle = {How to Capitalize on a Priori Contrasts in Linear (Mixed) Models},
  author = {Schad, Daniel J. and Vasishth, Shravan and Hohenstein, Sven and Kliegl, Reinhold},
  year = {2020},
  month = feb,
  journal = {Journal of Memory and Language},
  volume = {110},
  pages = {104038},
  issn = {0749596X},
  doi = {10.1016/j.jml.2019.104038},
  urldate = {2024-06-05},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/TI8H823J/Schad et al. - 2020 - How to capitalize on a priori contrasts in linear .pdf}
}

@article{scherbaumEstimatingStatisticalPower2009,
  title = {Estimating {{Statistical Power}} and {{Required Sample Sizes}} for {{Organizational Research Using Multilevel Modeling}}},
  author = {Scherbaum, Charles A. and Ferreter, Jennifer M.},
  year = {2009},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {12},
  number = {2},
  pages = {347--367},
  issn = {1094-4281, 1552-7425},
  doi = {10.1177/1094428107308906},
  urldate = {2023-08-08},
  abstract = {The use of multilevel modeling to investigate organizational phenomena is rapidly increasing. Unfortunately, little advice is readily available for organizational researchers attempting to determine statistical power when using multilevel models or when determining sample sizes for each level that will maximize statistical power. This article presents an introduction to statistical power in multilevel models. The unique factors influencing power in multilevel models and calculations for estimating power for simple fixed effects, variance components, and cross-level interactions are presented. The results of simulation studies and the existing general rules of thumb are discussed, and the available power analysis software is reviewed.},
  langid = {english}
}

@incollection{singmannIntroductionMixedModels2019a,
  title = {An {{Introduction}} to {{Mixed Models}} for {{Experimental Psychology}}},
  booktitle = {New {{Methods}} in {{Cognitive Psychology}}},
  author = {Singmann, Henrik and Kellen, David},
  editor = {Spieler, Daniel and Schumacher, Eric},
  year = {2019},
  month = oct,
  edition = {1},
  pages = {4--31},
  publisher = {Routledge},
  doi = {10.4324/9780429318405-2},
  urldate = {2023-04-24},
  isbn = {978-0-429-31840-5},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/CDEQRLAW/Singmann und Kellen - 2019 - An Introduction to Mixed Models for Experimental P.pdf}
}

@article{uyguntuncEpistemicPragmaticFunction2023,
  title = {The Epistemic and Pragmatic Function of Dichotomous Claims Based on Statistical Hypothesis Tests},
  author = {Uygun Tun{\c c}, Duygu and Tun{\c c}, Mehmet Necip and Lakens, Dani{\"e}l},
  year = {2023},
  month = jun,
  journal = {Theory \& Psychology},
  volume = {33},
  number = {3},
  pages = {403--423},
  issn = {0959-3543, 1461-7447},
  doi = {10.1177/09593543231160112},
  urldate = {2024-06-04},
  abstract = {Researchers commonly make dichotomous claims based on continuous test statistics. Many have branded the practice as a misuse of statistics and criticize scientists for the widespread application of hypothesis tests to tentatively reject a hypothesis (or not) depending on whether a p-value is below or above an alpha level. Although dichotomous claims are rarely explicitly defended, we argue they play an important epistemological and pragmatic role in science. The epistemological function of dichotomous claims consists in transforming data into quasibasic statements, which are tentatively accepted singular facts that can corroborate or falsify theoretical claims. This transformation requires a prespecified methodological decision procedure such as Neyman-Pearson hypothesis tests. From the perspective of methodological falsificationism these decision procedures are necessary, as probabilistic statements (e.g., continuous test statistics) cannot function as falsifiers of substantive hypotheses. The pragmatic function of dichotomous claims is to facilitate scrutiny and criticism among peers by generating contestable claims, a process referred to by Popper as ``conjectures and refutations.'' We speculate about how the surprisingly widespread use of a 5\% alpha level might have facilitated this pragmatic function. Abandoning dichotomous claims, for example because researchers commonly misuse p-values, would sacrifice their crucial epistemic and pragmatic functions.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/WNK8RBAR/Uygun Tunç et al. - 2023 - The epistemic and pragmatic function of dichotomou.pdf}
}

@article{wangPowerAnalysisParameter2021,
  title = {Power {{Analysis}} for {{Parameter Estimation}} in {{Structural Equation Modeling}}: {{A Discussion}} and {{Tutorial}}},
  shorttitle = {Power {{Analysis}} for {{Parameter Estimation}} in {{Structural Equation Modeling}}},
  author = {Wang, Y. Andre and Rhemtulla, Mijke},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920918253},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920918253},
  urldate = {2023-10-19},
  abstract = {Despite the widespread and rising popularity of structural equation modeling (SEM) in psychology, there is still much confusion surrounding how to choose an appropriate sample size for SEM. Currently available guidance primarily consists of sample-size rules of thumb that are not backed up by research and power analyses for detecting model misspecification. Missing from most current practices is power analysis for detecting a target effect (e.g., a regression coefficient between latent variables). In this article, we (a) distinguish power to detect model misspecification from power to detect a target effect, (b) report the results of a simulation study on power to detect a target regression coefficient in a three-predictor latent regression model, and (c) introduce a user-friendly Shiny app, pwrSEM, for conducting power analysis for detecting target effects in structural equation models.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/LHWJ6VCS/Wang and Rhemtulla - 2021 - Power Analysis for Parameter Estimation in Structu.pdf}
}

@misc{watsonGeneralisedLinearMixed2023,
  title = {Generalised {{Linear Mixed Model Specification}}, {{Analysis}}, {{Fitting}}, and {{Optimal Design}} in {{R}} with the Glmmr {{Packages}}},
  author = {Watson, Samuel I.},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2303.12657},
  urldate = {2024-06-24},
  abstract = {We describe the {\textbackslash}proglang\{R\} package {\textbackslash}pkg\{glmmrBase\} and an extension {\textbackslash}pkg\{glmmrOptim\}. {\textbackslash}pkg\{glmmrBase\} provides a flexible approach to specifying, fitting, and analysing generalised linear mixed models. We use an object-orientated class system within {\textbackslash}proglang\{R\} to provide methods for a wide range of covariance and mean functions, including specification of non-linear functions of data and parameters, relevant to multiple applications including cluster randomised trials, cohort studies, spatial and spatio-temporal modelling, and split-plot designs. The class generates relevant matrices and statistics and a wide range of methods including full likelihood estimation of generalised linear mixed models using stochastic Maximum Likelihood, Laplace approximation, power calculation, and access to relevant calculations. The class also includes Hamiltonian Monte Carlo simulation of random effects, sparse matrix methods, and other functionality to support efficient estimation. The {\textbackslash}pkg\{glmmrOptim\} package implements a set of algorithms to identify c-optimal experimental designs where observations are correlated and can be specified using the generalised linear mixed model classes. Several examples and comparisons to existing packages are provided to illustrate use of the packages.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation (stat.CO),FOS: Computer and information sciences,Methodology (stat.ME)}
}

@article{westfallStatisticalPowerOptimal2014,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  pages = {2020--2045},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2222},
  doi = {10.1037/xge0000014},
  abstract = {Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Design,Sampling (Experimental),Statistical Power,Stimulus Parameters},
  file = {/Users/florianpargent/Zotero/storage/HVBDJDCL/Westfall et al. - 2014 - Statistical power and optimal design in experiment.pdf;/Users/florianpargent/Zotero/storage/HQ9U2NBA/doiLanding.html}
}

@misc{wickelmaierSimulatingPowerStatistical2022,
  title = {Simulating the {{Power}} of {{Statistical Tests}}: {{A Collection}} of {{R Examples}}},
  shorttitle = {Simulating the {{Power}} of {{Statistical Tests}}},
  author = {Wickelmaier, Florian},
  year = {2022},
  month = mar,
  number = {arXiv:2110.09836},
  eprint = {2110.09836},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.09836},
  urldate = {2023-05-30},
  abstract = {This paper illustrates how to calculate the power of a statistical test by computer simulation. It provides R code for power simulations of several classical inference procedures including one- and two-sample t tests, chi-squared tests, regression, and analysis of variance.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/Users/florianpargent/Zotero/storage/5EZR87BR/Wickelmaier - 2022 - Simulating the Power of Statistical Tests A Colle.pdf;/Users/florianpargent/Zotero/storage/EDTJ448P/2110.html}
}

@book{wickhamDataScienceImport2023,
  title = {R for Data Science: Import, Tidy, Transform, Visualize, and Model Data},
  shorttitle = {R for Data Science},
  author = {Wickham, Hadley and {\c C}etinkaya-Rundel, Mine and Grolemund, Garrett},
  year = {2023},
  edition = {2nd edition},
  publisher = {O'Reilly},
  address = {Beijing Boston Farnham Sebastopol Tokyo},
  abstract = {Use R to turn data into insight, knowledge, and understanding. With this practical book, aspiring data scientists will learn how to do data science with R and RStudio, along with the tidyverse---a collection of R packages designed to work together to make data science fast, fluent, and fun. Even if you have no programming experience, this updated edition will have you doing data science quickly. You'll learn how to import, transform, and visualize your data and communicate the results. And you'll get a complete, big-picture understanding of the data science cycle and the basic tools you need to manage the details. Updated for the latest tidyverse features and best practices, new chapters show you how to get data from spreadsheets, databases, and websites. Exercises help you practice what you've learned along the way},
  isbn = {978-1-4920-9740-2},
  langid = {english}
}

@article{wickhamWelcomeTidyverse2019,
  title = {Welcome to the {{Tidyverse}}},
  author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy and Fran{\c c}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas and Miller, Evan and Bache, Stephan and M{\"u}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  year = {2019},
  month = nov,
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  issn = {2475-9066},
  doi = {10.21105/joss.01686},
  urldate = {2024-05-21},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  file = {/Users/florianpargent/Zotero/storage/UB27KTZ8/Wickham et al. - 2019 - Welcome to the Tidyverse.pdf}
}

@article{yarkoniGeneralizabilityCrisis2022,
  title = {The Generalizability Crisis},
  author = {Yarkoni, Tal},
  year = {2022},
  journal = {Behavioral and Brain Sciences},
  volume = {45},
  pages = {e1},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X20001685},
  urldate = {2022-11-30},
  abstract = {Abstract             Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned -- that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology -- the linear mixed model -- I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that although the ``random effect'' formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/Z4TK4K3B/Yarkoni - 2022 - The generalizability crisis.pdf}
}

@article{yarkoniRepliesCommentariesGeneralizability2022,
  title = {Replies to Commentaries on the Generalizability Crisis},
  author = {Yarkoni, Tal},
  year = {2022},
  journal = {Behavioral and Brain Sciences},
  volume = {45},
  pages = {e40},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X21001758},
  urldate = {2022-06-08},
  abstract = {Abstract             The 38 commentaries on the target article span a broad range of disciplines and perspectives. I have organized my response to the commentaries around three broad questions: First, how serious are the problems discussed in the target article? Second, are there are other, potentially more productive, ways to think about the issues that the target article framed in terms of generalizability? And third, what, if anything, should we collectively do about these problems?},
  langid = {english}
}

@incollection{yataniEffectSizesPower2016,
  title = {Effect {{Sizes}} and {{Power Analysis}} in {{HCI}}},
  booktitle = {Modern {{Statistical Methods}} for {{HCI}}},
  author = {Yatani, Koji},
  editor = {Robertson, Judy and Kaptein, Maurits},
  year = {2016},
  pages = {87--110},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-26633-6_5},
  urldate = {2023-09-05},
  abstract = {Null hypothesis significance testing (NHST) is a common statistical analysis method in HCI. But its usage and interpretation are often misunderstood. In particular, NHST does not offer the magnitude of differences observed, which is more desirable to determine the effect of comparative studies than the p value. Effect sizes and power analysis can mitigate over-reliance on the p value, and offer researchers better informed preparation and interpretation on experiments. Many research fields now require authors to include effect sizes in NHST results, and this trend is expected to be more and more common. In this chapter, I first discuss common misunderstandings of NHST and p value, and how effect sizes can complement them. I then present methods for calculating effect sizes with examples. I also describe another closely related topic, power analysis. Power analysis can be useful for appropriately designing experiments though it is not frequently used in HCI. I present power analysis methods and discuss how they should and should not be used.},
  isbn = {978-3-319-26631-2 978-3-319-26633-6},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/UQCIIWZJ/Yatani - 2016 - Effect Sizes and Power Analysis in HCI.pdf}
}

@misc{yuPassLmePower2019,
  title = {Pass.Lme: {{Power}} and {{Sample Size}} for {{Linear Mixed Effect Models}}},
  shorttitle = {Pass.Lme},
  author = {Yu, Marco},
  year = {2019},
  month = aug,
  pages = {0.9.0},
  publisher = {Comprehensive R Archive Network},
  doi = {10.32614/CRAN.package.pass.lme},
  urldate = {2024-06-24},
  abstract = {Power and sample size calculation for testing fixed effect coefficients in multilevel linear mixed effect models with one or more than one independent populations. Laird, Nan M. and Ware, James H. (1982) {$<$}doi:10.2307/2529876{$>$}.},
  langid = {english}
}

@article{yuTestANOVAApplications2022,
  title = {Beyond t Test and {{ANOVA}}: Applications of Mixed-Effects Models for More Rigorous Statistical Analysis in Neuroscience Research},
  shorttitle = {Beyond t Test and {{ANOVA}}},
  author = {Yu, Zhaoxia and Guindani, Michele and Grieco, Steven F. and Chen, Lujia and Holmes, Todd C. and Xu, Xiangmin},
  year = {2022},
  month = jan,
  journal = {Neuron},
  volume = {110},
  number = {1},
  pages = {21--35},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.10.030},
  urldate = {2023-09-05},
  abstract = {In basic neuroscience research, data are often clustered or collected with repeated measures, hence correlated. The most widely used methods such as t test and ANOVA do not take data dependence into account and thus are often misused. This Primer introduces linear and generalized mixed-effects models that consider data dependence and provides clear instruction on how to recognize when they are needed and how to apply them. The appropriate use of mixed-effects models will help researchers improve their experimental design and will lead to data analyses with greater validity and higher reproducibility of the experimental findings.},
  keywords = {Bayesian analysis,clustered data,generalized linear mixed-effects model,linear mixed-effects model,linear regression model,repeated measures},
  file = {/Users/florianpargent/Zotero/storage/CNYRSB9R/Yu et al. - 2022 - Beyond t test and ANOVA applications of mixed-eff.pdf}
}

@book{zhangPracticalStatisticalPower2018,
  title = {Practical Statistical Power Analysis Using {{Webpower}} and {{R}}},
  author = {Zhang, Zhiyong and Yuan, Ke-Hai},
  year = {2018},
  publisher = {ISDSA Press},
  doi = {10.35566/power},
  urldate = {2024-06-24},
  isbn = {978-1-946728-02-9},
  file = {/Users/florianpargent/Zotero/storage/DKA922ZY/Zhang und Yuan - 2018 - Practical statistical power analysis using Webpowe.pdf}
}

@article{zimmerSampleSizePlanning2023,
  title = {Sample Size Planning for Complex Study Designs: {{A}} Tutorial for the Mlpwr Package},
  shorttitle = {Sample Size Planning for Complex Study Designs},
  author = {Zimmer, Felix and Henninger, Mirka and Debelak, Rudolf},
  year = {2023},
  month = nov,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02269-0},
  urldate = {2024-06-28},
  abstract = {Abstract             A common challenge in designing empirical studies is determining an appropriate sample size. When more complex models are used, estimates of power can only be obtained using Monte Carlo simulations. In this tutorial, we introduce the R package  to perform simulation-based power analysis based on surrogate modeling. Surrogate modeling is a powerful tool in guiding the search for study design parameters that imply a desired power or meet a cost threshold (e.g., in terms of monetary cost).  can be used to search for the optimal allocation when there are multiple design parameters, e.g., when balancing the number of participants and the number of groups in multilevel modeling. At the same time, the approach can take into account the cost of each design parameter, and aims to find a cost-efficient design. We introduce the basic functionality of the package, which can be applied to a wide range of statistical models and study designs. Additionally, we provide two examples based on empirical studies for illustration: one for sample size planning when using an item response theory model, and one for assigning the number of participants and the number of countries for a study using multilevel modeling.},
  langid = {english},
  file = {/Users/florianpargent/Zotero/storage/4MHIXDAQ/Zimmer et al. - 2023 - Sample size planning for complex study designs A .pdf}
}
