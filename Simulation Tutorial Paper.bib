@article{albersWhenPowerAnalyses2018,
  title = {When Power Analyses Based on Pilot Data Are Biased: {{Inaccurate}} Effect Size Estimators and Follow-up Bias},
  shorttitle = {When Power Analyses Based on Pilot Data Are Biased},
  author = {Albers, Casper and Lakens, Dani{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Experimental Social Psychology},
  volume = {74},
  pages = {187--195},
  issn = {00221031},
  doi = {10.1016/j.jesp.2017.09.004},
  urldate = {2023-08-08},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/DKBUKKFH/Albers und Lakens - 2018 - When power analyses based on pilot data are biased.pdf}
}

@article{arendStatisticalPowerTwolevel2019,
  title = {Statistical Power in Two-Level Models: {{A}} Tutorial Based on {{Monte Carlo}} Simulation.},
  shorttitle = {Statistical Power in Two-Level Models},
  author = {Arend, Matthias G. and Sch{\"a}fer, Thomas},
  year = {2019},
  month = feb,
  journal = {Psychological Methods},
  volume = {24},
  number = {1},
  pages = {1--19},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000195},
  urldate = {2023-08-08},
  langid = {english}
}

@article{batesFittingLinearMixedEffects2015,
  title = {Fitting {{Linear Mixed-Effects Models Using}} {\textbf{Lme4}}},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  urldate = {2022-07-14},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/HDSUVZ87/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf}
}

@article{brauerLinearMixedeffectsModels2018,
  title = {Linear Mixed-Effects Models and the Analysis of Nonindependent Data: {{A}} Unified Framework to Analyze Categorical and Continuous Independent Variables That Vary within-Subjects and/or within-Items.},
  shorttitle = {Linear Mixed-Effects Models and the Analysis of Nonindependent Data},
  author = {Brauer, Markus and Curtin, John J.},
  year = {2018},
  month = sep,
  journal = {Psychological Methods},
  volume = {23},
  number = {3},
  pages = {389--411},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000159},
  urldate = {2023-04-24},
  abstract = {In this article we address a number of important issues that arise in the analysis of nonindependent data. Such data are common in studies in which predictors vary within ``units'' (e.g., within-subjects, within-classrooms). Most researchers analyze categorical within-unit predictors with repeated-measures ANOVAs, but continuous within-unit predictors with linear mixed-effects models (LMEMs). We show that both types of predictor variables can be analyzed within the LMEM framework. We discuss designs with multiple sources of nonindependence, for example, studies in which the same subjects rate the same set of items or in which students nested in classrooms provide multiple answers. We provide clear guidelines about the types of random effects that should be included in the analysis of such designs. We also present a number of corrective steps that researchers can take when convergence fails in LMEM models with too many parameters. We end with a brief discussion on the trade-off between power and generalizability in designs with ``within-unit'' predictors.},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/YHA4SGV9/Brauer und Curtin - 2018 - Linear mixed-effects models and the analysis of no.pdf}
}

@article{brownIntroductionLinearMixedEffects2021,
  title = {An {{Introduction}} to {{Linear Mixed-Effects Modeling}} in {{R}}},
  author = {Brown, Violet A.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920960351},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920960351},
  urldate = {2022-07-14},
  abstract = {This Tutorial serves as both an approachable theoretical introduction to mixed-effects modeling and a practical introduction to how to implement mixed-effects models in R. The intended audience is researchers who have some basic statistical knowledge, but little or no experience implementing mixed-effects models in R using their own data. In an attempt to increase the accessibility of this Tutorial, I deliberately avoid using mathematical terminology beyond what a student would learn in a standard graduate-level statistics course, but I reference articles and textbooks that provide more detail for interested readers. This Tutorial includes snippets of R code throughout; the data and R script used to build the models described in the text are available via OSF at https://osf.io/v6qag/, so readers can follow along if they wish. The goal of this practical introduction is to provide researchers with the tools they need to begin implementing mixed-effects models in their own research.},
  langid = {english},
  keywords = {language,mixed-effects modeling,open data,R,speech perception},
  file = {/Users/timokoch/Zotero/storage/6ET46ETC/Brown - 2021 - An Introduction to Linear Mixed-Effects Modeling i.pdf}
}

@article{brysbaertPowerAnalysisEffect2018,
  title = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}: {{A Tutorial}}},
  shorttitle = {Power {{Analysis}} and {{Effect Size}} in {{Mixed Effects Models}}},
  author = {Brysbaert, Marc and Stevens, Micha{\"e}l},
  year = {2018},
  month = jan,
  journal = {Journal of Cognition},
  volume = {1},
  number = {1},
  pages = {9},
  issn = {2514-4820},
  doi = {10.5334/joc.10},
  urldate = {2022-07-14},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/N9VC25CC/Brysbaert und Stevens - 2018 - Power Analysis and Effect Size in Mixed Effects Mo.pdf}
}

@article{burknerBrmsPackageBayesian2017,
  title = {Brms: {{An R Package}} for {{Bayesian Multilevel Models Using Stan}}},
  shorttitle = {Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2017},
  month = aug,
  journal = {Journal of Statistical Software},
  volume = {80},
  pages = {1--28},
  issn = {1548-7660},
  doi = {10.18637/jss.v080.i01},
  urldate = {2023-08-11},
  abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit  -  among others  -  linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as meta-analytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation.},
  copyright = {Copyright (c) 2017 Paul-Christian B\"urkner},
  langid = {english},
  keywords = {Bayesian inference,MCMC,multilevel model,ordinal data,R,Stan},
  file = {/Users/timokoch/Zotero/storage/466XWGHA/Bürkner - 2017 - brms An R Package for Bayesian Multilevel Models .pdf}
}

@article{chambersPresentFutureRegistered2022,
  title = {The Past, Present and Future of {{Registered Reports}}},
  author = {Chambers, Christopher D. and Tzavella, Loukia},
  year = {2022},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {6},
  number = {1},
  pages = {29--42},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01193-7},
  urldate = {2023-09-11},
  abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
  copyright = {2021 Springer Nature Limited},
  langid = {english},
  keywords = {Culture,Publishing},
  file = {/Users/timokoch/Zotero/storage/7VLRUK5E/Chambers und Tzavella - 2022 - The past, present and future of Registered Reports.pdf}
}

@article{champelyPackagePwr2018,
  title = {Package `Pwr'},
  author = {Champely, Stephane and Ekstrom, Claus and Dalgaard, Peter and Gill, Jeffrey and Weibelzahl, Stephan and Anandkumar, Aditya and Ford, Clay and Volcic, Robert and De Rosario, Helios and De Rosario, Maintainer Helios},
  year = {2018},
  journal = {R package version},
  volume = {1},
  number = {2},
  keywords = {⛔ No DOI found}
}

@article{cockburnThreatsReplicationCrisis2020,
  title = {Threats of a Replication Crisis in Empirical Computer Science},
  author = {Cockburn, Andy and Dragicevic, Pierre and Besan{\c c}on, Lonni and Gutwin, Carl},
  year = {2020},
  month = jul,
  journal = {Communications of the ACM},
  volume = {63},
  number = {8},
  pages = {70--79},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3360311},
  urldate = {2023-09-05},
  abstract = {Research replication only works if there is confidence built into the results.},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/APYYCZ6X/Cockburn et al. - 2020 - Threats of a replication crisis in empirical compu.pdf}
}

@misc{debruineFauxSimulationFactorial2023,
  title = {Faux: {{Simulation}} for {{Factorial Designs}}},
  shorttitle = {Faux},
  author = {DeBruine, Lisa},
  year = {2023},
  month = feb,
  doi = {10.5281/ZENODO.2669586},
  urldate = {2023-08-03},
  abstract = {Create datasets with factorial structure through simulation by specifying variable parameters.},
  copyright = {MIT License, Open Access},
  howpublished = {Zenodo},
  langid = {english}
}

@misc{debruineFauxSimulationFactorial2023a,
  title = {Faux: {{Simulation}} for {{Factorial Designs}}},
  shorttitle = {Faux},
  author = {DeBruine, Lisa},
  year = {2023},
  month = feb,
  doi = {10.5281/ZENODO.2669586},
  urldate = {2023-08-18},
  abstract = {Create datasets with factorial structure through simulation by specifying variable parameters.},
  copyright = {MIT License, Open Access},
  howpublished = {Zenodo},
  langid = {english}
}

@article{debruineUnderstandingMixedEffectsModels2021,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}}},
  author = {DeBruine, Lisa and Barr, Dale J.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920965119},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920965119},
  urldate = {2022-07-14},
  abstract = {Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed-effects models. However, much of this research is analyzed using analysis of variance on aggregated responses because researchers are not confident specifying and interpreting mixed-effects models. This Tutorial explains how to simulate data with random-effects structure and analyze the data using linear mixed-effects regression (with the lme4 R package), with a focus on interpreting the output in light of the simulated parameters. Data simulation not only can enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs. All materials associated with this article can be accessed at https://osf.io/3cz2e/.},
  langid = {english},
  keywords = {lme4,mixed-effects models,open materials,power,R,simulation},
  file = {/Users/timokoch/Zotero/storage/E6KAKUDI/DeBruine und Barr - 2021 - Understanding Mixed-Effects Models Through Data Si.pdf}
}

@article{dmitrienkoTraditionalMultiplicityAdjustment2013,
  title = {Traditional Multiplicity Adjustment Methods in Clinical Trials},
  author = {Dmitrienko, Alex and D'Agostino, Ralph},
  year = {2013},
  month = dec,
  journal = {Statistics in Medicine},
  volume = {32},
  number = {29},
  pages = {5172--5218},
  issn = {02776715},
  doi = {10.1002/sim.5990},
  urldate = {2023-08-18},
  langid = {english}
}

@inproceedings{echtlerOpenSourceOpen2018,
  title = {Open {{Source}}, {{Open Science}}, and the {{Replication Crisis}} in {{HCI}}},
  booktitle = {Extended {{Abstracts}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Echtler, Florian and H{\"a}u{\ss}ler, Maximilian},
  year = {2018},
  month = apr,
  series = {{{CHI EA}} '18},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3170427.3188395},
  urldate = {2023-09-05},
  abstract = {The open-source model of software development is an established and widely used method that has been making inroads into several scientific disciplines which use software, thereby also helping much-needed efforts at replication of scientific results. However, our own discipline of HCI does not seem to follow this trend so far. We analyze the entire body of papers from CHI 2016 and CHI 2017 regarding open-source releases, and compare our results with the discipline of bioinformatics. Based on our comparison, we suggest future directions for publication practices in HCI in order to improve scientific rigor and replicability.},
  isbn = {978-1-4503-5621-3},
  keywords = {hci,open science,open source,replication}
}

@inproceedings{echtlerOpenSourceOpen2018a,
  title = {Open {{Source}}, {{Open Science}}, and the {{Replication Crisis}} in {{HCI}}},
  booktitle = {Extended {{Abstracts}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Echtler, Florian and H{\"a}u{\ss}ler, Maximilian},
  year = {2018},
  month = apr,
  pages = {1--8},
  publisher = {{ACM}},
  address = {{Montreal QC Canada}},
  doi = {10.1145/3170427.3188395},
  urldate = {2023-09-12},
  abstract = {The open-source model of software development is an established and widely used method that has been making inroads into several scientific disciplines which use software, thereby also helping much-needed efforts at replication of scientific results. However, our own discipline of HCI does not seem to follow this trend so far. We analyze the entire body of papers from CHI 2016 and CHI 2017 regarding open-source releases, and compare our results with the discipline of bioinformatics. Based on our comparison, we suggest future directions for publication practices in HCI in order to improve scientific rigor and replicability.},
  isbn = {978-1-4503-5621-3},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/PPQ5ATYT/Echtler und Häußler - 2018 - Open Source, Open Science, and the Replication Cri.pdf}
}

@book{fahrmeirRegressionModelsMethods2021,
  title = {Regression: {{Models}}, {{Methods}} and {{Applications}}},
  shorttitle = {Regression},
  author = {Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan and Marx, Brian D.},
  year = {2021},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-63882-8},
  urldate = {2023-08-18},
  isbn = {978-3-662-63881-1 978-3-662-63882-8},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/P75BJQUB/Fahrmeir et al. - 2021 - Regression Models, Methods and Applications.pdf}
}

@article{faulStatisticalPowerAnalyses2009,
  title = {Statistical Power Analyses Using {{G}}*{{Power}} 3.1: {{Tests}} for Correlation and Regression Analyses},
  shorttitle = {Statistical Power Analyses Using {{G}}*{{Power}} 3.1},
  author = {Faul, Franz and Erdfelder, Edgar and Buchner, Axel and Lang, Albert-Georg},
  year = {2009},
  month = nov,
  journal = {Behavior Research Methods},
  volume = {41},
  number = {4},
  pages = {1149--1160},
  issn = {1554-3528},
  doi = {10.3758/BRM.41.4.1149},
  urldate = {2023-09-05},
  abstract = {G*Power is a free power analysis program for a variety of statistical tests. We present extensions and improvements of the version introduced by Faul, Erdfelder, Lang, and Buchner (2007) in the domain of correlation and regression analyses. In the new version, we have added procedures to analyze the power of tests based on (1) single-sample tetrachoric correlations, (2) comparisons of dependent correlations, (3) bivariate linear regression, (4) multiple linear regression based on the random predictor model, (5) logistic regression, and (6) Poisson regression. We describe these new features and provide a brief introduction to their scope and handling.},
  langid = {english},
  keywords = {Effect Size Measure,Implicit Association Test,Linear Multiple Regression,Multiple Correlation Coefficient,Noncentrality Parameter},
  file = {/Users/timokoch/Zotero/storage/LQK3YIMQ/Faul et al. - 2009 - Statistical power analyses using GPower 3.1 Test.pdf}
}

@article{greenSIMRPackagePower2016,
  title = {{{SIMR}}: An {{R}} Package for Power Analysis of Generalized Linear Mixed Models by Simulation},
  shorttitle = {{{SIMR}}},
  author = {Green, Peter and MacLeod, Catriona J.},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {4},
  pages = {493--498},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12504},
  urldate = {2023-04-24},
  abstract = {The r package simr allows users to calculate power for generalized linear mixed models from the lme4 package. The power calculations are based on Monte Carlo simulations. It includes tools for (i) running a power analysis for a given model and design; and (ii) calculating power curves to assess trade-offs between power and sample size. This paper presents a tutorial using a simple example of count data with mixed effects (with structure representative of environmental monitoring data) to guide the user along a gentle learning curve, adding only a few commands or options at a time.},
  langid = {english},
  keywords = {experimental design,glmm,Monte Carlo,random effects,sample size,type II error},
  file = {/Users/timokoch/Zotero/storage/YCQ47I3E/Green und MacLeod - 2016 - SIMR an R package for power analysis of generaliz.pdf;/Users/timokoch/Zotero/storage/CYYNEQY9/2041-210X.html}
}

@article{heGeneralizedLinearMixedeffects2022,
  title = {Generalized Linear Mixed-Effects Models for Studies Using Different Sets of Stimuli across Conditions},
  author = {He, ShunCheng and Lee, Wooyeol},
  year = {2022},
  journal = {Frontiers in Psychology},
  volume = {13},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2022.955722},
  urldate = {2023-09-05},
  abstract = {A non-repeated item (NRI) design refers to an experimental design in which items used in one level of experimental conditions are not repeatedly used at other levels. Recent literature has suggested the use of generalized linear mixed-effects models (GLMMs) for experimental data analysis, but the existing specification of GLMMs does not account for all possible dependencies among the outcomes in NRI designs. Therefore, the current study proposed a GLMM with a level-specific item random effect for NRI designs. The hypothesis testing performance of the newly proposed model was evaluated via a simulation study to detect the experimental condition effect. The model with a level-specific item random effect performed better than the existing model in terms of power when the variance of the item effect was heterogeneous. Based on these results, we suggest that experimental researchers using NRI designs consider setting a level-specific item random effect in the model.},
  file = {/Users/timokoch/Zotero/storage/493VLEM5/He und Lee - 2022 - Generalized linear mixed-effects models for studie.pdf}
}

@article{hothornSimultaneousInferenceGeneral2008,
  title = {Simultaneous {{Inference}} in {{General Parametric Models}}},
  author = {Hothorn, Torsten and Bretz, Frank and Westfall, Peter},
  year = {2008},
  month = jun,
  journal = {Biometrical Journal},
  volume = {50},
  number = {3},
  pages = {346--363},
  issn = {03233847, 15214036},
  doi = {10.1002/bimj.200810425},
  urldate = {2023-08-18},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/3FRQQ26Y/Hothorn et al. - 2008 - Simultaneous Inference in General Parametric Model.pdf}
}

@article{johnsonPowerAnalysisGeneralized2015,
  title = {Power Analysis for Generalized Linear Mixed Models in Ecology and Evolution},
  author = {Johnson, Paul C. D. and Barry, Sarah J. E. and Ferguson, Heather M. and M{\"u}ller, Pie},
  year = {2015},
  journal = {Methods in Ecology and Evolution},
  volume = {6},
  number = {2},
  pages = {133--142},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12306},
  urldate = {2023-09-13},
  abstract = {`Will my study answer my research question?' is the most fundamental question a researcher can ask when designing a study, yet when phrased in statistical terms \textendash{} `What is the power of my study?' or `How precise will my parameter estimate be?' \textendash{} few researchers in ecology and evolution (EE) try to answer it, despite the detrimental consequences of performing under- or over-powered research. We suggest that this reluctance is due in large part to the unsuitability of simple methods of power analysis (broadly defined as any attempt to quantify prospectively the `informativeness' of a study) for the complex models commonly used in EE research. With the aim of encouraging the use of power analysis, we present simulation from generalized linear mixed models (GLMMs) as a flexible and accessible approach to power analysis that can account for random effects, overdispersion and diverse response distributions. We illustrate the benefits of simulation-based power analysis in two research scenarios: estimating the precision of a survey to estimate tick burdens on grouse chicks and estimating the power of a trial to compare the efficacy of insecticide-treated nets in malaria mosquito control. We provide a freely available R function, sim.glmm, for simulating from GLMMs. Analysis of simulated data revealed that the effects of accounting for realistic levels of random effects and overdispersion on power and precision estimates were substantial, with correspondingly severe implications for study design in the form of up to fivefold increases in sampling effort. We also show the utility of simulations for identifying scenarios where GLMM-fitting methods can perform poorly. These results illustrate the inadequacy of standard analytical power analysis methods and the flexibility of simulation-based power analysis for GLMMs. The wider use of these methods should contribute to improving the quality of study design in EE.},
  copyright = {\textcopyright{} 2014 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
  langid = {english},
  keywords = {experimental design,generalized linear mixed model,long-lasting insecticidal net,overdispersion,precision,random effects,sample size,simulation},
  file = {/Users/timokoch/Zotero/storage/RK7LIRY2/Johnson et al. - 2015 - Power analysis for generalized linear mixed models.pdf}
}

@inproceedings{kapteinRethinkingStatisticalAnalysis2012,
  title = {Rethinking Statistical Analysis Methods for {{CHI}}},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kaptein, Maurits and Robertson, Judy},
  year = {2012},
  month = may,
  series = {{{CHI}} '12},
  pages = {1105--1114},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2207676.2208557},
  urldate = {2023-09-05},
  abstract = {CHI researchers typically use a significance testing approach to statistical analysis when testing hypotheses during usability evaluations. However, the appropriateness of this approach is under increasing criticism, with statisticians, economists, and psychologists arguing against the use of routine interpretation of results using "canned" p values. Three problems with current practice - the fallacy of the transposed conditional, a neglect of power, and the reluctance to interpret the size of effects - can lead us to build weak theories based on vaguely specified hypothesis, resulting in empirical studies which produce results that are of limited practical or scientific use. Using publicly available data presented at CHI 2010 [19] as an example we address each of the three concerns and promote consideration of the magnitude and actual importance of effects, as opposed to statistical significance, as the new criteria for evaluating CHI research.},
  isbn = {978-1-4503-1015-4},
  keywords = {bayesian statistics,research methods,usability evaluation},
  file = {/Users/timokoch/Zotero/storage/GLNDLPAR/Kaptein und Robertson - 2012 - Rethinking statistical analysis methods for CHI.pdf}
}

@incollection{kapteinUsingGeneralizedLinear2016,
  title = {Using {{Generalized Linear}} ({{Mixed}}) {{Models}} in {{HCI}}},
  booktitle = {Modern {{Statistical Methods}} for {{HCI}}},
  author = {Kaptein, Maurits},
  editor = {Robertson, Judy and Kaptein, Maurits},
  year = {2016},
  pages = {251--274},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-26633-6_11},
  urldate = {2023-09-05},
  abstract = {In HCI we often encounter dependent variables which are not (conditionally) normally distributed: we measure response-times, mouse-clicks, or the number of dialog steps it took a user to complete a task. Furthermore, we often encounter nested or grouped data; users are grouped within companies or institutes, or we obtain multiple observations within users. The standard linear regression models and ANOVAs used to analyze our experimental data are not always feasible in such cases since their assumptions are violated, or the predictions from the fitted models are outside the range of the observed data. In this chapter we introduce extensions to the standard linear model (LM) to enable the analysis of these data. The use of [R] to fit both Generalized Linear Models (GLMs) as well as Generalized Linear Mixed Models (GLMMs, also known as random effects models or hierarchical models) is explained. The chapter also briefly covers regularized regression models which are hardly used in the social sciences despite the fact that these models are extremely popular in Machine Learning, often for good reasons. We end with a number of recommendations for further reading on the topics that are introduced: the current text serves as a basic introduction.},
  isbn = {978-3-319-26631-2 978-3-319-26633-6},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/DXTM4TDX/Kaptein - 2016 - Using Generalized Linear (Mixed) Models in HCI.pdf}
}

@inproceedings{kayResearcherCenteredDesignStatistics2016,
  title = {Researcher-{{Centered Design}} of {{Statistics}}: {{Why Bayesian Statistics Better Fit}} the {{Culture}} and {{Incentives}} of {{HCI}}},
  shorttitle = {Researcher-{{Centered Design}} of {{Statistics}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kay, Matthew and Nelson, Gregory L. and Hekler, Eric B.},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {4521--4532},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2858036.2858465},
  urldate = {2023-09-05},
  abstract = {A core tradition of HCI lies in the experimental evaluation of the effects of techniques and interfaces to determine if they are useful for achieving their purpose. However, our individual analyses tend to stand alone, and study results rarely accrue in more precise estimates via meta-analysis: in a literature search, we found only 56 meta-analyses in HCI in the ACM Digital Library, 3 of which were published at CHI (often called the top HCI venue). Yet meta-analysis is the gold standard for demonstrating robust quantitative knowledge. We treat this as a user-centered design problem: the failure to accrue quantitative knowledge is not the users' (i.e. researchers') failure, but a failure to consider those users' needs when designing statistical practice. Using simulation, we compare hypothetical publication worlds following existing frequentist against Bayesian practice. We show that Bayesian analysis yields more precise effects with each new study, facilitating knowledge accrual without traditional meta-analyses. Bayesian practices also allow more principled conclusions from small-n studies of novel techniques. These advantages make Bayesian practices a likely better fit for the culture and incentives of the field. Instead of admonishing ourselves to spend resources on larger studies, we propose using tools that more appropriately analyze small studies and encourage knowledge accrual from one study to the next. We also believe Bayesian methods can be adopted from the bottom up without the need for new incentives for replication or meta-analysis. These techniques offer the potential for a more user- (i.e. researcher-) centered approach to statistical analysis in HCI.},
  isbn = {978-1-4503-3362-7},
  keywords = {bayesian statistics,effect size,estimation,meta-analysis,replication,small stud-ies},
  file = {/Users/timokoch/Zotero/storage/CCUKDBDR/Kay et al. - 2016 - Researcher-Centered Design of Statistics Why Baye.pdf}
}

@article{kingPointMinimalImportant2011,
  title = {A Point of Minimal Important Difference ({{MID}}): A Critique of Terminology and Methods},
  shorttitle = {A Point of Minimal Important Difference ({{MID}})},
  author = {King, Madeleine T},
  year = {2011},
  month = apr,
  journal = {Expert Review of Pharmacoeconomics \& Outcomes Research},
  volume = {11},
  number = {2},
  pages = {171--184},
  issn = {1473-7167, 1744-8379},
  doi = {10.1586/erp.11.9},
  urldate = {2023-08-18},
  langid = {english}
}

@article{kumleEstimatingPowerGeneralized2021,
  title = {Estimating Power in (Generalized) Linear Mixed Models: {{An}} Open Introduction and Tutorial in {{R}}},
  shorttitle = {Estimating Power in (Generalized) Linear Mixed Models},
  author = {Kumle, Levi and V{\~o}, Melissa L.-H. and Draschkow, Dejan},
  year = {2021},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {53},
  number = {6},
  pages = {2528--2543},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01546-0},
  urldate = {2022-07-07},
  abstract = {Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to analytic power solutions are simulation-based power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
  langid = {english},
  keywords = {lme4,Mixed models,mixedpower,Power,R,Simulation},
  file = {/Users/timokoch/Zotero/storage/YR3EVKIK/Kumle et al. - 2021 - Estimating power in (generalized) linear mixed mod.pdf}
}

@article{lafitSelectionNumberParticipants2021,
  title = {Selection of the {{Number}} of {{Participants}} in {{Intensive Longitudinal Studies}}: {{A User-Friendly Shiny App}} and {{Tutorial}} for {{Performing Power Analysis}} in {{Multilevel Regression Models That Account}} for {{Temporal Dependencies}}},
  shorttitle = {Selection of the {{Number}} of {{Participants}} in {{Intensive Longitudinal Studies}}},
  author = {Lafit, Ginette and Adolf, Janne K. and Dejonckheere, Egon and {Myin-Germeys}, Inez and Viechtbauer, Wolfgang and Ceulemans, Eva},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {251524592097873},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245920978738},
  urldate = {2023-08-08},
  abstract = {In recent years, the popularity of procedures for collecting intensive longitudinal data, such as the experience-sampling method, has increased greatly. The data collected using such designs allow researchers to study the dynamics of psychological functioning and how these dynamics differ across individuals. To this end, the data are often modeled with multilevel regression models. An important question that arises when researchers design intensive longitudinal studies is how to determine the number of participants needed to test specific hypotheses regarding the parameters of these models with sufficient power. Power calculations for intensive longitudinal studies are challenging because of the hierarchical data structure in which repeated observations are nested within the individuals and because of the serial dependence that is typically present in these data. We therefore present a user-friendly application and step-by-step tutorial for performing simulation-based power analyses for a set of models that are popular in intensive longitudinal research. Because many studies use the same sampling protocol (i.e., a fixed number of at least approximately equidistant observations) within individuals, we assume that this protocol is fixed and focus on the number of participants. All included models explicitly account for the temporal dependencies in the data by assuming serially correlated errors or including autoregressive effects.},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/32ZYBH9U/Lafit et al. - 2021 - Selection of the Number of Participants in Intensi.pdf}
}

@misc{lakensImprovingYourStatistical2022,
  title = {Improving {{Your Statistical Inferences}}},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = apr,
  doi = {10.5281/ZENODO.6409077},
  urldate = {2023-08-09},
  abstract = {This open educational resource contains information to improve statistical inferences, design better experiments, and report scientific research more transparently.},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International, Open Access},
  howpublished = {Zenodo},
  keywords = {experimental design,frequentist statistics,hypothesis testing,open science,statistical inferences}
}

@article{lakensJustifyYourAlpha2018,
  title = {Justify Your Alpha},
  author = {Lakens, Dani{\"e}l and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and {van Harmelen}, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and {de Oliveira}, Cilene Lino and {de Xivry}, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k{a}}tkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {3},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  urldate = {2022-03-17},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/Q2TZAF3Z/Lakens et al. - 2018 - Justify your alpha.pdf}
}

@article{lakensSampleSizeJustification2022,
  title = {Sample {{Size Justification}}},
  author = {Lakens, Dani{\"e}l},
  year = {2022},
  month = mar,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  urldate = {2023-04-24},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  file = {/Users/timokoch/Zotero/storage/RIYUXBC7/Lakens - 2022 - Sample Size Justification.pdf;/Users/timokoch/Zotero/storage/MMJADTTA/Sample-Size-Justification.html}
}

@article{lanePowerStrugglesEstimating2018,
  title = {Power Struggles: {{Estimating}} Sample Size for Multilevel Relationships Research},
  shorttitle = {Power Struggles},
  author = {Lane, Sean P. and Hennes, Erin P.},
  year = {2018},
  month = jan,
  journal = {Journal of Social and Personal Relationships},
  volume = {35},
  number = {1},
  pages = {7--31},
  issn = {0265-4075, 1460-3608},
  doi = {10.1177/0265407517710342},
  urldate = {2023-08-08},
  langid = {english}
}

@article{lukeEvaluatingSignificanceLinear2017,
  title = {Evaluating Significance in Linear Mixed-Effects Models in {{R}}},
  author = {Luke, Steven G.},
  year = {2017},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {49},
  number = {4},
  pages = {1494--1502},
  issn = {1554-3528},
  doi = {10.3758/s13428-016-0809-y},
  urldate = {2023-04-24},
  abstract = {Mixed-effects models are being used ever more frequently in the analysis of experimental data. However, in the lme4 package in R the standards for evaluating significance of fixed effects in these models (i.e., obtaining p-values) are somewhat vague. There are good reasons for this, but as researchers who are using these models are required in many cases to report p-values, some method for evaluating the significance of the model output is needed. This paper reports the results of simulations showing that the two most common methods for evaluating significance, using likelihood ratio tests and applying the z distribution to the Wald t values from the model output (t-as-z), are somewhat anti-conservative, especially for smaller sample sizes. Other methods for evaluating significance, including parametric bootstrapping and the Kenward-Roger and Satterthwaite approximations for degrees of freedom, were also evaluated. The results of these simulations suggest that Type 1 error rates are closest to .05 when models are fitted using REML and p-values are derived using the Kenward-Roger or Satterthwaite approximations, as these approximations both produced acceptable Type 1 error rates even for smaller samples.},
  langid = {english},
  keywords = {Linear mixed-effects models,lme4,p-values,Statistics,Type 1 error},
  file = {/Users/timokoch/Zotero/storage/TLJVWC3F/Luke - 2017 - Evaluating significance in linear mixed-effects mo.pdf}
}

@article{maierJustifyYourAlpha2022,
  title = {Justify {{Your Alpha}}: {{A Primer}} on {{Two Practical Approaches}}},
  shorttitle = {Justify {{Your Alpha}}},
  author = {Maier, Maximilian and Lakens, Dani{\"e}l},
  year = {2022},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {2},
  pages = {251524592210803},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/25152459221080396},
  urldate = {2022-12-20},
  abstract = {The default use of an alpha level of .05 is suboptimal for two reasons. First, decisions based on data can be made more efficiently by choosing an alpha level that minimizes the combined Type 1 and Type 2 error rate. Second, it is possible that in studies with very high statistical power, p values lower than the alpha level can be more likely when the null hypothesis is true than when the alternative hypothesis is true (i.e., Lindley's paradox). In this article, we explain two approaches that can be used to justify a better choice of an alpha level than relying on the default threshold of .05. The first approach is based on the idea to either minimize or balance Type 1 and Type 2 error rates. The second approach lowers the alpha level as a function of the sample size to prevent Lindley's paradox. An R package and Shiny app are provided to perform the required calculations. Both approaches have their limitations (e.g., the challenge of specifying relative costs and priors) but can offer an improvement to current practices, especially when sample sizes are large. The use of alpha levels that are better justified should improve statistical inferences and can increase the efficiency and informativeness of scientific research.},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/VWYQYDEV/Maier und Lakens - 2022 - Justify Your Alpha A Primer on Two Practical Appr.pdf}
}

@article{maxwellSampleSizePlanning2008,
  title = {Sample {{Size Planning}} for {{Statistical Power}} and {{Accuracy}} in {{Parameter Estimation}}},
  author = {Maxwell, Scott E. and Kelley, Ken and Rausch, Joseph R.},
  year = {2008},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {59},
  number = {1},
  pages = {537--563},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.59.103006.093735},
  urldate = {2023-08-08},
  abstract = {This review examines recent advances in sample size planning, not only from the perspective of an individual researcher, but also with regard to the goal of developing cumulative knowledge. Psychologists have traditionally thought of sample size planning in terms of power analysis. Although we review recent advances in power analysis, our main focus is the desirability of achieving accurate parameter estimates, either instead of or in addition to obtaining sufficient power. Accuracy in parameter estimation (AIPE) has taken on increasing importance in light of recent emphasis on effect size estimation and formation of confidence intervals. The review provides an overview of the logic behind sample size planning for AIPE and summarizes recent advances in implementing this approach in designs commonly used in psychological research.},
  langid = {english}
}

@article{meteyardBestPracticeGuidance2020,
  title = {Best Practice Guidance for Linear Mixed-Effects Models in Psychological Science},
  author = {Meteyard, Lotte and Davies, Robert A. I.},
  year = {2020},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {112},
  pages = {104092},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2020.104092},
  urldate = {2023-04-24},
  abstract = {The use of Linear Mixed-effects Models (LMMs) is set to dominate statistical analyses in psychological science and may become the default approach to analyzing quantitative data. The rapid growth in adoption of LMMs has been matched by a proliferation of differences in practice. Unless this diversity is recognized, and checked, the field shall reap enormous difficulties in the future when attempts are made to consolidate or synthesize research findings. Here we examine this diversity using two methods \textendash{} a survey of researchers (n~=~163) and a quasi-systematic review of papers using LMMs (n~=~400). The survey reveals substantive concerns among psychologists using or planning to use LMMs and an absence of agreed standards. The review of papers complements the survey, showing variation in how the models are built, how effects are evaluated and, most worryingly, how models are reported. Using these data as our departure point, we present a set of best practice guidance, focusing on the reporting of LMMs. It is the authors' intention that the paper supports a step-change in the reporting of LMMs across the psychological sciences, preventing a trajectory in which findings reported today cannot be transparently understood and used tomorrow.},
  langid = {english},
  keywords = {Hierarchical models,Linear mixed effects models,Multilevel models},
  file = {/Users/timokoch/Zotero/storage/I3I8TLAM/Meteyard und Davies - 2020 - Best practice guidance for linear mixed-effects mo.pdf;/Users/timokoch/Zotero/storage/WY8PX7FR/S0749596X20300061.html}
}

@article{murayamaSummarystatisticsbasedPowerAnalysis2022,
  title = {Summary-Statistics-Based Power Analysis: {{A}} New and Practical Method to Determine Sample Size for Mixed-Effects Modeling.},
  shorttitle = {Summary-Statistics-Based Power Analysis},
  author = {Murayama, Kou and Usami, Satoshi and Sakaki, Michiko},
  year = {2022},
  month = jan,
  journal = {Psychological Methods},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000330},
  urldate = {2023-08-07},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/YSRNCZBA/Murayama et al. - 2022 - Summary-statistics-based power analysis A new and.pdf}
}

@inproceedings{phelanPriorExperienceNecessary2019,
  title = {Some {{Prior}}(s) {{Experience Necessary}}: {{Templates}} for {{Getting Started With Bayesian Analysis}}},
  shorttitle = {Some {{Prior}}(s) {{Experience Necessary}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Phelan, Chanda and Hullman, Jessica and Kay, Matthew and Resnick, Paul},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3290605.3300709},
  urldate = {2023-09-04},
  abstract = {Bayesian statistical analysis has gained attention in recent years, including in HCI. The Bayesian approach has several advantages over traditional statistics, including producing results with more intuitive interpretations. Despite growing interest, few papers in CHI use Bayesian analysis. Existing tools to learn Bayesian statistics require significant time investment, making it difficult to casually explore Bayesian methods. Here, we present a tool that lowers the barrier to exploration: a set of R code templates that guide Bayesian novices through their first analysis. The templates are tailored to CHI, supporting analyses found to be most common in recent CHI papers. In a user study, we found that the templates were easy to understand and use. However, we found that participants without a statistical background were not confident in their use. Together our contributions provide a concise analysis tool and empirical results for understanding and addressing barriers to using Bayesian analysis in HCI.},
  isbn = {978-1-4503-5970-2},
  keywords = {bayesian statistics,code templates,evaluation,hypothesis testing,statistics,tutorials}
}

@article{raudenbushStatisticalPowerOptimal,
  title = {Statistical {{Power}} and {{Optimal Design}} for {{Multisite Randomized Trials}}},
  author = {Raudenbush, Stephen W and Liu, Xiaofeng},
  doi = {10.1037/1082-989X.5.2.199},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/6QYR8ETB/Raudenbush und Liu - Statistical Power and Optimal Design for Multisite.pdf}
}

@incollection{robertsonImprovingStatisticalPractice2016,
  title = {Improving {{Statistical Practice}} in {{HCI}}},
  booktitle = {Modern {{Statistical Methods}} for {{HCI}}},
  author = {Robertson, Judy and Kaptein, Maurits},
  editor = {Robertson, Judy and Kaptein, Maurits},
  year = {2016},
  pages = {331--348},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-26633-6_14},
  urldate = {2023-09-05},
  isbn = {978-3-319-26631-2 978-3-319-26633-6},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/XJ56VFD8/Robertson und Kaptein - 2016 - Improving Statistical Practice in HCI.pdf}
}

@book{robertsonModernStatisticalMethods2016,
  title = {Modern {{Statistical Methods}} for {{HCI}}},
  editor = {Robertson, Judy and Kaptein, Maurits},
  year = {2016},
  series = {Human\textendash{{Computer Interaction Series}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-26633-6},
  urldate = {2023-09-05},
  isbn = {978-3-319-26631-2 978-3-319-26633-6},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/NHP7PTVL/Robertson und Kaptein - 2016 - Modern Statistical Methods for HCI.pdf}
}

@book{robertsonModernStatisticalMethods2016a,
  title = {Modern {{Statistical Methods}} for {{HCI}}},
  editor = {Robertson, Judy and Kaptein, Maurits},
  year = {2016},
  series = {Human\textendash{{Computer Interaction Series}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-26633-6},
  urldate = {2023-09-05},
  isbn = {978-3-319-26631-2 978-3-319-26633-6},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/BAYH5MAW/Robertson und Kaptein - 2016 - Modern Statistical Methods for HCI.pdf}
}

@article{scherbaumEstimatingStatisticalPower2009,
  title = {Estimating {{Statistical Power}} and {{Required Sample Sizes}} for {{Organizational Research Using Multilevel Modeling}}},
  author = {Scherbaum, Charles A. and Ferreter, Jennifer M.},
  year = {2009},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {12},
  number = {2},
  pages = {347--367},
  issn = {1094-4281, 1552-7425},
  doi = {10.1177/1094428107308906},
  urldate = {2023-08-08},
  abstract = {The use of multilevel modeling to investigate organizational phenomena is rapidly increasing. Unfortunately, little advice is readily available for organizational researchers attempting to determine statistical power when using multilevel models or when determining sample sizes for each level that will maximize statistical power. This article presents an introduction to statistical power in multilevel models. The unique factors influencing power in multilevel models and calculations for estimating power for simple fixed effects, variance components, and cross-level interactions are presented. The results of simulation studies and the existing general rules of thumb are discussed, and the available power analysis software is reviewed.},
  langid = {english}
}

@incollection{singmannIntroductionMixedModels2019,
  title = {An {{Introduction}} to {{Mixed Models}} for {{Experimental Psychology}}},
  booktitle = {New {{Methods}} in {{Cognitive Psychology}}},
  author = {Singmann, Henrik and Kellen, David},
  editor = {Spieler, Daniel and Schumacher, Eric},
  year = {2019},
  month = oct,
  edition = {1},
  pages = {4--31},
  publisher = {{Routledge}},
  doi = {10.4324/9780429318405-2},
  urldate = {2023-04-24},
  isbn = {978-0-429-31840-5},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/CDEQRLAW/Singmann und Kellen - 2019 - An Introduction to Mixed Models for Experimental P.pdf}
}

@article{westfallStatisticalPowerOptimal2014,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  pages = {2020--2045},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222},
  doi = {10.1037/xge0000014},
  abstract = {Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Design,Sampling (Experimental),Statistical Power,Stimulus Parameters},
  file = {/Users/timokoch/Zotero/storage/HVBDJDCL/Westfall et al. - 2014 - Statistical power and optimal design in experiment.pdf;/Users/timokoch/Zotero/storage/HQ9U2NBA/doiLanding.html}
}

@misc{wickelmaierSimulatingPowerStatistical2022,
  title = {Simulating the {{Power}} of {{Statistical Tests}}: {{A Collection}} of {{R Examples}}},
  shorttitle = {Simulating the {{Power}} of {{Statistical Tests}}},
  author = {Wickelmaier, Florian},
  year = {2022},
  month = mar,
  number = {arXiv:2110.09836},
  eprint = {2110.09836},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.09836},
  urldate = {2023-05-30},
  abstract = {This paper illustrates how to calculate the power of a statistical test by computer simulation. It provides R code for power simulations of several classical inference procedures including one- and two-sample t tests, chi-squared tests, regression, and analysis of variance.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications},
  file = {/Users/timokoch/Zotero/storage/5EZR87BR/Wickelmaier - 2022 - Simulating the Power of Statistical Tests A Colle.pdf;/Users/timokoch/Zotero/storage/EDTJ448P/2110.html}
}

@article{yarkoniGeneralizabilityCrisis2022,
  title = {The Generalizability Crisis},
  author = {Yarkoni, Tal},
  year = {2022},
  journal = {Behavioral and Brain Sciences},
  volume = {45},
  pages = {e1},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X20001685},
  urldate = {2022-11-30},
  abstract = {Abstract             Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned \textendash{} that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology \textendash{} the linear mixed model \textendash{} I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that although the ``random effect'' formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/Z4TK4K3B/Yarkoni - 2022 - The generalizability crisis.pdf}
}

@article{yarkoniRepliesCommentariesGeneralizability2022,
  title = {Replies to Commentaries on the Generalizability Crisis},
  author = {Yarkoni, Tal},
  year = {2022},
  journal = {Behavioral and Brain Sciences},
  volume = {45},
  pages = {e40},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X21001758},
  urldate = {2022-06-08},
  abstract = {Abstract             The 38 commentaries on the target article span a broad range of disciplines and perspectives. I have organized my response to the commentaries around three broad questions: First, how serious are the problems discussed in the target article? Second, are there are other, potentially more productive, ways to think about the issues that the target article framed in terms of generalizability? And third, what, if anything, should we collectively do about these problems?},
  langid = {english}
}

@incollection{yataniEffectSizesPower2016,
  title = {Effect {{Sizes}} and {{Power Analysis}} in {{HCI}}},
  booktitle = {Modern {{Statistical Methods}} for {{HCI}}},
  author = {Yatani, Koji},
  editor = {Robertson, Judy and Kaptein, Maurits},
  year = {2016},
  pages = {87--110},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-26633-6_5},
  urldate = {2023-09-05},
  abstract = {Null hypothesis significance testing (NHST) is a common statistical analysis method in HCI. But its usage and interpretation are often misunderstood. In particular, NHST does not offer the magnitude of differences observed, which is more desirable to determine the effect of comparative studies than the p value. Effect sizes and power analysis can mitigate over-reliance on the p value, and offer researchers better informed preparation and interpretation on experiments. Many research fields now require authors to include effect sizes in NHST results, and this trend is expected to be more and more common. In this chapter, I first discuss common misunderstandings of NHST and p value, and how effect sizes can complement them. I then present methods for calculating effect sizes with examples. I also describe another closely related topic, power analysis. Power analysis can be useful for appropriately designing experiments though it is not frequently used in HCI. I present power analysis methods and discuss how they should and should not be used.},
  isbn = {978-3-319-26631-2 978-3-319-26633-6},
  langid = {english},
  file = {/Users/timokoch/Zotero/storage/UQCIIWZJ/Yatani - 2016 - Effect Sizes and Power Analysis in HCI.pdf}
}

@article{yuTestANOVAApplications2022,
  title = {Beyond t Test and {{ANOVA}}: Applications of Mixed-Effects Models for More Rigorous Statistical Analysis in Neuroscience Research},
  shorttitle = {Beyond t Test and {{ANOVA}}},
  author = {Yu, Zhaoxia and Guindani, Michele and Grieco, Steven F. and Chen, Lujia and Holmes, Todd C. and Xu, Xiangmin},
  year = {2022},
  month = jan,
  journal = {Neuron},
  volume = {110},
  number = {1},
  pages = {21--35},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.10.030},
  urldate = {2023-09-05},
  abstract = {In basic neuroscience research, data are often clustered or collected with repeated measures, hence correlated. The most widely used methods such as t test and ANOVA do not take data dependence into account and thus are often misused. This Primer introduces linear and generalized mixed-effects models that consider data dependence and provides clear instruction on how to recognize when they are needed and how to apply them. The appropriate use of mixed-effects models will help researchers improve their experimental design and will lead to data analyses with greater validity and higher reproducibility of the experimental findings.},
  keywords = {Bayesian analysis,clustered data,generalized linear mixed-effects model,linear mixed-effects model,linear regression model,repeated measures},
  file = {/Users/timokoch/Zotero/storage/CNYRSB9R/Yu et al. - 2022 - Beyond t test and ANOVA applications of mixed-eff.pdf}
}

@misc{zimmerSampleSizePlanning2022,
  title = {Sample {{Size Planning}} for {{Complex Study Designs}}: {{A Tutorial}} for the Mlpwr {{Package}}},
  shorttitle = {Sample {{Size Planning}} for {{Complex Study Designs}}},
  author = {Zimmer, Felix and Henninger, Mirka and Debelak, Rudolf},
  year = {2022},
  month = oct,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/r9w6t},
  urldate = {2023-04-24},
  abstract = {A common challenge in designing empirical studies is determining an appropriate sample size. When more complex models are used, estimates of power can only be obtained using Monte Carlo simulations. In this tutorial, we introduce the R package mlpwr to perform simulation-based power analysis based on surrogate modeling. Surrogate modeling is a powerful tool to guide the search for study design parameters that imply a desired power or meet a cost threshold (e.g., in terms of monetary cost). mlpwr can be used to search for the optimal allocation when there are multiple design parameters, e.g., when balancing the number of participants and the number of groups in multilevel modeling. At the same time, the approach can take into account the cost of each design parameter, and aims to find a cost-efficient design. We introduce the basic functionality of the package, which can be applied to a wide range of statistical models and study designs. Additionally, we provide two examples based on empirical studies for illustration: one for sample size planning when using an item response theory model, and one for assigning the number of participants and the number of countries for a study using multilevel modeling.},
  langid = {american},
  keywords = {machine learning,power analysis,Quantitative Methods,sample size,simulation,Social and Behavioral Sciences},
  file = {/Users/timokoch/Zotero/storage/73M67QE6/Zimmer et al. - 2022 - Sample Size Planning for Complex Study Designs A .pdf}
}
