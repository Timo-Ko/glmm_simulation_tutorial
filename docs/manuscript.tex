% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  man,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\makeatletter
\usepackage{etoolbox}
\patchcmd{\maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{power analysis, data simulation, sample size, generalized linear mixed model\newline\indent Word count: 7504}
\usepackage{csquotes}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={A Tutorial on Tailored Simulation-Based Sample Size Planning for Experimental Designs with Generalized Linear Mixed Models},
  pdfauthor={Timo K. Koch,1, 2, Florian Pargent,1, Anne-Kathrin Kleine1, Eva Lermer1,3, \& Susanne Gaube1,4},
  pdflang={en-EN},
  pdfkeywords={power analysis, data simulation, sample size, generalized linear mixed model},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{A Tutorial on Tailored Simulation-Based Sample Size Planning for Experimental Designs with Generalized Linear Mixed Models}
\author{Timo K. Koch\textsuperscript{$\dagger{}$,1, 2}, Florian Pargent\textsuperscript{$\dagger{}$,1}, Anne-Kathrin Kleine\textsuperscript{1}, Eva Lermer\textsuperscript{1,3}, \& Susanne Gaube\textsuperscript{1,4}}
\date{}


\shorttitle{Simulation-Based Sample Size Planning for GLMMs}

\authornote{

This research was funded by a grant from the Volkswagen Foundation (Grant No.~98525).
All materials (reproducible manuscript incl.~R code, simulation results) are available in the project's repository on the Open Science Framework (OSF): \url{https://osf.io/dhwf4/}

The authors made the following contributions. Timo K. Koch: Conceptualization, Formal Analysis, Writing - Original Draft Preparation, Writing - Review \& Editing; Florian Pargent: Formal Analysis, Methodology, Visualization, Writing - Original Draft Preparation, Writing - Review \& Editing; Anne-Kathrin Kleine: Formal Analysis, Writing - Review \& Editing; Eva Lermer: Conceptualization, Resources, Writing - Review \& Editing; Susanne Gaube: Conceptualization, Supervision, Writing - Review \& Editing. \textsuperscript{$\dagger{}$} Timo K. Koch and Florian Pargent contributed equally to this work.

Correspondence concerning this article should be addressed to Timo K. Koch, Torstrasse 25, 9000 St.~Gallen, Switzerland. E-mail: \href{mailto:timo.koch@unisg.ch}{\nolinkurl{timo.koch@unisg.ch}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} LMU Munich, Department of Psychology\\\textsuperscript{2} University of St.~Gallen, Institute of Behavioral Science \& Technology\\\textsuperscript{3} Technical University of Applied Sciences Augsburg, Department of Business Psychology\\\textsuperscript{4} University College London, Global Business School for Health}

\note{\clearpage}

\abstract{%
When planning experimental research, determining an appropriate sample size and using suitable statistical models are crucial for robust and informative results. However, the recent replication crisis underlines the need for more rigorous statistical methodology and well-powered designs. Generalized linear mixed models (GLMMs) offer a flexible statistical framework to analyze experimental data with complex (e.g., dependent and hierarchical) data structures. Yet, available methods and software for a priori power analyses for GLMMs are often limited to specific designs, while data simulation approaches offer more flexibility. Based on a practical case study, the current tutorial equips researchers with a step-by-step guide and corresponding code for conducting tailored a priori power analyses to determine appropriate sample sizes with GLMMs. Finally, we give an outlook on the increasing importance of simulation-based power analysis in experimental research.
}



\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

When planning experimental research, it is essential to determine an appropriate sample size to ensure that the results obtained are both robust and informative, and to use appropriate statistical models to analyze the data (Lakens, 2022b). However, the recent replication crisis has illustrated many challenges surrounding the reproducibility and reliability of findings (Yarkoni, 2022). As a result, there is a growing need for more rigorous statistical methodology and the adoption of well-powered experimental designs. While easy-to-use software solutions exist for simple statistical models and experimental designs, many researchers lack the skills and tools to conduct ``a priori'' (i.e., before data collection) sample size planning for more complex research designs using the flexible generalized linear mixed models (GLMM) framework in order to determine the required sample size in their experiments. In the present work, we provide a tutorial on performing tailored simulation based sample size planning for GLMMs. After introducing general concepts, we review existing software solutions in R and discuss under which circumstances tailored data simulations are necessary. Then we describe general steps and decisions that are necessary whenever performing tailored data simulation. Because the details of these steps vary considerably for each specific study, we finish with a case study from the field of human-AI (artificial intelligence) interaction research.

--\textgreater{}

\hypertarget{prior-knowledge}{%
\section{Prior Knowledge}\label{prior-knowledge}}

TODO: fertig ausformulieren

Simulation-based sample size planning for generalized linear models is a complex topic that touches on a variety of statistical concepts and programming skills.
Although we give some short introduction to clarify our terminology, to profit the most from our tutorial, our readers require prior knowledge in the following topics (or refer to our collection of background literature):

Readers should understand basic statistical concepts like hypothesis tests and their statistical power but also confidence intervals and their precision.
Some background knowledge in empirical estimands, statistical estimates, and causal inference is very useful but not absolutely necessary.

In addition, readers should have basic programming skills in R on how to simulate data and how to repeat these simulations in parallel to speed up computation.
While there are a myriad of options for data simulation in R beyond the basic functionality in the base and stats packages, in our tutorial we use functions from the tidyverse and the faux package.

Finally, readers should be familiar with regression modeling in general and GLMMs in particular.
In our tutorial, we simulate data by manually specifying the model equation of a GLMM that represents our assumed data generating process.
While it is not necessary to understand the technical details of how GLMMs are estimated, it is crucial to understand the structure of a basic GLMM (e.g., logistic regression with random intercepts) and how the model assumes that the values in the dependent variable are determined by the predictor variables and the random effects.

Literature:

\begin{itemize}
\tightlist
\item
  Lakens ebook (Lakens, 2022a) evtl. Andy Field für die Basics?
\item
  R for data science (Wickham, Çetinkaya-Rundel, \& Grolemund, 2023), Bruine article (DeBruine \& Barr, 2021), faux documentation (DeBruine, 2023) für die R skills
\item
  kumle power tutorial (Kumle, Võ, \& Draschkow, 2021) und irgendwelche paper (am besten aus AMPPS) zu GLMMs
\item
  what is your estimand? paper (Lundberg, Johnson, \& Stewart, 2021) für das estimand topic
\end{itemize}

\hypertarget{general-concepts}{%
\section{General concepts}\label{general-concepts}}

\hypertarget{planning-for-statistical-power-or-precision}{%
\subsection{Planning for statistical power or precision}\label{planning-for-statistical-power-or-precision}}

Conducting research with too small sample sizes can have many negative consequences.
First, experiments may yield inconclusive or misleading results, hindering the accumulation of knowledge.
Second, studies that are doomed to never finding a postulated effect due to small sample size waste resources by consuming time, effort, and funding without delivering meaningful results.
For such reasons, many journals and funding agencies now require that a justification of sample size is included in study protocols and grant proposals, recognizing its significance in ensuring robust and meaningful findings.
Although many pragmatic justifications are possible (Lakens, 2022b), the ideal is often considered to determine a suitable sample size a priori (i.e., before the study is conducted) based on some meaningful computation to assure that the study will be able to fulfill its purpose.
In this way, adding a solid sample size calculation to the research process can act as a safeguard for ensuring high-quality research.

--\textgreater{}

The majority of empirical studies in psychology applies hypothesis testing.
As a consequence, the dominant approach for sample size planning is based on power analysis (i.e., planing for power).
However, sample size planning can also be based on the precision of parameter estimates (i.e., planning for precision).

\hypertarget{planning-for-power}{%
\subsubsection{Planning for power}\label{planning-for-power}}

In empirical research relying on hypothesis testing, the most common strategy for determining an adequate sample size is based on statistical power (Lakens, 2022b).
Statistical power is defined as the probability that a hypothesis test has a significant p-value when analyzing repeated samples from a population with a true effect of some pre-specified size.
Less formally, power is described as the probability that a hypothesis test correctly rejects the null hypothesis when the alternative hypothesis is true.
If the sample size (i.e., the number of participants and/or stimuli) used for data collection is insufficient to detect the effects or relationships being investigated with high probability, the study would be considered ``underpowered''.

When planning for power, a target is set for the statistical power of a hypothesis test of interest (a very common heuristic is 0.8).
Assuming some effect size of interest and a desired significance level, a minimum sample size can be determined that (on average) would guarantee reaching this target.

\hypertarget{planning-for-precision}{%
\subsubsection{Planning for precision}\label{planning-for-precision}}

Not all research questions are best answered using hypothesis testing.
Especially for exploratory studies when little previous research has been conducted, only estimating effects of interest is often more useful.
When no hypothesis tests are conducted when analysing the planned study, power analysis is not not available for sample size planning.
Nonetheless, sample size still has a crucial effect on how informative the planned study will be to learn about the effect of interest.
In this case, the expected width of a confidence interval is used as a target for sample size planning.
The values inside a confidence interval are commonly interpreted as plausible values for the quantity of interest the confidence interval is supposed to estimate.
More formally, a 95\%-confidence interval provides the smallest interval with the property that upon repeated sampling, 95\% of individual confidence intervals would include the true quantity of interest.
Thus, a narrow confidence interval (assuming the confidence level is kept at a high value) is more informative about the size of the true effect than a wide interval.
Apart from the desired confidence level, the width of a confidence interval depends strongly on the sample size.
Because bigger samples carry more information, they lead to smaller confidence intervals (assuming everything else stays the same).
When planning for precision, a target is set for the expected width of a confidence interval of interest.
Assuming some effect size of interest and a desired confidence level, a minimum sample size can be determined that (on average) would guarantee reaching this target.

\hypertarget{generalized-linear-mixed-models-glmms}{%
\subsection{Generalized linear mixed models (GLMMs)}\label{generalized-linear-mixed-models-glmms}}

As study designs become more complex, psychological researchers require more sophisticated statistical models to capture the nuanced relationships and grouping structures introduced by their study designs (Yarkoni, 2022).
GLMMs (also called multilevel models) are gaining increasing popularity in analyzing data because they offer a flexible framework for handling data with outcome variables that are not normally distributed (e.g., categorical outcomes) while accounting for both fixed and random effects (Fahrmeir, Kneib, Lang, \& Marx, 2021).

GLMMs are an extension of LMMs (Linear Mixed Models), which are, in turn, extensions of linear regression models that account for correlated data including hierarchical structures (Fahrmeir et al., 2021).
In this context, correlated data means that the value in the outcome variable for one observation may be related (i.e., more similar or less similar) to the value of another observation in a systematic way that is not already accounted for by the usual (fixed) predictor variables (e.g., age of participants).
This correlation can arise for various reasons: Responses to some stimuli from some participants might be more similar because the same person was measured twice (repeated measurements), both participants come from the same neighborhood (clustering) or both participants responded to the same stimulus (stimulus effects).
Thus, modeling such correlations is especially important whenever the data has a clear structure, while the grouping variables can be hierarchically organized (e.g., students nested in schools, schools nested in districts) or not (e.g., students solve math exercises, but neither student sees all exercises).
LMMs are used when the outcome variable is continuous and follows a normal distribution (when conditioned on all fixed and random effects).
They allow for the modeling of fixed effects, which capture the relationships between our usual predictors and the outcome, as well as random effects, which account for the different types of correlation structure and grouping effects exemplified above.
Random effects are typically assumed to follow a normal distribution with a mean of zero and a variance that quantifies the heterogeneity across groups.

As mentioned, GLMMs extend the LMM framework to accommodate non-normally distributed continuous and categorical outcome variables.
GLMMs incorporate both fixed and random effects, similar to LMMs, but also involve a link function that connects the linear combination of predictor variables to the expected value of the outcome variable.
The link function allows for modeling the relationship between predictors and the outcome in a non-linear way that is appropriate for the specific distribution family of the outcome variable.
As an example, think of an experiment with different design factors (e.g., picture, headline) impacting the likelihood of users clicking on an online advertisement.
Here, participants' behavior is measured repeatedly (e.g., over several sessions).
The click patterns of participants in one session are likely to be correlated with their previous sessions.
Finally, the outcome variable is binary (click/no click) for each session, which follows a binomial distribution.

\hypertarget{simulation-based-sample-size-planning-with-glmms}{%
\section{Simulation based sample size planning with GLMMs}\label{simulation-based-sample-size-planning-with-glmms}}

For simpler statistical models, like t-tests, ANOVA, and linear regression, with common study designs (e.g., mean comparison between two groups), user-friendly software for a priori sample size planning is readily available (Champely et al., 2018; Faul, Erdfelder, Buchner, \& Lang, 2009; Kelley, Maxwell, \& Rausch, 2003; Kelley \& Rausch, 2006).
However, these software packages are often not flexible enough to perform sample size planning for complex designs.

Existing approaches for sample size planning with GLMMs have almost exlusively focused on planning for power.
Power analysis methods for multilevel models can be categorized into formula-based methods and simulation-based methods (Murayama, Usami, \& Sakaki, 2022).
Formula-based methods rely on often complicated formulas that can be used to directly calculate power while simulation-based methods rely on repeatedly simulating data with a known true effect size and estimating power empirically (i.e., how often the hypothesis test is significant for the simulated data).
Currently available formula-based software packages for power analysis with multilevel models often do not include GLMMs or are limited to very simple designs (Murayama et al., 2022; Westfall, Kenny, \& Judd, 2014), making it necessary to build data simulations tailored specifically to the study design.
A number of tutorials have been published describing how to perform such simulation-based power analysis for multilevel models (Arend \& Schäfer, 2019; Brysbaert \& Stevens, 2018; DeBruine \& Barr, 2021; Kumle et al., 2021; Lafit et al., 2021; Zimmer, Henninger, \& Debelak, 2022).
However, most of these tutorials focus on linear mixed models (LMMs) and the most common designs (but see Kumle et al., 2021 for a tutorial that also covers more advanced settings). This narrow focus provides limited guidance for researchers faced with more complex study designs, especially when little prior knowledge about plausible effect sizes is available (see the discussion in Kumle et al., 2021).
The necessary presumptions for simulation-based power analysis with GLMMs include assumptions about the distributional form of the outcome variable, the random effects, and the correlation structure. The distributional assumption specifies the distributional family for the outcome variable (when conditioned on all fixed and random effects). Assumptions about the random effects include the assumption of normality (i.e., that the random effects follow a normal distribution) and the covariance structure among the random effects (i.e., if and how they are correlated). Interpreting these presumptions entails understanding the underlying presumptions of the model and ensuring they align with the characteristics of the data being analyzed.
Existing tutorials often rely on heuristics for specifying variance components (e.g., the standard deviation of random intercepts) or assume that results from meta-analyses or data from pilot studies are available to determine plausible values for all model parameters. However, in practice, knowledge about those parameters from prior studies is often limited, which makes specifying assumptions a practical challenge Kumle et al. (2021).

\begin{itemize}
\tightlist
\item
  Insert table on existing R packages for power analysis with GLMMs
\end{itemize}

\hypertarget{when-do-we-profit-from-tailored-data-simulation}{%
\section{When do we profit from tailored data simulation?}\label{when-do-we-profit-from-tailored-data-simulation}}

Performing tailored simulation based sample size planning is more complicated and time consuming than using existing software tools for sample size planning.
Thus, it is worth discussing under which circumstances we most profit from or are forced to perform tailored simulation based sample size planning, because existing software solutions do not meet our requirements.

\hypertarget{complex-study-designs}{%
\subsection{Complex study designs}\label{complex-study-designs}}

TODO: fertig ausformulieren

Real studies are often more complex than the simplified designs assumed by many user-friendly software packages for sample size planning.

\hypertarget{missing-data}{%
\subsubsection{Missing data}\label{missing-data}}

TODO: fertig ausformulieren

Missing data is a frequent issue in applied data analysis.
There can be various reasons for why data is missing.
For example data can be missing completely at random.
Alternatively, subjects might drop out or produce missing data as a result of some covariate also measured in the dataset.
A a different note, many experimental designs contain some conditions in which some values of the predictor variables are missing by design.
Missing by design conditions can make data analysis more complicated because predictors have to be coded in highly specific ways that prevent the estimated GLMM to become unidentified.
Whether missing data has an effect on the result of sample size planning depends on the characteristics of the missing data process.
However, except for very simple scenarios, deciding whether missing data can be safely ignored in the sample size planning is often difficult to decide on a theoretical basis.
A better strategy is to explicitly include the (assumed) process of missing data into the data simulation process.

\hypertarget{data-generating-processes-beyond-standard-glmms}{%
\subsubsection{Data generating processes beyond standard GLMMs}\label{data-generating-processes-beyond-standard-glmms}}

TODO: fertig ausformulieren

While GLMMs are a very flexible model class that can handle a large variety of outcome variables, psychological researchers are becoming increasingly aware that many psychological datasets might profit from even more sophisticated models.
Common examples are zero-inflated outcomes, censoring, and nonlinear predictor effects.

\begin{verbatim}
- possibility to extend beyond standard GLMMs (e.g. zero-inflated outcomes, censoring, nonlinear effects, …)
\end{verbatim}

\hypertarget{complex-statistical-hypotheses}{%
\subsection{Complex statistical hypotheses}\label{complex-statistical-hypotheses}}

TODO: fertig ausformulieren

The most common hypotheses tested in psychological research are of the type \(H_0: \beta = 0\), where \(\beta\) is some coefficient in a regression model.
However, many research questions in psychology actually require testing more complex statistical hypotheses.

\hypertarget{directed-hypotheses-and-testing-against-values-other-than-0}{%
\subsubsection{Directed hypotheses and testing against values other than 0}\label{directed-hypotheses-and-testing-against-values-other-than-0}}

TODO: fertig ausformulieren

In the new era of preregistration and registered reports, most research questions are tested with directed hypotheses, because good theories should at least postulate whether some psychological effect of interest is positive or negative.
Even better theories should be able to specify a smallest effect sizes of interest (SESOI) that must be exceeded if the psychological effect has some practical relevance in a specific application.
In combination, this might require a test like \(H_0: \beta \leq 0.1\).

\hypertarget{combined-hypotheses}{%
\subsubsection{Combined hypotheses}\label{combined-hypotheses}}

TODO: fertig ausformulieren

Elaborate research questions often require testing hypotheses that consist of a combination of model parameters, for example \(H_0: \beta_0 + \beta_1 \leq 0\).
If the research question consists only of a single hypothesis of this sort, it is often possible to reduce the hypothesis to a single regression coefficient by clever coding and/or centering of predictor variables.
However, interesting research questions often consist of combined hypotheses that consist of more than one separate statistical hypotheses.
For example, a combined null hypothesis \(H_0\) might consist of two single null hypotheses \(H_{01}: \beta_1 \leq 0\) and \(H_{02}: \beta_0 + \beta_1 \leq 0\).
For some research questions, the combined null hypothesis \(H_0\) would be rejected if both \(H_{01}\) \emph{AND} \(H_{02}\) are rejected.
For other research questions, the combined null hypothesis \(H_0\) would be rejected if \(H_{01}\) \emph{OR} \(H_{02}\) \emph{OR} both are rejected.
If the global hypothesis \(H_0\) is combined with \emph{OR}, the p-values of the single hypotheses must be corrected for multiple testing to avoid \(\alpha\)-inflation for the global hypothesis.
However, if the global hypothesis \(H_0\) is combined with \emph{AND}, a correction for multiple testing is not necessary but rather a mistake that unnecessarily reduces the power of the global hypothesis test.

None of the software packages for sample size planning in table X can handle combined hypotheses as discussed here.
In contrast, our case study will demonstrate how we can test directed combined hypotheses with tailored simulation based sample size planning.

\hypertarget{planning-for-precision-1}{%
\subsection{Planning for precision}\label{planning-for-precision-1}}

TODO: fertig ausformulieren

As discussed earlier, the two major frameworks for sample size planning are planning for power and planning for precision.
Although planning for precision has been increasingly discussed as a more suitable strategy for exploratory research, all available software packages for sample size planning with GLMMs are based on power analysis.

As long as one were interested only in confidence intervals of a single model parameter, it might be possible to adapt existing software packages to plan for precision.
However, we demonstrate in our case study that in realistic research projects with GLMMs, the contrasts of interests and their respective confidence intervals are usually situated on the scale of the (expected) outcome in contrast to the scale of the model parameters.
We will show that tailored simulation based sample size planning can easily handle the planning for precision scenario.
The only change in procedure is that instead of computing hypothesis tests for each simulated dataset and estimating statistical power across repetitions, confidence intervals are computed for each simulated dataset and the expected width is estimated.

\hypertarget{no-prior-studies-or-pilot-data}{%
\subsection{No prior studies or pilot data}\label{no-prior-studies-or-pilot-data}}

TODO: fertig ausformulieren

All frameworks for sample size planning require the user to make assumptions about the expected effect size.
Assuming the true effect is of this size (or greater), one can compute the (minimum) power of a hypothesis test or the (maximum) expected width of a confidence interval.
Existing software packages for sample size planning for GLMMs usually require to provide the assumed effect in the unit of some standardized measure of effect size.
When the researcher has access to similar studies or pilot data, providing such standardized effect sizes is feasible.
However, this requirement can be an almost impossible challenge when no prior studies of pilot data are available.
This problem is further exacerbated by the fact that GLMMs are so flexible, that general heuristics of what should be considered a small effect do not exist or are difficult to defend.
In our experience, using domain knowledge to construct a tailored data simulation is the only solution to determine plausible effect sizes in the absence of prior evidence.
It would be possible to use these tailored simulations to extract plausible values for standardized effect sizes that could then be inserted in existing software packages for sample size planning.
However, we would argue that when tailored data simulations are necessary anyways, performing the whole desing analysis in a customized way is preferred over using the existing software packages.

\hypertarget{general-steps-in-tailored-simulation-based-sample-size-planning}{%
\section{General steps in tailored simulation-based sample size planning}\label{general-steps-in-tailored-simulation-based-sample-size-planning}}

Although the details differ depending on the specific study characteristics, each tailored simulation based sample size planning requires a series of steps and decisions that we will outline in the remainder of our tutorial.
Will introduce each step in a theoretical section, followed by the practical application based on a case study.
All code in this manuscript and simulation results are available in the project's repository on the Open Science Framework (\url{https://osf.io/dhwf4/}).

\hypertarget{define-the-estimand}{%
\subsection{Define the estimand}\label{define-the-estimand}}

\hypertarget{theory}{%
\subsubsection{THEORY}\label{theory}}

The first step in every research process should always be a clear definition of which theoretical quantity is necessary to answer the specific research question of the study under planning.
Unfortunately, many psychological researchers are still not well trained in this task.
The theoretical estimand is a unit specific quantity defined outside of any statistical model (Lundberg et al., 2021).
Depending on the research and whether the research question is causal, this quantity might be counterfactual and not directly observable.
Part of the theoretical estimand is also a clear definition of the target population, for which the quantity is of interest.

When the theoretical estimand has been specified, the next step is to define the empirical estimand, which reformulate the theoretical estimand in a way that can be estimated based on the observed data that will be collected in the planned study.
For many common research questions in psychology, the empirical estimand can be expressed as a statistical quantities within the scope that can be estimated with a regression model.
Note however, that this is not always possible and that the literature on causal inference includes many estimation strategies beyond regression (CITATION).

When the empirical estimand has been specified, the last step is to clearly define how a concrete estimate of the empirical estimand shall be computed based on the observed study data.
When a regression model is used for estimation, this includes decisions like which statistical framework estimation technique, and software package is used to fit the model, and how to compute hypothesis tests or confidence intervals.

\hypertarget{practice}{%
\subsubsection{PRACTICE}\label{practice}}

In the present case study, we consider the effectiveness of feedback provided by an artificial intelligence (AI) in an AI-enabled diagnostic decision support system.
The context is a clinical setting, where expert radiologists and students under training must detect bleeding based on head scans from computer tomography.
In the investigated AI-enabled diagnostic decision support system, an AI model can provide initial diagnostic advice, which can be used as guidance by the humans that are required to make the final diagnostic decision.
The research goal is to validate the effectiveness of the AI-enabled advice.
We consider the AI-enabled advice as effective, if the following pattern holds, which we will first describe verbally:

\emph{We expect that for BOTH expert radiologists and students under training, correct AI-advice leads to a higher probability of accurately diagnosing a CT scan compared to no AI-advice presented, AND, we expect that for BOTH experts and non-experts, incorrect advice leads to a lower probability of accurately diagnosing a CT scan compared to no advice presented.}

I becomes clear that our theoretical estimand is be a causal quantity, based on several individual quantities that are defined outside of any statistical model (Lundberg et al., 2021).
To elaborate further, we focus on only one part of the research question concerning the comparison between correct AI-advice and no AI-advice for expert radiologists.
For a specific expert radiologist and a specific CT scan, our question is concerned with the difference between the probability that a correct diagnosis is made if correct AI-advice were presented and the probability that a correct diagnosis is made if no AI-advice were presented.
Mathematically, we could describe this unit-specific quantity with some pseudo-equation like this (where \(s\) stands for a specific expert radiologist and \(i\) stands for a specific CT scan):

\[
\begin{aligned}
& P(\text{correct diagnosis} | \text{correct advice, specific expert, specific scan}) \\
& \quad - P(\text{correct diagnosis} | \text{no advice, specific expert, specific scan})
\end{aligned}
\]
However, our verbal description is not precise enough yet, because the above quantity may differ for each combination of expert and scan.
We must specify how we want to aggregate the quantity across different experts and scans.
If we would compute the average of our individual quantity across all experts and scans in the target population, the result would be called the average treatment effect (ATE) in the population of experts (CITATION).
Although often debated whether this is a good idea, psychological research often focuses on a different quantity:
Instead of averaging over experts and scans, we focus in a hypothetical \emph{typical} expert and a \emph{typical} scan, where \emph{typical} is usually defined as an average score on all attributes of the expert or scan.
With this definition, the theoretical estimand in our above example can be expressed as:

\[
\begin{aligned}
& P(\text{correct diagnosis} | \text{correct advice, average expert, average scan}) \\
& \quad - P(\text{correct diagnosis} | \text{no advice, average expert, average scan})
\end{aligned}
\]
In total, our theoretical estimand consists of four such expressions, the remaining three are:

\[
\begin{aligned}
& P(\text{correct diagnosis} | \text{no advice, average expert, average scan}) \\
& \quad - P(\text{correct diagnosis} | \text{wrong advice, average expert, average scan})
\end{aligned}
\]
\[
\begin{aligned}
& P(\text{correct diagnosis} | \text{correct advice, average student, average scan}) \\
& \quad - P(\text{correct diagnosis} | \text{no advice, average student, average scan})
\end{aligned}
\]

\[
\begin{aligned}
& P(\text{correct diagnosis} | \text{no advice, average student, average scan}) \\
& \quad - P(\text{correct diagnosis} | \text{wrong advice, average student, average scan})
\end{aligned}
\]
To complete our definition of the theoretical estimand, we have to clearly define out target population that consists of both persons and scans:
With respect to persons, we are only interested in expert radiologists and medical students at German universities.
With respect to stimuli, we are only interested in the head CT scans made from subjects that do or do not suffer from intracerebral hemorrhage.
Similarly, we are only interested in AI-advice given by the specific AI-enabled diagnostic decision support system under study.

While a theoretical estimand is defined outside of any statistical model and can be impossible to be observe directly, an empirical estimand is a quantity that can be estimated based on observed data.
For our exemplary research question, it is possible to construct an experimental study, where all participants are confronted with the same set of brain scans, but the kind of AI-advice given for each brain scan is randomly assigned within participants.
This random intervention allows to estimate our theoretical estimand, although in reality, each person receives only one kind of AI-advice (correct advice, wrong advice, no advice) for each brain scan.
For our example, each probability expression in our theoretical estimand can be modeled with the same GLMM.
Estimating the GLMM based on the data observed in our planned study produces an estimate for each probability and these estimates can be used to compute an estimate for each of the four probability contrasts.
For pedagogical reasons, we will skip the concrete definition of our empirical estimand until we have discussed how to simulate data based on a concrete GLMM in the next section.

\hypertarget{simulate-the-data-generating-process}{%
\subsection{Simulate the data generating process}\label{simulate-the-data-generating-process}}

\hypertarget{theory-1}{%
\paragraph{THEORY}\label{theory-1}}

When the estimand has been defined, the next step in the research process is to write code that simulates the (assumed) data generating process of the planned study.
This requires specifying a generative process for all predictor variables used in the final data analysis.
While such assumptions can be quite challenging for observational studies and/or continuous predictor variables, this is less of a problem for experimental studies with only categorical predictor variables.
When all predictor variables have been simulated, one can use the structure of a suitable GLMM to simulate the dependent variable.
To simulate the GLMM, one requires plausible values for all model parameters.
We will discuss strategies on how these values can be obtained later.
Because one has full control over the data generating process in a tailored simulation, it is simple to also model highly specific aspects of the planned study, like data missing by design or assuming that subjects drop out (i.e.~deleting their data from the simulated dataset) based on other variables in the simulated dataset.
The quality of the results of the sample size planning crucially depends on the plausibility of the simulated data generating process.
For this reason, deciding on a specific set of assumptions can easily feel overwelming.
However, we would argue that even a strongly simplified data generating process (e.g.~including only a small number of interaction effects; including only random intercepts but no random slopes; making rather implausible assumptions for missing data) can yield informative result and should be strongly preferred to the current status quo in psychological research, where most studies do not justify their sample size or use highly questionable heuristics that are independent of their specific research question.

\hypertarget{practice-1}{%
\paragraph{PRACTICE}\label{practice-1}}

In our case study, we simulate data for an experiment where the diagnostic performance of users of an AI-enabled diagnostic decision support system will be evaluated.
Participants, radiologists (task experts) and students/interns (non experts), review a series of head computer tomography (CT) scans to assess the presence of a bleeding.
To support their decision-making, an AI model provides initial diagnostic advice, which can be used as guidance by the participants.
In the control condition, no AI advice is presented, meaning that the participants have to read the CT scan without any support.
When AI advice is given, this advice can be either correct or incorrect.
The type of advice (no advice, wrong advice, correct advice) is randomized within subjects across brain scans.
After reviewing a CT scan, participants deliver a medical diagnosis (bleeding or no bleeding), which may be either accurate or inaccurate.
This experimental design introduces some missing values by design since the advice is neither correct nor incorrect when no advice is present, which must be taken into account when simulating and analyzing the data.
In this example, recruiting task experts (i.e., radiologists) is more challenging due to their limited availability, while non-experts (i.e., students/interns) are more readily accessible.
The goal of simulation-based sample size planning is to determine how many task experts and non-experts must be recruited to achieve sufficient statistical power or precision in the planned experiment.

\hypertarget{our-specific-glmm}{%
\subparagraph{Our specific GLMM}\label{our-specific-glmm}}

In a GLMM, the expected value of the dependent variable Y conditioned on the vector of predictor variables \(\mathbf{X}\) and random effects \(\mathbf{U}\), transformed by a link function \(g()\) is modeled as a linear combination \(\eta\) of the predictor variables \(\mathbf{X}\), the random effects \(\mathbf{U}\) and the model parameters \(\mathbf{\beta}\) (Fahrmeir et al., 2021):
\[
g(E(Y|\mathbf{X}=\mathbf{x},\mathbf{U}=\mathbf{u})) = \eta
\]
Equivalently, the conditional expected value is modeled as the linear combination \(\eta\), transformed by the inverse link function \(g^{-1}()\):
\[
E(Y|\mathbf{X}=\mathbf{x},\mathbf{U}=\mathbf{u}) = g^{-1}(\eta)
\]
If the dependent variable (i.e., diagnostic decision) \(Y\) is a binary variable with values \(0\) (i.e., inaccurate), or \(1\) (i.e., accurate), the conditional expected value is equivalent to the probability:
\[
P_{si} := P(Y = 1|\mathbf{X}=\mathbf{x},\mathbf{U}=\mathbf{u})
\]
In our case study, \(P_{si}\) is the conditional probability that subject \(s\) gives the correct response to item (i.e., CT scan) \(i\).

In such a setting, we model this probability as
\[
P_{si} = \text{inverse\_logit}(\eta_{si})
\]
with the inverse-logit link \(g^{-1}(\eta_{si}) = inverse\_logit(\eta_{si}) = \frac{exp(\eta_{si})}{1 + exp(\eta_{si})}\) or equivalently
\[
\text{logit}(P_{si}) = \eta_{si}
\]
with the logit link \(g(P_{si}) = \text{logit}(P_{si}) = \text{ln} (\frac{P_{si}}{1 - P_{si}})\).

In our case study, the probability of making an accurate diagnostic decision is assumed to depend on the predictors:

\begin{itemize}
\tightlist
\item
  \(advice\_present_{si}\): whether subject \(s\) was presented with AI advice (1) or not (0) when asked to assess item \(i\)
\item
  \(advice\_correct_{si}\): whether this advice was correct (1) or not (0)
\item
  \(expert_s\): whether subject \(s\) was a task expert (1) or not (0)
\end{itemize}

and the random effects:

\begin{itemize}
\tightlist
\item
  \(u_{0s}\): the deviation of subject \(s\) from the average ability to solve an item (i.e., CT scan) with average difficulty; assumed to be distributed as \(u_{0s} \sim N(0, \sigma_S^2)\)
\item
  \(u_{0i}\): the deviation of item (i.e., CT scan) \(i\) from the average difficulty to be solved by a person with average ability; assumed to be distributed as \(u_{0i} \sim N(0, \sigma_I^2)\)
\end{itemize}

In total, we assume the model
\[
\begin{aligned}
\text{logit}[P_{si}] =\ (&\beta_0 + u_{0s} + u_{0i}) + \\
&\beta_a \cdot advice\_present_{si} + \beta_c \cdot advice\_correct_{si} + \beta_e \cdot expert_s + \\
&\beta_{ea} \cdot expert_{s} \cdot advice\_present_{si} + \beta_{ec} \cdot expert_{s} \cdot advice\_correct_{si}
\end{aligned}
\]
or equivalently
\[
\begin{aligned}
P_{si} = \text{inverse\_logit}[&(\beta_0 + u_{0s} + u_{0i}) + \\
&\beta_a \cdot advice\_present_{si} + \beta_c \cdot advice\_correct_{si} + \beta_e \cdot expert_s + \\
&\beta_{ea} \cdot expert_{s} \cdot advice\_present_{si} + \beta_{ec} \cdot expert_{s} \cdot advice\_correct_{si}]
\end{aligned}
\]
with model parameters \(\beta_0\), \(\beta_e\), \(\beta_a\), \(\beta_c\), \(\beta_{ea}\), \(\beta_{ec}\), \(\sigma_S\), and \(\sigma_I\).

In the GLMM literature, this would be called a binomial GLMM with two random intercepts (for subjects and items), two level-1 predictors (\(advice\_present\), \(advice\_correct\)), one level-2 predictor (\(expert\)) and two cross-level interactions (\(expert \cdot advice\_present\), \(expert \cdot advice\_correct\)).
To limit complexity, we do not consider random slopes, additional predictors or higher-level interactions.

\hypertarget{simulation-function-in-r}{%
\subparagraph{Simulation function in R}\label{simulation-function-in-r}}

The following R function simulates a full dataset structured according to the design of our case study.
The faux package (DeBruine, 2023) contains useful functions when simulating factorial designs, including random effects.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulate }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{n\_subjects =} \DecValTok{100}\NormalTok{, }\AttributeTok{n\_items =} \DecValTok{50}\NormalTok{,}
  \AttributeTok{b\_0 =} \FloatTok{0.847}\NormalTok{, }\AttributeTok{b\_e =} \FloatTok{1.350}\NormalTok{, }\AttributeTok{b\_a =} \SpecialCharTok{{-}}\FloatTok{1.253}\NormalTok{, }\AttributeTok{b\_c =} \FloatTok{2.603}\NormalTok{,}
  \AttributeTok{b\_ea =} \FloatTok{0.790}\NormalTok{, }\AttributeTok{b\_ec =} \SpecialCharTok{{-}}\FloatTok{1.393}\NormalTok{,}
  \AttributeTok{sd\_u0s =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{sd\_u0i =} \FloatTok{0.5}\NormalTok{, ...)\{}
  \FunctionTok{require}\NormalTok{(dplyr)}
  \FunctionTok{require}\NormalTok{(faux)}
  \CommentTok{\# simulate design}
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{add\_random}\NormalTok{(}\AttributeTok{subject =}\NormalTok{ n\_subjects, }\AttributeTok{item =}\NormalTok{ n\_items) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{add\_between}\NormalTok{(}\StringTok{"subject"}\NormalTok{, }\AttributeTok{expert =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{.prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{advice\_present =} \FunctionTok{rbinom}\NormalTok{(}\FunctionTok{n}\NormalTok{(), }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \DecValTok{2}\SpecialCharTok{/}\DecValTok{3}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{advice\_correct =} \FunctionTok{if\_else}\NormalTok{(advice\_present }\SpecialCharTok{==}\NormalTok{ 1L, }
                                    \FunctionTok{rbinom}\NormalTok{(}\FunctionTok{n}\NormalTok{(), 1L, }\AttributeTok{prob =} \FloatTok{0.8}\NormalTok{), 0L)) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# add random effects}
    \FunctionTok{add\_ranef}\NormalTok{(}\StringTok{"subject"}\NormalTok{, }\AttributeTok{u0s =}\NormalTok{ sd\_u0s) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{add\_ranef}\NormalTok{(}\StringTok{"item"}\NormalTok{, }\AttributeTok{u0i =}\NormalTok{ sd\_u0i) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# compute dependent variable}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{linpred =}\NormalTok{ b\_0 }\SpecialCharTok{+}\NormalTok{ u0i }\SpecialCharTok{+}\NormalTok{ u0s }\SpecialCharTok{+}
\NormalTok{        b\_e }\SpecialCharTok{*}\NormalTok{ expert }\SpecialCharTok{+}\NormalTok{ b\_a }\SpecialCharTok{*}\NormalTok{ advice\_present }\SpecialCharTok{+}\NormalTok{ b\_c }\SpecialCharTok{*}\NormalTok{ advice\_correct }\SpecialCharTok{+}
\NormalTok{        b\_ea }\SpecialCharTok{*}\NormalTok{ expert }\SpecialCharTok{*}\NormalTok{ advice\_present }\SpecialCharTok{+}\NormalTok{ b\_ec }\SpecialCharTok{*}\NormalTok{ expert }\SpecialCharTok{*}\NormalTok{ advice\_correct) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{y\_prob =} \FunctionTok{plogis}\NormalTok{(linpred)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{y\_bin =} \FunctionTok{rbinom}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ y\_prob))}
\NormalTok{  dat}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

In the first six lines of the function definition, we set some default parameter values (which we will explain in the next section) and load the packages we use to manipulate and simulate data.
In our case study, each subject (\texttt{n\_subjects} in total) is assumed to respond to each item (i.e., CT scan; \texttt{n\_items} in total). Thus, the \texttt{add\_random} command creates a fully-crossed \texttt{data.frame} with \texttt{n\_subjects} \(\times\) \texttt{n\_items} rows.
We add a between-subject effect with the \texttt{add\_between} command, simulating that about \(25\%\) of subjects are experts.
The next two lines simulate that in \(\frac{2}{3}\) of trials, subjects will be presented with AI advice, and if advice is presented, the advice will be correct in about \(80\%\) of cases (the variable \texttt{advice\_correct} is always 0 when no advice is presented). Next, we simulate one random effect for each subject (\texttt{u0s}) and for each item (\texttt{u0i}).
As assumed by standard GLMMs, the \texttt{add\_ranef} function draws the random effects from a normal distribution with a mean 0 and a standard deviation specified by the user.
With all design variables done, we are ready to simulate our model equation outlined in the last section.
The linear predictor variable \texttt{linpred} (\(\eta\) in the GLMM model equations) combines the predictor variables, random effects, and model parameters as assumed by our model.
We then transform the linear predictor with the inverse-link function to compute \texttt{y\_prob}, the probability that the subject correctly solved the item (in R, the inverse-logit link is computed with \texttt{plogis} and the logit link with \texttt{qlogis}).
In the final step, we simulate the binary dependent variable \texttt{y\_bin} (i.e., whether the subject makes an accurate diagnostic decision for the CT scan) by -- for each trial -- drawing from a Bernoulli distribution with success probability \texttt{y\_prob}.

\hypertarget{strategies-on-specifying-the-population-parameters}{%
\subsection{Strategies on specifying the population parameters}\label{strategies-on-specifying-the-population-parameters}}

\hypertarget{theory-2}{%
\subsubsection{THEORY}\label{theory-2}}

Tailored simulation based sample size planning is especially relevant when studies have to be planned in the absence of previous studies with the same design or pilot data.
In this scenario, researchers require strategies on how to specify the population parameters used in their data simulation.
Population parameters are all model parameters estimated in a GLMM, in particular the regression coefficients of the fixed effects and the standard deviation of the random effects (and the correlation between random effects in more complicated models).
In contrast to non-hierarchical linear regression, common heuristics based on standardized effect sizes are less useful or not even available for GLMMs.
Our strategies to specify population parameters will require access to domain knowledge from domain experts.
Because most domain knowledge of domain experts can only be expressed in unstandardized measurement units of a specific application, we argue that unstandardized effect sizes are usually preferable over standardized effect sizes for tailored simulation based sample size planning.
The basic idea of all strategies is how the data generating process implied by certain values of population parameters can be quantified or visualized in an intuitive way that enables a calibration of population parameters based on the available knowledge of domain experts.

Although we focus on frequentist statistics in our tutorial, selecting plausible values for data simulation shares many similarities with the specification of (weakly) informative prior distribution in applied Bayesian statistics.
In this literature, strategies have been developed to monitor the plausibility of (prior) model assumptions (Gelman et al., 2020), that can be adapted for our purposes.

\hypertarget{practice-2}{%
\subsubsection{PRACTICE}\label{practice-2}}

When introducing the simulation function for our case study, we have used theoretically plausible values as defaults for all model parameters (\(\beta_0\), \(\beta_e\), \(\beta_a\), \(\beta_c\), \(\beta_{ea}\), \(\beta_{ec}\), \(\sigma_S\), and \(\sigma_I\)), but have not talked about where these numbers came from.

Ideally, one would rely on meta-analytic results or conclusive data from pilot studies. However, these are sometimes not readily available. All parameter values in our present case study have been determined based on results from distantly related study designs in the literature. Additionally, we had repeated discussions with our affiliated domain experts in radiology to check whether our assumptions seem plausible.

We now outline our main strategy to determine plausible parameter values for the fixed effects (\(\beta\) parameters):
Unfortunately, the model parameters in a binomial GLMM are hard to interpret in isolation because 1) the \(\beta\) parameters are connected to the modeled probability via the non-linear inverse-logit link, and 2) we also have to consider the random effects.
The most simple interpretation, that allows us to ignore the random effects for now, works by imagining a subject with average ability (\(u_{0s} = 0\)) responding to an item (i.e., CT scan) with average difficulty (\(u_{0i} = 0\)).
Then the model implied probability that such a person solves such an item accurately is given by:

\[
\begin{aligned}
P(Y=1|\mathbf{X=x}, \mathbf{U} = \mathbf{0}) = \\
= \text{inverse\_logit}[&\beta_0 + \beta_a \cdot advice\_present_{si} + \beta_c \cdot advice\_correct_{si} + \beta_e \cdot expert_s + \\
&\beta_{ea} \cdot expert_{s} \cdot advice\_present_{si} + \beta_{ec} \cdot expert_{s} \cdot advice\_correct_{si}]
\end{aligned}
\]
In fact, we would only need the full equation if the subject is an expert and correct advice is presented.
In all other experimental conditions, some terms drop from the equation because they are multiplied by \(0\).
The other extreme case would be the probability that a non-expert with average ability solves an item with average difficulty when no advice is presented:
\[
\begin{aligned}
P(Y=1| advice\_present = 0, advice\_correct = 0, expert = 0, u_{0s} = 0, u_{0i} = 0) = \\
= \text{inverse\_logit}[\beta_0]
\end{aligned}
\]
We can revert this perspective by choosing plausible probability values based on domain knowledge and deriving the parameter values implied by these probabilities for each experimental condition.

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:probtable}Assumed probabilities that an average subject solves an average item in each experimental condition.}

\begin{tabular}{lll}
\toprule
Experimental condition & $P(Y=1|\mathbf{X=x}, \mathbf{U} = \mathbf{0})$ & Implied equation\\
\midrule
no advice, no expert & 0.70 & $logit(0.70) = \beta_0$\\
no advice, expert & 0.90 & $logit(0.90) = \beta_0 + \beta_e$\\
false advice, no expert & 0.40 & $logit(0.40) = \beta_0 + \beta_a$\\
false advice, expert & 0.85 & $logit(0.85) = \beta_0 + \beta_e + \beta_{a} + \beta_{ea}$\\
correct advice, no expert & 0.90 & $logit(0.90) = \beta_0 + \beta_a + \beta_c$\\
correct advice, expert & 0.95 & $logit(0.95) = \beta_0 + \beta_e + \beta_a + \beta_c + \beta_{ea} + \beta_{ec}$\\
\bottomrule
\addlinespace
\end{tabular}

\begin{tablenotes}[para]
\normalsize{\textit{Note.} Implied equations are derived based on the model equations and setting all random intercept terms to 0.}
\end{tablenotes}

\end{threeparttable}
\end{center}

\end{table}

Table \ref{tab:probtable} shows our set of assumptions concerning the probability that an average subject solves an average item for each experimental condition, as well as the corresponding equations implied by the model.
The table can be used to compute the implied values for the \(\beta\) parameters, starting with the first equation and reinserting the computed \(\beta\) values in all following equations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b\_0 }\OtherTok{\textless{}{-}} \FunctionTok{qlogis}\NormalTok{(}\FloatTok{0.7}\NormalTok{)}
\NormalTok{b\_e }\OtherTok{\textless{}{-}} \FunctionTok{qlogis}\NormalTok{(}\FloatTok{0.9}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ b\_0}
\NormalTok{b\_a }\OtherTok{\textless{}{-}} \FunctionTok{qlogis}\NormalTok{(}\FloatTok{0.4}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ b\_0}
\NormalTok{b\_ea }\OtherTok{\textless{}{-}} \FunctionTok{qlogis}\NormalTok{(}\FloatTok{0.85}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ b\_0 }\SpecialCharTok{{-}}\NormalTok{ b\_e }\SpecialCharTok{{-}}\NormalTok{ b\_a}
\NormalTok{b\_c }\OtherTok{\textless{}{-}} \FunctionTok{qlogis}\NormalTok{(}\FloatTok{0.9}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ b\_0 }\SpecialCharTok{{-}}\NormalTok{ b\_a}
\NormalTok{b\_ec }\OtherTok{\textless{}{-}} \FunctionTok{qlogis}\NormalTok{(}\FloatTok{0.95}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ b\_0 }\SpecialCharTok{{-}}\NormalTok{ b\_e }\SpecialCharTok{{-}}\NormalTok{ b\_a }\SpecialCharTok{{-}}\NormalTok{ b\_c }\SpecialCharTok{{-}}\NormalTok{ b\_ea}
\FunctionTok{c}\NormalTok{(}\AttributeTok{b\_0 =}\NormalTok{ b\_0, }\AttributeTok{b\_e =}\NormalTok{ b\_e, }\AttributeTok{b\_a =}\NormalTok{ b\_a, }\AttributeTok{b\_c =}\NormalTok{ b\_c, }\AttributeTok{b\_ea =}\NormalTok{ b\_ea, }\AttributeTok{b\_ec =}\NormalTok{ b\_ec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        b_0        b_e        b_a        b_c       b_ea       b_ec 
##  0.8472979  1.3499267 -1.2527630  2.6026897  0.7901394 -1.3928518
\end{verbatim}

It is always possible to double-check these computations by transforming the parameter values back to probabilities, e.g.~
\[
\begin{aligned}
P(Y=1|expert = 1, advice\_present = 1, advice\_correct = 1, u_{0s} = 0, u_{0i} = 0) = \\
= inverse\_logit[\beta_0 + \beta_e + \beta_a + \beta_c + \beta_{ea} + \beta_{ec}]
\end{aligned}
\]

which we compute in R as:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plogis}\NormalTok{(b\_0 }\SpecialCharTok{+}\NormalTok{ b\_e }\SpecialCharTok{+}\NormalTok{ b\_a }\SpecialCharTok{+}\NormalTok{ b\_c }\SpecialCharTok{+}\NormalTok{ b\_ea }\SpecialCharTok{+}\NormalTok{ b\_ec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.95
\end{verbatim}

This leaves us with the question on how to determine plausible values for the two remaining model parameters (\(\sigma_S\), and \(\sigma_I\)) that are the standard deviations for the random intercepts.
For this, we introduce two more strategies.

\hypertarget{insightful-descriptive-statistics}{%
\subsubsection{Insightful descriptive statistics}\label{insightful-descriptive-statistics}}

\hypertarget{theory-3}{%
\paragraph{THEORY}\label{theory-3}}

The mathematical structure of GLMMs determines which patterns in data would be produced by the model, if a specific set of values for the population parameters would be specified.
The knowledge of how to simulate from a GLMM enables us to compute insightful descriptive statistics that can be compared to available domain knowledge much more easily than the opaque values of model parameters.
For example, domain experts might not be able to directly choose plausible values for the \(\beta\) coefficients in a logistic regression model (which are measured on the log-odds scale).
However, they should be able to reason about the expected ratio of the binary dependent variable in different experimental condition, i.e.~which descriptive statistics they expect to observe (on average) when they will analyse the actual dataset produced by the study under planning (if everything turns out as postulated by the investigated theory).
The job of the analyst who is familiar with the mathematical structure of the GLMM is to produce the model implied value of the insightful descriptive statistic expected by the domain expert.
Although insightful descriptive statistics often depend on the model parameters in complicated ways, it is usually possible to manually adjust the population parameters (if only with the help of trial and error) until the model implied quantities produce the desired result.

\hypertarget{practice-3}{%
\paragraph{PRACTICE}\label{practice-3}}

In the last section, we showed how we can derive the model implied probability for each experimental condition, that a subject with average ability solves an item with average difficulty.
Although these derivations are straightforward, it is important not to misinterpret their implications: In binomial GLMMs, the average probability to solve an item (averaged across persons of varying ability and items of varying difficulty) is \textbf{not} equal to the probability that a person with average ability solves an item with average difficulty (Fahrmeir et al., 2021).
The first perspective implies a so-called marginal interpretation, while the second one implies a conditional interpretation.

For example, we determined the \(\beta\) parameters in a way that corresponds to a desired conditional probability of \(0.95\), that an expert with average ability solves an item with average difficulty when presented with correct advice (the conditional perspective).
However, even if the model were true, we would not observe that 95\% of experts responding to items presented with correct advice from a big sample of subjects drawn from their natural distribution of ability and items drawn from their natural distribution of difficulty (the marginal perspective).
How much the two probabilities differ depend on the standard deviations of the random intercepts (the two probabilities are only equal if both standard deviations would be zero).
We want to use the model implied observed proportion of correct diagnoses in each experimental condition as an insightful descriptive statistics to determine plausible values for the random effect standard deviations.

We will simulate a large dataset (for which the observed values of the descriptive statistic will be close to their model implied true values) and simply compute the relative frequency of correct diagnoses for each experimental condition.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{simulate}\NormalTok{(}\AttributeTok{n\_subjects =} \DecValTok{3000}\NormalTok{, }\AttributeTok{n\_items =} \DecValTok{3000}\NormalTok{,}
  \AttributeTok{sd\_u0s =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{sd\_u0i =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{dat }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{condition =} \FunctionTok{fct\_cross}\NormalTok{(}
    \FunctionTok{factor}\NormalTok{(expert), }\FunctionTok{factor}\NormalTok{(advice\_present), }\FunctionTok{factor}\NormalTok{(advice\_correct))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{condition =} \FunctionTok{fct\_recode}\NormalTok{(condition,}
    \StringTok{"no expert, no advice"} \OtherTok{=} \StringTok{"0:0:0"}\NormalTok{, }\StringTok{"expert, no advice"} \OtherTok{=} \StringTok{"1:0:0"}\NormalTok{, }
    \StringTok{"no expert, wrong advice"} \OtherTok{=} \StringTok{"0:1:0"}\NormalTok{, }\StringTok{"expert, wrong advice"} \OtherTok{=} \StringTok{"1:1:0"}\NormalTok{,}
    \StringTok{"no expert, correct advice"} \OtherTok{=} \StringTok{"0:1:1"}\NormalTok{, }\StringTok{"expert, correct advice"} \OtherTok{=} \StringTok{"1:1:1"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(condition) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{relative\_frequency =} \FunctionTok{sum}\NormalTok{(y\_bin) }\SpecialCharTok{/} \FunctionTok{n}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   condition                 relative_frequency
##   <fct>                                  <dbl>
## 1 no expert, no advice                   0.683
## 2 expert, no advice                      0.881
## 3 no expert, wrong advice                0.409
## 4 expert, wrong advice                   0.828
## 5 no expert, correct advice              0.883
## 6 expert, correct advice                 0.938
\end{verbatim}

We tried using these descriptive statistics to judge together with our domain experts whether our chosen values for the random effect standard deviations would produce data that align with out domain expertise.
However although they result was deemed plausible, in our example these statistics are not informative enough to determine a final set of plausible parameter values.
For this reason, we will additionally look at insightful model based quantities.

\hypertarget{insightful-model-based-quantities}{%
\subsubsection{Insightful model based quantities}\label{insightful-model-based-quantities}}

\hypertarget{theory-4}{%
\paragraph{THEORY}\label{theory-4}}

Because GLMMs are complicated models, descriptive statistics alone are usually not enough to specify plausible values for all model parameters.
This is especially true for the standard deviation of random effects, that have complicated (and often unexpected) effects on the model-implied results.
An important advantage of data simulation (where one has full control over parameter values and sample sizes) is that one can produce insightful model based quantities that can never be directly observed in an actual empirical dataset.
For example, in a logistic model with random intercepts for participants, one can produce a visualization of the implied distribution of the probability that a participant on average solves a cognitive task.
Although domain knowledge will probably not suffice to specify this distribution completely, it should be possible to rule out unplausible boundary conditions.
For example, the domain expert might deem it unplausible that the 5\% most able participants have a probability of more than 0.99 to solve the difficult cognitive task.

\hypertarget{practice-4}{%
\paragraph{PRACTICE}\label{practice-4}}

The discussed inequality of conditional and marginal effects in GLMMs (Fahrmeir et al., 2021) makes their interpretation more difficult.
One must be careful when specifying parameter values based on previous studies or pilot data that use the marginal interpretation (e.g., a pilot study providing an estimate of how often neurologists make an accurate diagnosis based on brain scans).
However, this does not mean that we cannot use the marginal interpretation (average probability across persons and items) to inform plausible parameter values: When parameter values have been selected, we can compute the implied marginal distributions and compare this information to our domain knowledge.
Then, we can iteratively adjust the parameter values until we are satisfied with the implied distributions.

In the last section, we simulated a large simulated dataset and computed descriptive statistics, the relative frequencies of correct diagnoses, for each experimental condition.
We will now use the model implied probability of each simulated data point (stored in the variable \texttt{y\_prob}) to visualize the whole model implied marginal distribution of correct diagnoses for each experimental condition.



\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggdist)}
\NormalTok{dat }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{condition =} \FunctionTok{fct\_cross}\NormalTok{(}
    \FunctionTok{factor}\NormalTok{(expert), }\FunctionTok{factor}\NormalTok{(advice\_present), }\FunctionTok{factor}\NormalTok{(advice\_correct))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{condition =} \FunctionTok{fct\_recode}\NormalTok{(condition,}
    \StringTok{"no expert, no advice"} \OtherTok{=} \StringTok{"0:0:0"}\NormalTok{, }\StringTok{"expert, no advice"} \OtherTok{=} \StringTok{"1:0:0"}\NormalTok{, }
    \StringTok{"no expert, wrong advice"} \OtherTok{=} \StringTok{"0:1:0"}\NormalTok{, }\StringTok{"expert, wrong advice"} \OtherTok{=} \StringTok{"1:1:0"}\NormalTok{,}
    \StringTok{"no expert, correct advice"} \OtherTok{=} \StringTok{"0:1:1"}\NormalTok{, }\StringTok{"expert, correct advice"} \OtherTok{=} \StringTok{"1:1:1"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y\_prob, }\AttributeTok{y =}\NormalTok{ condition)) }\SpecialCharTok{+}
  \FunctionTok{stat\_histinterval}\NormalTok{(}\AttributeTok{point\_interval =} \StringTok{"mean\_qi"}\NormalTok{, }\AttributeTok{slab\_color =} \StringTok{"gray45"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/margdist1-1.jpeg}
\caption{\label{fig:margdist1}Marginal distributions including means, 66\% and 95\% confidence intervals for all experimental conditions.}
\end{figure}

Figure \ref{fig:margdist1} shows the model implied marginal distributions, including the mean, 66\% and 95\% intervals. We can see that, indeed, the average probabilities (black dots) slightly differ from the probabilities of average subjects and items considered in the previous section. This difference increases with the variability of the random effects.

We can use plots like the one above as a useful tool to decide whether the specified standard deviations of the subject and item random intercepts (\(\sigma_S\) and \(\sigma_I\)) are reasonable by comparing the ranges and overlap between conditions to domain knowledge.

In the next plot, we have set the item standard deviation to almost zero (\(\sigma_I = 0.01\)). This gives us a better way to see the variability between persons.



\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/margdist2-1.jpeg}
\caption{\label{fig:margdist2}Marginal distributions including means, 66\% and 95\% confidence intervals for all experimental conditions while setting the standard deviation of item random intercepts to 0.01.}
\end{figure}

As an example, Figure \ref{fig:margdist2} reveals a number of implicit assumptions about the comparison between experts and non-experts: With wrong advice, virtually all experts have a higher probability of making a correct diagnosis compared to non-experts when considering only items with average difficulty. In contrast, there is considerable overlap in probability between experts and non-experts with no advice and even higher overlap with correct advice. Patterns like these should be considered carefully and discussed with the domain experts. Parameter values (\(\beta\) parameters, and \(\sigma_S\)) should be adjusted if the implications do not seem reasonable.

We could also have a closer look at variability between items by setting the subject standard deviation to almost zero (\(\sigma_S = 0.01\)).

The final plot demonstrates that these plots are also useful for spotting standard deviations that are specified too high. For Figure \ref{fig:margdist3}, we have set \(\sigma_S = 3\) and \(\sigma_I = 3\). This implies that in each experimental condition, the probabilities that a subject solves an item are usually close to either 0 or 1, which is not a plausible assumption. However, these high standard deviations do not account for the inherent variability and complexity of human performance. For example, we would expect that a participant with low ability compared to other task experts to solve a difficult item with a probability substantially larger than zero even when presented with wrong advice.



\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/margdist3-1.jpeg}
\caption{\label{fig:margdist3}Marginal distributions including means, 66\% and 95\% confidence intervals for all experimental conditions while setting the standard deviation of subject and item random intercepts to 3.}
\end{figure}

\hypertarget{iterative-process-with-domain-experts}{%
\subsubsection{Iterative process with domain experts}\label{iterative-process-with-domain-experts}}

\hypertarget{theory-5}{%
\paragraph{THEORY}\label{theory-5}}

In our experience, the gathering of domain knowledge by domain experts and the consecutive specification of population parameter values used in data simulation is not a one-time event but rather an iterative process.
In a first step, domain experts can be interviewed to ``elicit'' their domain knowledge about how the future data of the planned study is expected to look like.
As most domain experts are no experts in statistical modeling and GLMMs, they often struggle without further guidance to communicate their knowledge in a way that is useful when specifying the parameters for data simulation.
For this reason, we suggest that after an initial unstructured interview of domain experts, the analyst which is familiar with the structure of the GLMM under study selects an initial set of insightful descriptive statistics and model based quantities.
Then they reenter into an iterative discussion where some set of population values are selected and the plausibility of resulting implied quantities are discussed with the domain experts.
Then, the population parameters are updated based on this discussion until the domain experts are satisfied with the final result.
During this process, the monitored model based quantities and descriptive statistics can be updated or extended to capture as much available domain knowledge as possible.

\hypertarget{practice-5}{%
\subparagraph{PRACTICE}\label{practice-5}}

All parameter values in our present case study have been determined based on repeated discussions with our affiliated domain experts in Radiology to validate our assumptions.
Initially, we reviewed the literature to establish a reasonable baseline performance rate for examining head CT scans for intracranial hemorrhage.
Existing studies indicate that radiologists typically demonstrate high accuracies, often exceeding or hovering around 90\%, while interns have been shown to perform below 80\%, and medical students fall even shorter.
For simplicity, we assumed plausible probability values of .90 for experts and .70 for non-experts, respectively.
Our experts confirmed that these values are realistic baselines for reviewing diverse head CT images without AI assistance. Subsequently, we consulted several published papers investigating the effect of correct and incorrect advice on decision-making performance in other settings.
From their findings, we inferred that both experts and non-experts should benefit from correct and suffer losses from incorrect advice.
However, the magnitude of these effects should be substantially greater for non-experts, given their demonstrated reliance on advice compared to experts.
We further validated the plausibility of our estimated gains and losses with the collaborating radiologists.
For our simulation, we used the probabilities of average participants to solve an average case, as shown in Table 1.

\hypertarget{estimate-the-statistical-model}{%
\subsection{Estimate the statistical model}\label{estimate-the-statistical-model}}

\hypertarget{theory-6}{%
\subsubsection{THEORY}\label{theory-6}}

At this point, the researcher is capable of producing a simulated dataset similar to the actual dataset that will later be collected in the planned study.
The next step is to specify how the statistical model shall be estimated in the actual study collected later.
This usually includes the selection of 1) a statistical framework (e.g., frequentist statistics), 2) a software package that is capable of estimating the model class of interest (e.g., the lme4 R package), 3) an estimation algorithm (e.g., the default optimizer ``bobyqa''), and 4) the specific model structure including all fixed effects, random effects, and the model family of the dependent variable.\\
Note that this does not always mean that one will specify the same GLMM that was used when specifying the data generating process.
On the one hand, using a simpler model for data simulation than for model estimation can be a useful strategy in scenarios where making plausible assumptions for complicated random effect structures and interactions is not feasible.
On the other hand, using a more complex model (that perhaps even goes beyond a standard GLMM) for data simulation than for model estimation can be a useful strategy in scenarios where one has specific domain knowledge about aspects of the data generating process that are still difficult to estimate with the current state-of-the-art in multilevel modeling.

\hypertarget{practice-6}{%
\subsubsection{PRACTICE}\label{practice-6}}

In our case study, we use the lme4 R package (Bates, Mächler, Bolker, \& Walker, 2015), which is a state-of-the-art tool for fitting frequentist GLMMs.\footnote{For Bayesian GLMMs, the brms R package is currently the most prominent option (Bürkner, 2017).}
The lme4 package includes a function called \texttt{simulate} that allows researchers to simulate the dependent variable based on the same model formula used for model fitting, enabling simulation-based power analyses and other related analyses.

However, the model parameterization used by the lme4 package is quite technical, making it difficult for applied researchers to determine whether their specified population model (i.e., the theoretical model that describes the underlying data generation process for a specific population of interest) implies plausible associations in their simulated data. Therefore, for our case study, we have written our own function to simulate data for GLMMs from first principles (i.e., creating synthetic data step by step instead of using black box functions).
We think this choice assists applied researchers in better understanding all model assumptions before using lme4 to analyze the simulated datasets.\footnote{A less flexible alternative would be to use the simr package (Green \& MacLeod, 2016), which can be used to both simulate data and perform power analysis for models supported by the lme4 package.}

In this section, we show how to fit a GLMM with lme4.
We simulate data according to our model, in which 100 subjects respond to 50 items (we use \texttt{set.seed} to make the simulation reproducible). However, for the sake of the exercise, we can imagine that this would be real data resulting from our future experiment and think about how we would analyze this data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{simulate}\NormalTok{(}\AttributeTok{n\_subjects =} \DecValTok{100}\NormalTok{, }\AttributeTok{n\_items =} \DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The lme4 package uses a special syntax for model specification. Our specific GLMM is represented by the formula:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ y\_bin }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ expert }\SpecialCharTok{+}\NormalTok{ advice\_present }\SpecialCharTok{+}\NormalTok{ advice\_correct }\SpecialCharTok{+} 
\NormalTok{  expert}\SpecialCharTok{:}\NormalTok{advice\_present }\SpecialCharTok{+}\NormalTok{ expert}\SpecialCharTok{:}\NormalTok{advice\_correct }\SpecialCharTok{+}
\NormalTok{  (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{subject) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{item)}
\end{Highlighting}
\end{Shaded}

The first two lines look similar to any linear model in R (general intercept indicated by \texttt{1}; main effects indicated by variable names in the dataset; interactions indicated by \texttt{variable1:variable2}). The third line specifies a random intercept for each subject \texttt{(1\textbar{}subject)} and for each item \texttt{(1\textbar{}item)}. The complete set of rules for the syntax is outlined in Bates et al. (2015) and in the documentation of the lme4 package.

In lme4, a GLMM is fitted with the \texttt{glmer} function. By setting \texttt{family\ =\ \ "binomial"}, we request a binomial GLMM appropriate for our binary dependent variable \texttt{y\_bin} (the binomial GLMM uses the canonical logit link by default), which is defined as an accurate (1) vs.~inaccurate (0) diagnosis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glmer}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ dat, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can inspect the estimates for all model parameters with the \texttt{summary} command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: binomial  ( logit )
## Formula: 
## y_bin ~ 1 + expert + advice_present + advice_correct + expert:advice_present +  
##     expert:advice_correct + (1 | subject) + (1 | item)
##    Data: dat
## 
##      AIC      BIC   logLik deviance df.resid 
##   4149.4   4201.6  -2066.7   4133.4     4992 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.7669  0.2125  0.3046  0.4317  2.1056 
## 
## Random effects:
##  Groups  Name        Variance Std.Dev.
##  subject (Intercept) 0.3148   0.5611  
##  item    (Intercept) 0.1624   0.4029  
## Number of obs: 5000, groups:  subject, 100; item, 50
## 
## Fixed effects:
##                       Estimate Std. Error z value Pr(>|z|)    
## (Intercept)             1.0339     0.1103   9.374  < 2e-16 ***
## expert                  1.1849     0.2096   5.654 1.57e-08 ***
## advice_present         -1.3436     0.1206 -11.143  < 2e-16 ***
## advice_correct          2.6154     0.1273  20.540  < 2e-16 ***
## expert:advice_present   1.0589     0.2940   3.601 0.000317 ***
## expert:advice_correct  -1.8104     0.2915  -6.210 5.28e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Correlation of Fixed Effects:
##             (Intr) expert advc_p advc_c exprt:dvc_p
## expert      -0.377                                 
## advic_prsnt -0.349  0.176                          
## advic_crrct  0.023  0.001 -0.668                   
## exprt:dvc_p  0.143 -0.448 -0.412  0.276            
## exprt:dvc_c -0.008  0.004  0.292 -0.435 -0.686
\end{verbatim}

In the lme4 output, the \texttt{Estimate} column in the \texttt{Fixed\ effects} table contains the estimates for the \(\beta\) parameters, while the \texttt{Std.Dev.} column in the \texttt{Random\ effects} table contains the estimates for \(\sigma_S\) and \(\sigma_I\).

\hypertarget{compute-the-estimate}{%
\subsection{Compute the estimate}\label{compute-the-estimate}}

\hypertarget{theory-7}{%
\subsubsection{THEORY}\label{theory-7}}

When the theoretical and empirical estimands have been identified in the modeling framework used to estimate the GLMM, one has to decide on whether hypothesis testing or interval estimation should be used to answer the research question.
While the majority of psychological research uses hypothesis testing, there are repeated claims that a focus on estimation can also be a good choice in many circumstances (CITATION).
This decision on testing or estimating is then followed by selecting the specific statistical method that shall be applied to compute the hypothesis test(s) or confidence interval(s) (e.g., compute hypothesis tests and confidence intervals with the marginaleffects R package using the delta method).

\hypertarget{practice-7}{%
\subsubsection{PRACTICE}\label{practice-7}}

For our case study, we investigate the following research question:
\emph{We expect that for both experts and non-experts, correct advice leads to a higher probability of accurately diagnosing a CT scan compared to no advice presented, AND, we expect that for both experts and non-experts, incorrect advice leads to a lower probability of accurately diagnosing a CT scan compared to no advice presented.}

In the earlier section on our theoretical estimand we translated this verbal description into four probability statements that are specified outside of any specific statistical model.
The next translation step is to specify the empirical estimand, that is how these probabilites can be estimated within the scope of our concrete GLMM.

The first of the four statements of the theoretical estimand was:

\[
\begin{aligned}
& P(\text{correct diagnosis} | \text{correct advice, average expert, average scan}) \\
& \quad - P(\text{correct diagnosis} | \text{no advice, average expert, average scan})
\end{aligned}
\]
In the context of our GLMM, the corresponding probability statement is:

\[
\begin{aligned}
& P(Y=1|advice\_present = 1, advice\_correct = 1, expert = 1, u_{0s} = 0, u_{0i} = 0) \\
& \quad - P(Y=1|advice\_present = 0, advice\_correct = 0, expert = 1, u_{0s} = 0, u_{0i} = 0)
\end{aligned}
\]
Plugging in the model equation of the GLMM produces an equation on how to compute this contrast when all model parameters are known.
We have already discussed how to compute the individual probabilities in the section on specifying population parameters.
When we want to estimate the above contrast based on observed data, the only difference is that model parameters are not known and we instead plug the corresponding parameter estimates into the equation.

We could use our knowledge of the structure of our GLMM to determine the exact formula needed to compute our contrast of interest and then plug in the parameter estimates manually from the \texttt{summary(fit)} output.
However, this would be very tedious and we can use R to compute this contrast without doing the math.

We could use the \texttt{predict} function of the lme4 package to compute the predicted probability for a correct diagnosis based on our model, plug in the two sets of predictor values, and compute the difference between the two estimated probabilities.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{advice\_present =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{advice\_correct =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }
  \AttributeTok{expert =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{pred\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   advice_present advice_correct expert
## 1              1              1      1
## 2              0              0      1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{preds }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ pred\_df, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{, }\AttributeTok{re.form =} \ConstantTok{NA}\NormalTok{)}
\NormalTok{preds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1        2 
## 0.939292 0.901923
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{preds[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ preds[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          1 
## 0.03736901
\end{verbatim}

The argument \texttt{type\ =\ "response"} specifies that predictions are made on the probability scale (instead of the log-odds scale of the \(\beta\) parameters), while \texttt{re.form\ =\ NA} sets all random effects to 0.

We could use this method to compute point estimates for all four contrast that are part of our empirical estimand.
However, depending on whether we are interested in hypothesis testing or parameter estimation, we also need a method to compute hypothesis tests or confidence interval for our point estimates.

The marginaleffects package (Arel-Bundock, 2023) is a very flexible, increasingly popular package to compute hypothesis tests and confidence intervals for contrasts with a variety of statistical models, including GLMMs estimated with lme4.

First, we specify a grid of all combinations of predictor variable and then compute estimated probabilities for all experimental conditions in our experiment with the \texttt{predictions} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred\_df }\OtherTok{\textless{}{-}} \FunctionTok{expand\_grid}\NormalTok{(}\AttributeTok{advice\_present =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{, }\AttributeTok{advice\_correct =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{, }\AttributeTok{expert =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{)}
\NormalTok{pred\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 8 x 3
##   advice_present advice_correct expert
##            <int>          <int>  <int>
## 1              0              0      0
## 2              0              0      1
## 3              0              1      0
## 4              0              1      1
## 5              1              0      0
## 6              1              0      1
## 7              1              1      0
## 8              1              1      1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probs }\OtherTok{\textless{}{-}} \FunctionTok{predictions}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ pred\_df, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{, }\AttributeTok{re.form =} \ConstantTok{NA}\NormalTok{)}
\NormalTok{probs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Estimate Std. Error     z Pr(>|z|)     S 2.5 % 97.5 % advice_present
##     0.738    0.02134  34.6   <0.001 867.2 0.696  0.779              0
##     0.902    0.01739  51.9   <0.001   Inf 0.868  0.936              0
##     0.975    0.00421 231.6   <0.001   Inf 0.966  0.983              0
##     0.954    0.01454  65.6   <0.001   Inf 0.925  0.982              0
##     0.423    0.03221  13.1   <0.001 128.6 0.360  0.486              1
##     0.874    0.02793  31.3   <0.001 711.4 0.819  0.928              1
##     0.909    0.00967  94.1   <0.001   Inf 0.890  0.928              1
##     0.939    0.01091  86.1   <0.001   Inf 0.918  0.961              1
##  advice_correct expert
##               0      0
##               0      1
##               1      0
##               1      1
##               0      0
##               0      1
##               1      0
##               1      1
## 
## Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, advice_present, advice_correct, expert, y_bin 
## Type:  response
\end{verbatim}

The point estimates for all experimental conditions are reported in the \texttt{Estimate} column.
Note that the output also contains the two missing by design conditions that will never be observed in the actual study (\(advice\_present = 0, advice\_correct = 1, expert = 1\) and \(advice\_present = 0, advice\_correct = 1, expert = 0\)).
This is no problem as long as we never interpret those estimates.

Next, we use the estimated probabilities to compute the four specific contrasts that are part of our empirical estimand.
For this we must specify which rows in \texttt{probs} have to be subtracted from each other.
We will use the \texttt{hypotheses} function to compute our four contrasts of interest together with hypothesis tests and confidence intervals.
We use the default inference options of the marginaleffects package that compute hypothesis tests and confidence intervals based on the approximate delta method.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{hypotheses}\NormalTok{(}\AttributeTok{hypothesis =} \FunctionTok{c}\NormalTok{(}
    \StringTok{"b8 = b2"}\NormalTok{,}
    \StringTok{"b2 = b6"}\NormalTok{,}
    \StringTok{"b7 = b1"}\NormalTok{,}
    \StringTok{"b1 = b5"}\NormalTok{),}
    \AttributeTok{equivalence =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   Term Estimate Std. Error     z Pr(>|z|)    S    2.5 % 97.5 % p (NonSup)
##  b8=b2   0.0374     0.0162  2.31    0.021  5.6  0.00563 0.0691      0.989
##  b2=b6   0.0282     0.0279  1.01    0.312  1.7 -0.02653 0.0830      0.844
##  b7=b1   0.1717     0.0173  9.93   <0.001 74.8  0.13780 0.2056      1.000
##  b1=b5   0.3145     0.0280 11.24   <0.001 95.0  0.25965 0.3693      1.000
##  p (NonInf) p (Equiv)
##      0.0105     0.989
##      0.1562     0.844
##      <0.001     1.000
##      <0.001     1.000
## 
## Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv 
## Type:  response
\end{verbatim}

The expression \texttt{"b8\ =\ b2"} is special syntax to subtract the estimate in row number 8 from the estimate in row number 2.
The argument \texttt{equivalence\ =\ c(0,\ 0)} can be used to compute one-sided p-values, testing whether the contrast in the population is smaller than 0 (\texttt{p\ (NonSub)} column) or greater than 0 (\texttt{p\ (NonInf)} column).
The point estimates for four contrasts are reported in the \texttt{Estimate} column.
Note that to facilitate interpretation, we arranged the contrasts in a way that we theoretically expect positive values for all four of them.

\hypertarget{hypothesis-testing}{%
\paragraph{Hypothesis testing}\label{hypothesis-testing}}

If we chose hypothesis testing for our case study, we would test a combined null hypothesis \(H_0\) that consists of four separate null hypotheses:

\[
\begin{aligned}
H_{01}:\ & P(Y=1|advice\_present = 1, advice\_correct = 1, expert = 1, u_{0s} = 0, u_{0i} = 0) \leq \\
& P(Y=1|advice\_present = 0, advice\_correct = 0, expert = 1, u_{0s} = 0, u_{0i} = 0) \\
H_{02}:\ &P(Y=1|advice\_present = 0, advice\_correct = 0, expert = 1, u_{0s} = 0, u_{0i} = 0) \leq \\
& P(Y=1|advice\_present = 1, advice\_correct = 0, expert = 1, u_{0s} = 0, u_{0i} = 0) \\
H_{03}:\ &P(Y=1|advice\_present = 1, advice\_correct = 1, expert = 0, u_{0s} = 0, u_{0i} = 0) \leq \\
& P(Y=1|advice\_present = 0, advice\_correct = 0, expert = 0, u_{0s} = 0, u_{0i} = 0) \\
H_{04}:\ & P(Y=1|advice\_present = 0, advice\_correct = 0, expert = 0, u_{0s} = 0, u_{0i} = 0) \leq \\
& P(Y=1|advice\_present = 1, advice\_correct = 0, expert = 0, u_{0s} = 0, u_{0i} = 0)
\end{aligned}
\]
The combined null hypothesis \(H_0\) should only be rejected if \textbf{all} individual null hypotheses are rejected (i.e., intersection-union setting; Dmitrienko \& D'Agostino, 2013).
In such cases, the error probabilities do not accumulate, and we would waste power when correcting for multiple tests.

With a standard significance level of \(\alpha = 0.05\), we would not reject all four null hypotheses (the p-value in the \texttt{p\ (NonInf)} column for the third hypothesis is not significant) and therefore also not reject the combined null hypothesis for this particular (simulated) dataset.
Note that this decision would be wrong because we have simulated the data such that the combined alternative hypothesis \(H_1\) is actually true in the population.

\hypertarget{interval-estimation}{%
\paragraph{Interval estimation}\label{interval-estimation}}

If we chose parameter estimation for our case study, we would focus on the two-sided confidence intervals of the four contrasts of interest.
With a standard confidence level of \(1 - \alpha = 0.95\), plausible values are clearly in the positive range for the first, third and fourth contrast, while both negative and positive values seem plausible for the second contrast.
Note that due to the constrained range of the probability scale, the width of the confidence interval differs between the four contrasts (which the expected behavior for binomial GLMMs).
The smallest width is observed for the first contrast (expert with correct advice vs.~expert without advice) where both underlying probabilities are close to 1.
The largest width is observed for the fourth contrast (non-expert with wrong advice vs.~non-expert without advice), where both underlying probabilities are closer to 0.5.

\hypertarget{perform-repeated-simulations}{%
\subsection{Perform repeated simulations}\label{perform-repeated-simulations}}

\hypertarget{theory-8}{%
\subsubsection{THEORY}\label{theory-8}}

Conducting all previous steps enables the analyst to 1) simulate a dataset, 2) estimate a GLMM, and 3) compute hypothesis tests or confidence intervals for estimands of interest, mirroring the analysis that will later be performed for the actual dataset of the planned study.
The last missing piece is to write code to perform the above steps repeatedly and allow for a setting using different sample sizes.
On a conceptual level, we first require a function, that takes as input the sample size and the full set of population parameter values.
When planning for power, the function should return the p-value(s) of the hypothesis test(s) of interest when conducted on the simulated dataset.
When planning for precision, the function should return the width of the confidence interval(s) of interest.
Secondly, we must run this function repeatedly with the same sample size and population parameters.
Because even fitting GLMMs with frequentist methods can quickly become time-consuming, it is recommended to use parallel computing, that is running simulations on multiple cores of the computer at the same time to reduce total run time.
Thirdly, the results of the repeated simulation must be collected and aggregated.
When planning for power, we compute the relative frequency of a significant p-value(s) across repeated simulations.
When planning for precision, we compute the average width of the confidence interval(s).
Lastly, we have to repeat the complete simulation for different sample sizes, to determine how big the sample must be in order to achieve the targeted power or precision.

\hypertarget{practice-8}{%
\subsubsection{PRACTICE}\label{practice-8}}

We are finally ready to run our simulation-based sample size planning analyses to plan for power and for precision. Wrapping the \texttt{simulate} function already constructed earlier, the helper function \texttt{sim\_and\_analyse} performs all previous steps (simulate a dataset, fit a GLMM, compute p-values and confidence intervals) in a single command.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_and\_analyse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
  \AttributeTok{formula\_chr =} \StringTok{"y\_bin \textasciitilde{} 1 + expert + advice\_present + advice\_correct + }
\StringTok{    expert:advice\_present + expert:advice\_correct + (1|subject) + (1|item)"}\NormalTok{,}
  \AttributeTok{contrasts =} \FunctionTok{c}\NormalTok{(}\StringTok{"b8 = b2"}\NormalTok{, }\StringTok{"b2 = b6"}\NormalTok{, }\StringTok{"b7 = b1"}\NormalTok{, }\StringTok{"b1 = b5"}\NormalTok{), ...)\{}
  \FunctionTok{require}\NormalTok{(lme4)}
  \FunctionTok{require}\NormalTok{(marginaleffects)}
  \FunctionTok{require}\NormalTok{(tidyr)}
  \CommentTok{\# simulate data}
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{simulate}\NormalTok{(...)}
  \CommentTok{\# fit model}
\NormalTok{  model }\OtherTok{\textless{}{-}} \FunctionTok{glmer}\NormalTok{(}\FunctionTok{as.formula}\NormalTok{(formula\_chr), }\AttributeTok{data =}\NormalTok{ dat, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
  \CommentTok{\# compute contrasts}
\NormalTok{  contr\_df }\OtherTok{\textless{}{-}} \FunctionTok{expand\_grid}\NormalTok{(}\AttributeTok{advice\_present =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{, }\AttributeTok{advice\_correct =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{,}
    \AttributeTok{expert =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{)}
  \FunctionTok{predictions}\NormalTok{(model, }\AttributeTok{newdata =}\NormalTok{ contr\_df, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{, }\AttributeTok{re.form =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{hypotheses}\NormalTok{(}\AttributeTok{hypothesis =}\NormalTok{ contrasts, }\AttributeTok{equivalence =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{data.frame}\NormalTok{()}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Simulation-based sample size planning can quickly become computationally intensive when we repeatedly simulate data and fit models for different parameter combinations or sample sizes. Thus, we use the future (Bengtsson, 2021) and furrr (Vaughan \& Dancho, 2022) packages to perform computations in parallel. First, we enable parallelization with the \texttt{plan} function and specify how many parallel cores (``workers'') of our computer to use (users can find out the maximum number of cores on their computer with the command \texttt{parallel::detectCores()}), and set a seed to make the simulation reproducible.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(future)}
\FunctionTok{plan}\NormalTok{(}\StringTok{"multisession"}\NormalTok{, }\AttributeTok{workers =} \DecValTok{6}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The next code chunk specifies a simulation grid with different settings for both the number of subjects (\texttt{n\_subjects}) and the number of items (\texttt{n\_items}), each combination being repeated \texttt{rep} times.
We chose 300 repetitions for the data simulation at hand as it strikes a balance between achieving a robust statistical estimate and remaining computationally feasible.
With the current settings, this simulation takes about one hour on a MacBook Pro from 2020 with M1 chip and 16 GB working memory. If you want to quickly experiment with the code yourself, a setting with \texttt{workers\ =\ 4} and \texttt{rep\ =\ 5} should finish in less than 5 minutes, even on smaller machines.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(furrr)}
\NormalTok{sim\_result }\OtherTok{\textless{}{-}} \FunctionTok{crossing}\NormalTok{(}
  \AttributeTok{rep =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{300}\NormalTok{,}
  \AttributeTok{n\_subjects =} \FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{250}\NormalTok{),}
  \AttributeTok{n\_items =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{70}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{res =} \FunctionTok{future\_pmap}\NormalTok{(., sim\_and\_analyse, }
    \AttributeTok{.options =} \FunctionTok{furrr\_options}\NormalTok{(}\AttributeTok{seed =} \ConstantTok{TRUE}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{col =}\NormalTok{ res)}
\end{Highlighting}
\end{Shaded}

The result of this computation is a data frame that contains the p-values and confidence intervals of all specified contrasts for each simulated dataset.
In some iterations (predominantly in conditions with small sample sizes), model estimation did not converge with the lme4 package. When the model fails to converge, it means that the statistical model being fitted to the data failed to reach a stable or valid solution during the estimation process. We do not remove these results because non-convergence can also happen when analyzing the real data we plan to collect, thus, we want to factor in this possibility to keep our simulation more realistic.

\hypertarget{power-results}{%
\paragraph{Power results}\label{power-results}}

For our exemplary combined hypothesis, power is defined as the (long-run) percentage of simulations in which all four p-values of our individual hypotheses are significant at the \(\alpha = 0.05\) level. Based on our simulation outcomes, we compute a power estimate for each combination of \texttt{n\_subjects} \(\times\) \texttt{n\_items} (including 95\% confidence intervals) and visualize the results with the following code.\footnote{This code was inspired by the ``Mixed Design Simulation'' vignette of the faux package at \url{https://debruine.github.io/faux/articles/sim_mixed.html}.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(binom)}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{power }\OtherTok{\textless{}{-}}\NormalTok{ sim\_result }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ term, }\AttributeTok{names\_sep =} \StringTok{"\_"}\NormalTok{, }
    \AttributeTok{values\_from =}\NormalTok{ estimate}\SpecialCharTok{:}\NormalTok{p.value.equiv) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(n\_subjects, n\_items) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{power =} \FunctionTok{mean}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{p.value.noninf\_b1=b5}\StringTok{\textasciigrave{}} \SpecialCharTok{\textless{}}\NormalTok{ alpha }\SpecialCharTok{\&} 
        \StringTok{\textasciigrave{}}\AttributeTok{p.value.noninf\_b8=b2}\StringTok{\textasciigrave{}} \SpecialCharTok{\textless{}}\NormalTok{ alpha }\SpecialCharTok{\&} \StringTok{\textasciigrave{}}\AttributeTok{p.value.noninf\_b2=b6}\StringTok{\textasciigrave{}} \SpecialCharTok{\textless{}}\NormalTok{ alpha }\SpecialCharTok{\&} 
        \StringTok{\textasciigrave{}}\AttributeTok{p.value.noninf\_b7=b1}\StringTok{\textasciigrave{}} \SpecialCharTok{\textless{}}\NormalTok{ alpha), }
    \AttributeTok{n\_sig =} \FunctionTok{sum}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{p.value.noninf\_b1=b5}\StringTok{\textasciigrave{}} \SpecialCharTok{\textless{}}\NormalTok{ alpha }\SpecialCharTok{\&} 
        \StringTok{\textasciigrave{}}\AttributeTok{p.value.noninf\_b8=b2}\StringTok{\textasciigrave{}} \SpecialCharTok{\textless{}}\NormalTok{ alpha }\SpecialCharTok{\&} \StringTok{\textasciigrave{}}\AttributeTok{p.value.noninf\_b2=b6}\StringTok{\textasciigrave{}} \SpecialCharTok{\textless{}}\NormalTok{ alpha }\SpecialCharTok{\&} 
        \StringTok{\textasciigrave{}}\AttributeTok{p.value.noninf\_b7=b1}\StringTok{\textasciigrave{}} \SpecialCharTok{\textless{}}\NormalTok{ alpha),}
    \AttributeTok{n =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{ci.lwr =} \FunctionTok{binom.confint}\NormalTok{(n\_sig, n, }\AttributeTok{method =} \StringTok{"wilson"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{lower,}
    \AttributeTok{ci.upr =} \FunctionTok{binom.confint}\NormalTok{(n\_sig, n, }\AttributeTok{method =} \StringTok{"wilson"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{upper, }
    \AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{)}
\NormalTok{power }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(n\_subjects, n\_items), factor)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(n\_subjects, n\_items, }\AttributeTok{fill =}\NormalTok{ power)) }\SpecialCharTok{+}
  \FunctionTok{geom\_tile}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{sprintf}\NormalTok{(}\StringTok{"\%.2f }\SpecialCharTok{\textbackslash{}n}\StringTok{ [\%.2f; \%.2f]"}\NormalTok{, }
\NormalTok{                                power, ci.lwr, ci.upr)), }
    \AttributeTok{color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{size =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_c}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"number of subjects"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{ylab}\NormalTok{(}\StringTok{"number of items"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/finalpwr-1.jpeg}
\caption{\label{fig:finalpwr}Simulation-based power estimates including 95\% confidence interval of the case study for different numbers of subjects and items, based on a significance level of 0.05.}
\end{figure}

As should be the case, power estimates in Figure \ref{fig:finalpwr} increase with both the number of subjects and the number of items. The confidence intervals reported here indicate how precisely power was estimated by our simulation. Higher precision (which would be reflected in narrower confidence intervals) could be obtained by increasing the number of repetitions (\texttt{rep}) in the simulation. In practice, data simulations are often run multiple times with adjusted combinations of sample sizes. When running for the first time, it might be revealed that power is way too low (or much higher than required) for some combinations of \texttt{n\_subjects} and \texttt{n\_items}. When narrowing down the best combination that achieves sufficient power while at the same time striking a good balance of how many subjects and items are practically feasible, later rounds of data simulation will typically include a smaller grid of sample sizes combined with a higher number of repetitions. This will assure high precision for the final power estimates, which are then used for the sample size justification of the future study.

Much has been written on the optimal amount of power to target in empirical research. The most prominent heuristic is to target a power of 0.8 (when combined with a type I error rate of \(\alpha = 0.05\)), but depending on the research goals of the study, there are often good reasons to move away from this standard depending on the research goals and resource constraints (Lakens, 2022b; Lakens et al., 2018). When target power has been specified, the number of subjects and the number of items in our study design can be traded against each other based on practical considerations. For the sake of the example, let the targeted power be indeed about 0.8, using an \(\alpha\) of 0.05 to detect an effect of the expected size implied by our data simulation. This could be achieved by collecting data from 200 subjects (about 25\% of which will be experts), each completing the same 50 items (with advice present in about 67\% of cases, which is correct in about 80\% of cases with present advice). If collecting data from 200 subjects is not feasible, an alternative would be to recruit 150 subjects but increase the length of the experiment to over 70 items. However, 70 items might take too long to complete for the radiologists participating in the study, who have a busy schedule. The simulation suggests that it might also be possible to plan a shorter experiment with only 30 items if it is feasible to recruit an even higher number of subjects (\textgreater{} 250, to be determined by additional rounds of power analysis). Design parameters that also affect power, and which could be investigated in the simulation to find a more optimal trade-off, are the ratio of experts, the frequency of whether advice is presented and whether it is correct.

\hypertarget{precision-results}{%
\paragraph{Precision results}\label{precision-results}}

When planning for precision, one could monitor the width of all four confidence intervals at the same time.
However, because the confidence intervals of the four contrasts strongly differ in width, it is not trivial to decide which width one should target when deciding on the appropriate sample size.
In contrast to planning for power, there are no common standards on how to specify the targeted precision.
For our example, we use a simple heuristic but we strongly encourage readers to think about better alternatives that are appropriate in their own applications.

Our simulations show that the smallest confidence interval can be expected for the first contrast (expert with correct advice vs.~expert without advice).
The true contrast in probability for an average expert and an average item in this condition is \texttt{plogis(b\_0\ +\ b\_e\ +\ b\_a\ +\ b\_c\ +\ b\_ea\ +\ b\_ec)\ -\ plogis(b\_0\ +\ b\_e)\ =} \(0.05\).
We want the width of this confidence interval to be smaller than 0.1.
This would mean that if the point estimate happens to be close to the true value, the plausible values inside of a 95\% confidence interval would all be positive.

Thus in our example, precision is defined as the (long-run) average width of a 95\% confidence interval for the probability contrast between experts with correct advice and experts without advice.
Of course, lower width implies better precision.
Based on our simulation outcomes, we compute the precision estimate for each combination of \texttt{n\_subjects} \(\times\) \texttt{n\_items} (including 95\% confidence intervals) and visualize the results with the following code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{precision }\OtherTok{\textless{}{-}}\NormalTok{ sim\_result }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ term, }\AttributeTok{names\_sep =} \StringTok{"\_"}\NormalTok{, }
    \AttributeTok{values\_from =}\NormalTok{ estimate}\SpecialCharTok{:}\NormalTok{p.value.equiv) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(n\_subjects, n\_items) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{width =} \StringTok{\textasciigrave{}}\AttributeTok{conf.high\_b8=b2}\StringTok{\textasciigrave{}} \SpecialCharTok{{-}} \StringTok{\textasciigrave{}}\AttributeTok{conf.low\_b8=b2}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{precision =} \FunctionTok{mean}\NormalTok{(width),}
    \AttributeTok{ci.lwr =} \FunctionTok{t.test}\NormalTok{(width)}\SpecialCharTok{$}\NormalTok{conf.int[}\DecValTok{1}\NormalTok{],}
    \AttributeTok{ci.upr =} \FunctionTok{t.test}\NormalTok{(width)}\SpecialCharTok{$}\NormalTok{conf.int[}\DecValTok{2}\NormalTok{], }
    \AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{)}
\NormalTok{precision }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(n\_subjects, n\_items), factor)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(n\_subjects, n\_items, }\AttributeTok{fill =}\NormalTok{ precision)) }\SpecialCharTok{+}
  \FunctionTok{geom\_tile}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{sprintf}\NormalTok{(}\StringTok{"\%.2f }\SpecialCharTok{\textbackslash{}n}\StringTok{ [\%.2f; \%.2f]"}\NormalTok{, }
\NormalTok{                                precision, ci.lwr, ci.upr)), }
    \AttributeTok{color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{size =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_c}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.3}\NormalTok{), }\AttributeTok{direction =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{fill =} \FunctionTok{guide\_legend}\NormalTok{(}\AttributeTok{reverse=}\ConstantTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/finalprecision-1.jpeg}
\caption{\label{fig:finalprecision}Simulation-based precision estimates (expected width of confidence intervals) including 95\% confidence interval of the case study for different numbers of subjects and items, based on a confidence level of 0.95.}
\end{figure}

As should be the case, precision estimates in Figure \ref{fig:finalprecision} increase (i.e., average width of confidence interval decreases) with both the number of subjects and the number of items.
The confidence intervals reported here indicate how precisely the expected width of the confidence interval for our focal contrast was estimated by our simulation.
Applying our simple heuristic of targeting an expected width smaller than 0.1, we see the same trade-off between the number of subjects and the number of items as with planning for power.
We could either choose 100 subjects and 30 items or 200 subjects and 10 items.

Note that our simple heuristic for determining sample size in the planning for precision scenario was quite liberal.
This is reflected by the result that we would need a smaller sample size than in the planning for power scenario.
With a more conservative precision target, the result is generally the opposite: As a rule, precise parameter estimates usually require bigger samples than null hypothesis testing.

\hypertarget{sensitivity-analysis}{%
\subsection{Sensitivity analysis}\label{sensitivity-analysis}}

In our case study, we have performed simulation-based sample size planning from a single set of parameter values that reflect our assumptions of an expected effect size. Instead of extracting this expected effect size from meta-analyses or pilot data, which has been the main focus of previous tutorials, we have demonstrated some strategies to determine plausible parameter values in GLMMs based on domain knowledge. Domain knowledge can be considered a vague theoretical model about the data-generating process that is less formal and can only be accessed by a back-and-forth exchange in which domain experts assess the plausibility of simulated data. When sample sizes are chosen based on the results of our simulation-based power analysis, a future study will be informative to reject the null hypothesis if an effect of our \emph{expected size} is present (or estimate the effect with satisfying precision). However, if the true effect is indeed smaller, power (or precision) will be lower, and the study might not be sufficiently informative. A common, more conservative strategy for sample size justification is to perform sample size planning for the smallest effect size of interest (SESOI). An effect smaller than the SESOI would be considered too small to be interesting or practically meaningful, even if the effect is not actually zero (King, 2011). For strategies on the even more difficult task of specifying a plausible SESOI, as well as a thorough discussion of various topics concerning power analysis, see (Lakens, 2022a). When domain knowledge or formal theories about the research topic of interest are too vague to specify a meaningful SESOI, it is still recommended to demonstrate power or precision for different effect sizes in what is called \emph{sensitivity power analysis}. By simulating power (or precision) for different effect sizes (in addition to the different number of subjects and items), one can make sure that power (or precision) would still be sufficient to detect smaller effect sizes than our expected effect or at least get an impression of how strongly power (or precision) depends on the size of the true effect. In simple study designs, it is possible to perform sensitivity analysis based on a single standardized effect size (e.g., analyze power in a two-sample t-test for a standardized mean difference varying between 0.1 and 0.8). However, for our case study that investigates combined hypotheses in a GLMM modeling framework, the effect size is implicitly represented by the complex distribution of probabilities within and between experimental conditions. In this setting, sensitivity analysis would require manually specifying additional sets of plausible parameter values that reflect scenarios with smaller or larger differences between groups with respect to our specific research question. Power (or precision) could then be simulated for several of these scenarios (across different numbers of subjects and items, as considered earlier).

\hypertarget{outlook}{%
\section{Outlook}\label{outlook}}

Beyond the specifics of our concrete case study, we want to outline six developments regarding the future role of simulation-based sample size planning in experimental research:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The growing need for simulation-based sample size planning in experimental research: In order to conduct informative research using varying complex experimental designs with GLMMs, formula-based heuristics and user-friendly software tools for a priori power analysis are often not suitable. Therefore, simulation-based power analysis is becoming increasingly needed since it provides experimental researchers with a tailored approach to estimating required sample sizes before data collection.
\item
  Managing data simulations more easily with discrete predictor variables: Simulation-based sample size planning becomes more manageable when all predictor variables are discrete (like in the presented case study) and fixed by the study design.
  This allows researchers to focus on simulating outcome variables while avoiding the need for complex simulations of predictor values, which would introduce additional assumptions. By simplifying the simulation process, researchers can obtain reliable estimates for power or precision without compromising realistic assumptions about the data-generating process implied by the study design.
\item
  Teaching data simulation skills: The ability to conduct simulation-based sample size planning is a valuable skill that should be taught to experimental researchers. By incorporating such training into research methods courses and workshops, researchers can gain a deeper understanding of statistical power or precision, and improve the quality of their experimental designs.
  Equipping researchers with the knowledge and tools to perform simulation-based sample size planning enables them to make informed decisions and enhance the rigor of their studies.
  The need to reason about how to simulate plausible data that is in line with the research hypothesis, while not violating domain expertise on how plausible data should look, might also contribute to planning more insightful studies that can answer more precise research questions (Yarkoni, 2022).
\item
  Addressing the mismatch in effort perception: There is often a significant disconnect between the amount of effort required to perform a priori simulation-based sample size planning and the perceived effort estimated by researchers and collaborators in experimental research. Many researchers request simulation-based power analyses from statisticians or methodological experts without fully comprehending the complexity and time-consuming nature of these tailored simulations. It is crucial to raise awareness about the effort involved to ensure realistic expectations and effective collaboration between researchers and methodological experts.
\item
  Recognizing the value of simulation-based design analysis: Simulation-based power analyses are not mere technicalities; they are valuable research contributions that deserve recognition in experimental research. They offer insights into the robustness and sensitivity of experimental designs, helping researchers make informed decisions about sample sizes, effect sizes, and statistical power or precision. Their importance can be reflected by allocating them a separate publication or incorporating them as a significant component of stage 1 preregistered reports (Chambers \& Tzavella, 2022).
\item
  Integration with Open Science and preregistration practices: Simulation-based sample size planning aligns well with the principles of Open Science and preregistration in experimental research. When researchers have access to simulated data based on their pre-specified model, analyzing the collected dataset becomes straightforward and unambiguous. By preregistering their simulation-based sample size plan, researchers enhance the transparency and accountability of their experimental procedures, contributing to the credibility and reproducibility of research.
\end{enumerate}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

In the wake of the replication crisis and myriad of underpowered experimental work, generalized linear mixed models (GLMMs) offer a flexible statistical framework to analyze experimental data with complex (e.g., dependent and hierarchical) data structures. Yet, analytic methods and software cannot be applied to conduct a priori sample size planning for GLMMs, necessitating data simulation-based approaches. Through this applied tutorial, we aim to provide researchers with the necessary skills and tools to perform simulation-based sample size planning with GLMMs themselves. By incorporating GLMMs and a priori sample size planning into their work, researchers can enhance the replicability and credibility of their experiments (Yarkoni, 2022).

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-R-marginaleffects}{}}%
Arel-Bundock, V. (2023). \emph{Marginaleffects: Predictions, comparisons, slopes, marginal means, and hypothesis tests}. Retrieved from \url{https://CRAN.R-project.org/package=marginaleffects}

\leavevmode\vadjust pre{\hypertarget{ref-arendStatisticalPowerTwolevel2019}{}}%
Arend, M. G., \& Schäfer, T. (2019). Statistical power in two-level models: {A} tutorial based on {Monte Carlo} simulation. \emph{Psychological Methods}, \emph{24}(1), 1--19. \url{https://doi.org/10.1037/met0000195}

\leavevmode\vadjust pre{\hypertarget{ref-batesFittingLinearMixedEffects2015}{}}%
Bates, D., Mächler, M., Bolker, B., \& Walker, S. (2015). Fitting {Linear Mixed-Effects Models Using} {\textbf{Lme4}}. \emph{Journal of Statistical Software}, \emph{67}(1). \url{https://doi.org/10.18637/jss.v067.i01}

\leavevmode\vadjust pre{\hypertarget{ref-R-RJ-2021-048}{}}%
Bengtsson, H. (2021). A unifying framework for parallel and distributed processing in r using futures. \emph{The R Journal}, \emph{13}(2), 208--227. \url{https://doi.org/10.32614/RJ-2021-048}

\leavevmode\vadjust pre{\hypertarget{ref-brysbaertPowerAnalysisEffect2018}{}}%
Brysbaert, M., \& Stevens, M. (2018). Power {Analysis} and {Effect Size} in {Mixed Effects Models}: {A Tutorial}. \emph{Journal of Cognition}, \emph{1}(1), 9. \url{https://doi.org/10.5334/joc.10}

\leavevmode\vadjust pre{\hypertarget{ref-burknerBrmsPackageBayesian2017}{}}%
Bürkner, P.-C. (2017). Brms: {An R Package} for {Bayesian Multilevel Models Using Stan}. \emph{Journal of Statistical Software}, \emph{80}, 1--28. \url{https://doi.org/10.18637/jss.v080.i01}

\leavevmode\vadjust pre{\hypertarget{ref-chambersPresentFutureRegistered2022}{}}%
Chambers, C. D., \& Tzavella, L. (2022). The past, present and future of {Registered Reports}. \emph{Nature Human Behaviour}, \emph{6}(1), 29--42. \url{https://doi.org/10.1038/s41562-021-01193-7}

\leavevmode\vadjust pre{\hypertarget{ref-champelyPackagePwr2018}{}}%
Champely, S., Ekstrom, C., Dalgaard, P., Gill, J., Weibelzahl, S., Anandkumar, A., \ldots{} De Rosario, M. H. (2018). Package {``pwr.''} \emph{R Package Version}, \emph{1}(2).

\leavevmode\vadjust pre{\hypertarget{ref-R-faux}{}}%
DeBruine, L. (2023). \emph{Faux: Simulation for factorial designs}. Zenodo. \url{https://doi.org/10.5281/zenodo.2669586}

\leavevmode\vadjust pre{\hypertarget{ref-debruineUnderstandingMixedEffectsModels2021}{}}%
DeBruine, L., \& Barr, D. J. (2021). Understanding {Mixed-Effects Models Through Data Simulation}. \emph{Advances in Methods and Practices in Psychological Science}, \emph{4}(1), 2515245920965119. \url{https://doi.org/10.1177/2515245920965119}

\leavevmode\vadjust pre{\hypertarget{ref-dmitrienkoTraditionalMultiplicityAdjustment2013}{}}%
Dmitrienko, A., \& D'Agostino, R. (2013). Traditional multiplicity adjustment methods in clinical trials. \emph{Statistics in Medicine}, \emph{32}(29), 5172--5218. \url{https://doi.org/10.1002/sim.5990}

\leavevmode\vadjust pre{\hypertarget{ref-fahrmeirRegressionModelsMethods2021}{}}%
Fahrmeir, L., Kneib, T., Lang, S., \& Marx, B. D. (2021). \emph{Regression: {Models}, {Methods} and {Applications}}. Berlin, Heidelberg: Springer Berlin Heidelberg. \url{https://doi.org/10.1007/978-3-662-63882-8}

\leavevmode\vadjust pre{\hypertarget{ref-faulStatisticalPowerAnalyses2009}{}}%
Faul, F., Erdfelder, E., Buchner, A., \& Lang, A.-G. (2009). Statistical power analyses using {G}*{Power} 3.1: {Tests} for correlation and regression analyses. \emph{Behavior Research Methods}, \emph{41}(4), 1149--1160. \url{https://doi.org/10.3758/BRM.41.4.1149}

\leavevmode\vadjust pre{\hypertarget{ref-gelmanBayesianWorkflow2020}{}}%
Gelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., \ldots{} Modrák, M. (2020). \emph{Bayesian {Workflow}}. arXiv. Retrieved from \url{https://arxiv.org/abs/2011.01808}

\leavevmode\vadjust pre{\hypertarget{ref-greenSIMRPackagePower2016}{}}%
Green, P., \& MacLeod, C. J. (2016). {SIMR}: An {R} package for power analysis of generalized linear mixed models by simulation. \emph{Methods in Ecology and Evolution}, \emph{7}(4), 493--498. \url{https://doi.org/10.1111/2041-210X.12504}

\leavevmode\vadjust pre{\hypertarget{ref-kelleyObtainingPowerObtaining2003}{}}%
Kelley, K., Maxwell, S. E., \& Rausch, J. R. (2003). Obtaining {Power} or {Obtaining Precision}: {Delineating Methods} of {Sample-Size Planning}. \emph{Evaluation \& the Health Professions}, \emph{26}(3), 258--287. \url{https://doi.org/10.1177/0163278703255242}

\leavevmode\vadjust pre{\hypertarget{ref-kelleySampleSizePlanning2006}{}}%
Kelley, K., \& Rausch, J. R. (2006). Sample size planning for the standardized mean difference: {Accuracy} in parameter estimation via narrow confidence intervals. \emph{Psychological Methods}, \emph{11}(4), 363--385. \url{https://doi.org/10.1037/1082-989X.11.4.363}

\leavevmode\vadjust pre{\hypertarget{ref-kingPointMinimalImportant2011}{}}%
King, M. T. (2011). A point of minimal important difference ({MID}): A critique of terminology and methods. \emph{Expert Review of Pharmacoeconomics \& Outcomes Research}, \emph{11}(2), 171--184. \url{https://doi.org/10.1586/erp.11.9}

\leavevmode\vadjust pre{\hypertarget{ref-kumleEstimatingPowerGeneralized2021}{}}%
Kumle, L., Võ, M. L.-H., \& Draschkow, D. (2021). Estimating power in (generalized) linear mixed models: {An} open introduction and tutorial in {R}. \emph{Behavior Research Methods}, \emph{53}(6), 2528--2543. \url{https://doi.org/10.3758/s13428-021-01546-0}

\leavevmode\vadjust pre{\hypertarget{ref-lafitSelectionNumberParticipants2021}{}}%
Lafit, G., Adolf, J. K., Dejonckheere, E., Myin-Germeys, I., Viechtbauer, W., \& Ceulemans, E. (2021). Selection of the {Number} of {Participants} in {Intensive Longitudinal Studies}: {A User-Friendly Shiny App} and {Tutorial} for {Performing Power Analysis} in {Multilevel Regression Models That Account} for {Temporal Dependencies}. \emph{Advances in Methods and Practices in Psychological Science}, \emph{4}(1), 251524592097873. \url{https://doi.org/10.1177/2515245920978738}

\leavevmode\vadjust pre{\hypertarget{ref-lakensImprovingYourStatistical2022}{}}%
Lakens, D. (2022a). \emph{Improving {Your Statistical Inferences}}. Zenodo. \url{https://doi.org/10.5281/ZENODO.6409077}

\leavevmode\vadjust pre{\hypertarget{ref-lakensSampleSizeJustification2022}{}}%
Lakens, D. (2022b). Sample {Size Justification}. \emph{Collabra: Psychology}, \emph{8}(1), 33267. \url{https://doi.org/10.1525/collabra.33267}

\leavevmode\vadjust pre{\hypertarget{ref-lakensJustifyYourAlpha2018}{}}%
Lakens, D., Adolfi, F. G., Albers, C. J., Anvari, F., Apps, M. A. J., Argamon, S. E., \ldots{} Zwaan, R. A. (2018). Justify your alpha. \emph{Nature Human Behaviour}, \emph{2}(3), 168--171. \url{https://doi.org/10.1038/s41562-018-0311-x}

\leavevmode\vadjust pre{\hypertarget{ref-lundbergWhatYourEstimand2021}{}}%
Lundberg, I., Johnson, R., \& Stewart, B. M. (2021). What {Is Your Estimand}? {Defining} the {Target Quantity Connects Statistical Evidence} to {Theory}. \emph{American Sociological Review}, \emph{86}(3), 532--565. \url{https://doi.org/10.1177/00031224211004187}

\leavevmode\vadjust pre{\hypertarget{ref-maxwellSampleSizePlanning2008}{}}%
Maxwell, S. E., Kelley, K., \& Rausch, J. R. (2008). Sample {Size Planning} for {Statistical Power} and {Accuracy} in {Parameter Estimation}. \emph{Annual Review of Psychology}, \emph{59}(1), 537--563. \url{https://doi.org/10.1146/annurev.psych.59.103006.093735}

\leavevmode\vadjust pre{\hypertarget{ref-murayamaSummarystatisticsbasedPowerAnalysis2022}{}}%
Murayama, K., Usami, S., \& Sakaki, M. (2022). Summary-statistics-based power analysis: {A} new and practical method to determine sample size for mixed-effects modeling. \emph{Psychological Methods}. \url{https://doi.org/10.1037/met0000330}

\leavevmode\vadjust pre{\hypertarget{ref-R-furrr}{}}%
Vaughan, D., \& Dancho, M. (2022). \emph{Furrr: Apply mapping functions in parallel using futures}. Retrieved from \url{https://CRAN.R-project.org/package=furrr}

\leavevmode\vadjust pre{\hypertarget{ref-westfallStatisticalPowerOptimal2014}{}}%
Westfall, J., Kenny, D. A., \& Judd, C. M. (2014). Statistical power and optimal design in experiments in which samples of participants respond to samples of stimuli. \emph{Journal of Experimental Psychology: General}, \emph{143}, 2020--2045. \url{https://doi.org/10.1037/xge0000014}

\leavevmode\vadjust pre{\hypertarget{ref-wickhamDataScienceImport2023}{}}%
Wickham, H., Çetinkaya-Rundel, M., \& Grolemund, G. (2023). \emph{R for data science: Import, tidy, transform, visualize, and model data} (2nd edition). Beijing Boston Farnham Sebastopol Tokyo: O'Reilly.

\leavevmode\vadjust pre{\hypertarget{ref-yarkoniGeneralizabilityCrisis2022}{}}%
Yarkoni, T. (2022). The generalizability crisis. \emph{Behavioral and Brain Sciences}, \emph{45}, e1. \url{https://doi.org/10.1017/S0140525X20001685}

\leavevmode\vadjust pre{\hypertarget{ref-zimmerSampleSizePlanning2022}{}}%
Zimmer, F., Henninger, M., \& Debelak, R. (2022). \emph{Sample {Size Planning} for {Complex Study Designs}: {A Tutorial} for the mlpwr {Package}}. PsyArXiv. \url{https://doi.org/10.31234/osf.io/r9w6t}

\end{CSLReferences}


\end{document}
